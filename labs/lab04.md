## Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming

#### Дедлайн

Понедельник, 22 марта, 23:59.

#### Дедлайн Github

Четверг, 25 марта, 23:59

#### Задача

У вас имеются данные: логи посещения пользователей по разным сайтам рунета. По этим пользователям вам также известны: пол и возрастная категория. Вам нужно будет в real-time спрогнозировать эти характеристики по пользователям, о которых у вас нет этой информации, но будут все те же самые логи посещения.

##### При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и прочие.

#### Описание данных

Обучающая выборка, с которой вы будете работать, выглядит следующим образом:

```json
gender	age	uid	user_json
F	18-24	d50192e5-c44e-4ae8-ae7a-7cfe67c8b777	{"visits": [{"url": "http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun", "timestamp": 1419688144068}, {"url": "http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story", "timestamp": 1426666298001}, {"url": "http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html", "timestamp": 1426666298000}, {"url": "http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story", "timestamp": 1426661722001}, {"url": "http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html", "timestamp": 1426661722000}]}
```

Она расположена на hdfs: `/labs/slaba04/`.

Поле `gender` принимает значения `F` (женщина) и `M` (мужчина).

Поле `age` принимает значения диапазона возраста: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`

Поле `uid` принимает значения уникального ID пользователя (cookies): `d50192e5-c44e-4ae8-ae7a-7cfe67c8b777`.

Поле `user_json` имеет внутри json со следующей схемой данных: `{"visits": [{"url": "url1", "timestamp": "timestamp1"}, {"url": "url2", "timestamp": "timestamp2"}]}`. В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения.

Данные, по которым вам надо будет построить прогноз (тестовая выборка), хранятся в Kafka в таком виде:

```json
{"uid": "bdd48781-8243-493d-8ae6-794050a4417f",
 "visits": "[{"url": "http://vk.com/feed", "timestamp": 1425077901001}, {"url": "http://big-cards.a5ltd.com/app/html/vk.html?api_url=http://api.vk.com/api.php&api_id=1804162&api_settings=663823&viewer_id=40849488&viewer_type=2&sid=1b77e9ca975573fed6e31746ca58ea2509f3588d918e7dab2a291f7f7579ae7dab6c21058c837b74b1fbb&secret=37339dfcbf&access_token=06c53bd8f9f9a0afad23b4e166e448495396259fc5a1c5e5243474df82591bd5d0afe877d993ea4a82a81&user_id=40849488&group_id=0&is_app_user=1&auth_key=d7c91ad4bb27e20cb3ed81810f5649db&language=0&parent_language=0&ad_info=elsdcqvcsvftaqxtawjsxht b0q8htjxuvbbjrvbnwojfji2ha8h&is_secure=0&ads_app_id=1804162_903aa103fa48b1e378&referrer=menu&lc_name=2c073eed&hash=", "timestamp": 1425077901000}]"}
```

То есть все то же самое, только без поля `gender` и `age` и немного с другой схемой данных. Название топика, в который мы будем присылать данные: `input_ivan.ivanov`, где вместо `ivan.ivanov` ваш логин от личного кабинета. Данные вам будут поступать по нажатию кнопки "Проверить" в личном кабинете. Вам топик создавать не нужно. Он будет автоматом создаваться при записи. Чистить его тоже не надо, потому что Spark Streaming хранит оффсеты, то есть знает, что он уже считывал, что еще не считывал. 

Порт брокера Kafka: `6667`. Hostname: `spark-master-1.newprolab.com` или `spark-node-1.newprolab.com`.

#### Результат

Вам в свою очередь нужно будет, считывая данные из Kafka, делать прогноз по ним в Spark Streaming и отправлять в real-time эти прогнозные значения в свой другой топик в Kafka в формате:

```json
{"uid": "fe1dba8f-3131-439f-9031-851c0da0f126", "gender": "M", "age": "25-34"}
{"uid": "d50192e5-c44e-4ae8-ae7a-7cfe67c8b777", "gender": "F", "age": "18-24"}
```

Название топика — ваш логин в личном кабинете: `ivan.ivanov`. После отправки данных чекер будет ждать 90 секунд, чтобы вы обработали входные данные из топика `input_ivan.ivanov` и записали результаты прогноза уже в этот топик. 

#### Подсказки

Чтение из Kafka:

```python
read_kafka_params = {
    "kafka.bootstrap.servers": 'spark-master-1.newprolab.com:6667',
    "subscribe": "input_ivan.ivanov",
    "startingOffsets": "latest"
}
kafka_sdf = spark.readStream.format("kafka").options(**read_kafka_params).load()
```

Запись в Kafka:

```python
write_kafka_params = {
   "kafka.bootstrap.servers": 'spark-master-1.newprolab.com:6667',
   "topic": "ivan.ivanov"
}
batch_df.writeStream.format("kafka").options(**write_kafka_params)\
    .option("checkpointLocation", "streaming/chk/chk_kafka")\
    .outputMode("append").start()
```
  
Бывают проблемы при чтении старых данных, которых уже нет в кафке:
  
```
.option("failOnDataLoss", 'False')
```
  
#### Проверка
Проверка лабы устроена следующим образом:
1. Вы готовите свою модель
2. Настраиваете чтение данных из одного топика и запись в другой топик.
4. Запускаете свой скрипт в стриминговым режиме
3. Идет подключение к топику на чтение данных.
4. На странице лабы нажимаете кнопку "Проверить".
5. Вам в топик текут данные.
6. Модель обрабатывает входные данные.
7. Ваш скрипт сохраняет данные в другой топик.
8. Чекер на странице лабы выдает результат проверки.


Проверка осуществляется автоматическим скриптом на странице лабы в личном кабинете. Наш чекер в конечном итоге подключится к вашему финальному топику в Kafka и сравнит ваш прогноз с эталонным ответом. На выходе вы получите свой score. В качестве него метрика `accuracy`: доля пользователей от общего числа, по которым вы верно угадали и пол, и возрастную категорию. Если `accuracy` будет больше **0.25**, то лаба будет засчитана.

##### Особенности проверки
1. Проверка (генерация событий, обработка вашим скриптом, чтение и проверка результатов) занимают до 90 секунд, просьба ожидать (можно нажимать кнопку "обновить")
2. За один раз (одно нажание "Проверить") чекер присылает батч в 5000 событий. Если для отладки / повторной проверки требуется еще, надо нажимать снова

####  Рекомендации по прохождению
1. Разработанную модель (и вообще весь пайплайн) можно сохранить в HDFS и потом загрузить уже в стриминговом скрипте (методы save/load - см.документацию)
2. Настоятельно рекомендуется сначала отладить весь процесс чтения данных, предсказания и сохранения результатов в **батчевом** режиме (сгенерить порцию событий чекером и потом считать ее в обычный DataFrame через  `spark.read.format("kafka")`. И уже только после проверки полной  работоспособсности всего скрипта и корректности формата выходных данных в Кафке, переписать на стриминговый режим (благодаря единому API Spark, это не потребует больших изменений)

Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack.
