{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "# exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"Lab04 test run mikhail.novikov\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab04 test run mikhail.novikov</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feec56a64a8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## либы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple, from_json, get_json_object, col, explode, expr, \\\n",
    "collect_set, collect_list, regexp_replace, get_json_object, to_json, struct\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vector, DenseVector\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import json, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/user/mikhail.novikov/lab04_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "INPUT_KAFKA_TOPIC = 'input_mikhail.novikov'\n",
    "OUTPUT_KAFKA_TOPIC = 'mikhail.novikov'\n",
    "\n",
    "# INPUT_KAFKA_TOPIC = 'input_alexander.sedykh'\n",
    "# OUTPUT_KAFKA_TOPIC = 'alexander.sedykh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## грузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /labs/slaba04/gender_age_dataset.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0 = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', header=True, inferSchema=True, sep='\\t')\n",
    "train_df_0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('count', train_df_0.count())\n",
    "train_df_0.show(1,truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisitsType = StructType([\n",
    "    StructField('visits', ArrayType(\n",
    "        StructType([\n",
    "            StructField('url', StringType(), True),\n",
    "            StructField('timestamp', LongType(), True)\n",
    "            ])\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "train_df_flattened = (\n",
    "    train_df_0\n",
    "    .withColumn('visits', from_json(col('user_json'), VisitsType))\n",
    "    .withColumn('visit', explode('visits.visits').alias('visit'))\n",
    "    .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "    .drop('visits', 'visit', 'user_json')\n",
    ")\n",
    "\n",
    "train_df_flattened.printSchema()\n",
    "train_df_flattened.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = (\n",
    "    train_df_flattened\n",
    "    .groupBy('gender', 'age', 'uid')\n",
    "    .agg(collect_list('host')\n",
    "    .alias('hosts'))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "full_train_df.printSchema()\n",
    "full_train_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## моделируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = full_train_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### задаем компоненты нашего pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_TF = HashingTF(inputCol='hosts', outputCol='rawFeatures', numFeatures=10000, binary=False)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "indexer_age = (StringIndexer(inputCol='age', \n",
    "                             outputCol='ageIndex'\n",
    "                            )\n",
    "               .fit(full_train_df)\n",
    "              )\n",
    "\n",
    "indexer_gender = (StringIndexer(inputCol='gender', \n",
    "                                outputCol='genderIndex'\n",
    "                               )\n",
    "                  .fit(full_train_df)\n",
    "                 )\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "rf_age = RandomForestClassifier(featuresCol = 'rawFeatures', \n",
    "                                labelCol = 'ageIndex',\n",
    "                                predictionCol='age_index_prediction', \n",
    "                                rawPredictionCol='age_index_raw_prediction',\n",
    "                                probabilityCol='age_probability'\n",
    "                               )\n",
    "\n",
    "rf_gender = RandomForestClassifier(featuresCol = 'rawFeatures', \n",
    "                                   labelCol = 'genderIndex',\n",
    "                                   predictionCol='gender_index_prediction', \n",
    "                                   rawPredictionCol='gender_index_raw_prediction',\n",
    "                                   probabilityCol='gender_probability'\n",
    "                                  )\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "converter_age = IndexToString(inputCol='age_index_prediction', \n",
    "                              outputCol='PredictedAge', \n",
    "                              labels=indexer_age.labels\n",
    "                             )\n",
    "\n",
    "converter_gender = IndexToString(inputCol='gender_index_prediction', \n",
    "                                 outputCol='PredictedGender', \n",
    "                                 labels=indexer_gender.labels\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### фитим pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    Pipeline(\n",
    "        stages=[hashing_TF, indexer_age, indexer_gender, rf_age, rf_gender, converter_age, converter_gender]\n",
    "    )\n",
    ")\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### селектим необходимые поля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('gender', 'age', 'PredictedAge', 'PredictedGender').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### оцениваем качество модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_age = (MulticlassClassificationEvaluator(\n",
    "    labelCol='ageIndex', \n",
    "    predictionCol='age_index_prediction', \n",
    "    metricName='accuracy')\n",
    "                )\n",
    "\n",
    "accuracy_age = evaluator_age.evaluate(predictions)\n",
    "\n",
    "evaluator_gender = (\n",
    "    MulticlassClassificationEvaluator(\n",
    "        labelCol='genderIndex', \n",
    "        predictionCol='gender_index_prediction', \n",
    "        metricName='accuracy')\n",
    ")\n",
    "accuracy_gender = evaluator_gender.evaluate(predictions)\n",
    "\n",
    "print('accuracy for age: ' + str(accuracy_age))\n",
    "print('accuracy for gender: ' + str(accuracy_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### сохраняем обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/mikhail.novikov/lab04_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## скоринг по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': INPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'earliest',\n",
    "    'endingOffsets': 'latest'\n",
    "}\n",
    "\n",
    "kafka_sdf = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .option(\"failOnDataLoss\", 'False')\n",
    "    .load()\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "kafka_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 10000\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "| key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|input_alexander.s...|        0| 75000|2021-03-24 13:29:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexander.s...|        0| 75001|2021-03-24 13:29:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_alexander.s...|        0| 75002|2021-03-24 13:29:...|            0|\n",
      "+----+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('count',kafka_sdf.count())\n",
    "kafka_sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|              visits|\n",
      "+--------------------+--------------------+\n",
      "|bd7a30e1-a25d-4cb...|[[http://www.inte...|\n",
      "|bd7a6f52-45db-49b...|[[https://www.pac...|\n",
      "|bd7a7fd9-ab06-42f...|[[http://www.mk.r...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventType = StructType([\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('visits', StringType(), True),\n",
    "])\n",
    "\n",
    "visitType = ArrayType(\n",
    "    StructType([\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('timestamp', LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "clean_df = (\n",
    "    kafka_sdf\n",
    "    .select(col('value').cast('string').alias('value'))\n",
    "    .select(from_json(col('value'), eventType).alias('data'))\n",
    "    .select('data.*')\n",
    "    .select('uid', from_json(col('visits'), visitType).alias('visits'))\n",
    "    )\n",
    "\n",
    "clean_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- hosts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 uid|               hosts|\n",
      "+--------------------+--------------------+\n",
      "|0108d217-e476-493...|[kvartblog.ru, kv...|\n",
      "|0192cc54-559c-4c8...|[metanol.lv, meta...|\n",
      "|019acd5e-be9a-4cd...|[www.russianfood....|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df = (\n",
    "    clean_df\n",
    "    .withColumn('visit', explode('visits').alias('visit'))\n",
    "    .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "    .drop('visits', 'visit')\n",
    "    .groupBy('uid')\n",
    "    .agg(collect_list('host').alias('hosts'))\n",
    ")\n",
    "\n",
    "prep_df.printSchema()\n",
    "prep_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|                 uid|gender|  age|\n",
      "+--------------------+------+-----+\n",
      "|0108d217-e476-493...|     M|25-34|\n",
      "|0192cc54-559c-4c8...|     M|25-34|\n",
      "|019acd5e-be9a-4cd...|     F|25-34|\n",
      "+--------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inf_model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "predictions_df = (\n",
    "    inf_model.transform(prep_df)\n",
    "    .select('uid', 'PredictedGender', 'PredictedAge')\n",
    "    .withColumnRenamed('PredictedAge', 'age')\n",
    "    .withColumnRenamed('PredictedGender', 'gender')\n",
    ")\n",
    "\n",
    "predictions_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"uid\":\"0108d217-...|\n",
      "|{\"uid\":\"0192cc54-...|\n",
      "|{\"uid\":\"019acd5e-...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_out_df = predictions_df.select(to_json(struct(*predictions_df.columns)).alias('value')).limit(10)\n",
    "\n",
    "kafka_out_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'topic': OUTPUT_KAFKA_TOPIC\n",
    "}\n",
    "\n",
    "(kafka_out_df.write\n",
    "    .format('kafka')\n",
    "    .options(**write_kafka_params)\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## скоринг в стриме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model = PipelineModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': INPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'latest'\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'topic': OUTPUT_KAFKA_TOPIC\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventType = StructType([\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('visits', StringType(), True)\n",
    "])\n",
    "\n",
    "visitType = ArrayType(\n",
    "    StructType([\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('timestamp', LongType(), True)  \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):  \n",
    "    clean_df = (\n",
    "        batch_df\n",
    "        .select(col('value').cast('string').alias('value'))\n",
    "        .select(from_json(col('value'), eventType).alias('data'))\n",
    "        .select('data.*')\n",
    "        .select('uid', from_json(col('visits'), visitType).alias('visits'))\n",
    "    )\n",
    "    \n",
    "    proc_df = (\n",
    "        clean_df\n",
    "        .withColumn('visit', explode('visits').alias('visits'))\n",
    "        .withColumn('host', expr('parse_url(visit.url, \"HOST\")').alias('host'))\n",
    "        .drop('visits', 'visit')\n",
    "        .groupBy('uid')\n",
    "        .agg(collect_list('host').alias('hosts'))\n",
    "    )\n",
    "    \n",
    "    predictions_df = (\n",
    "        inf_model.transform(proc_df)\n",
    "        .select('uid', 'PredictedGender', 'PredictedAge')\n",
    "        .withColumnRenamed('PredictedAge', 'age')\n",
    "        .withColumnRenamed('PredictedGender', 'gender')\n",
    "    )\n",
    "    \n",
    "    kafka_df = (\n",
    "        predictions_df\n",
    "        .select(to_json(struct(*predictions_df.columns)).alias('value'))\n",
    "    )\n",
    "       \n",
    "    kafka_df\\\n",
    "     .write\\\n",
    "     .format('kafka')\\\n",
    "     .options(**write_kafka_params)\\\n",
    "     .mode('append')\\\n",
    "     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_test_df = (spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .option(\"failOnDataLoss\", 'False')\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    return df.writeStream\\\n",
    "            .foreachBatch(process_batch)\\\n",
    "            .option('checkpointLocation', 'streaming/chk/chk_kafka_mikhail_novikov_lab04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = create_console_sink(kafka_test_df)\n",
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек нашей отправки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': OUTPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'earliest'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chk_sdf = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('kafka')\n",
    "    .options(**chk_kafka_params)\n",
    "    .load()\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "chk_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 20\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|alexander.sedykh|        0|     0|2021-03-24 13:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|alexander.sedykh|        0|     1|2021-03-24 13:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|alexander.sedykh|        0|     2|2021-03-24 13:41:...|            0|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('count',chk_sdf.count())\n",
    "chk_sdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## остановка стрима и spark-а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ae35e7a5c51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sq' is not defined"
     ]
    }
   ],
   "source": [
    "sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
