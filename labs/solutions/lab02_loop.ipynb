{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab02 Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/laba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Курсы, по которым нужно выдать решение\n",
    "given_courses = [\n",
    "    [8150, u'en', u'StatLearning: Statistical Learning'], \n",
    "    [25679, u'en', u'Video Lighting Basics - Udemy'], \n",
    "    [7791, u'es', u'Programaci\\xf3n CNC - Fresadoras'], \n",
    "    [23111, u'es', u'C\\xf3mo Crear un Blog Gratis en Google Blogger - Udemy'], \n",
    "    [1396, u'ru', u'\\u0412\\u0432\\u0435\\u0434\\u0435\\u043d\\u0438\\u0435 \\u0432\\u043e \\u0432\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0438 Windows Embedded CE'], \n",
    "    [1348, u'ru', u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f Microsoft ADO .NET']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8150, 'en', 'StatLearning: Statistical Learning'],\n",
       " [25679, 'en', 'Video Lighting Basics - Udemy'],\n",
       " [7791, 'es', 'Programación CNC - Fresadoras'],\n",
       " [23111, 'es', 'Cómo Crear un Blog Gratis en Google Blogger - Udemy'],\n",
       " [1396, 'ru', 'Введение во встроенные системы и Windows Embedded CE'],\n",
       " [1348, 'ru', 'Технология Microsoft ADO .NET']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_langs = [(course[0], course[1]) for course in given_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF + TFIDF + dot product + l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import re\n",
    "\n",
    "def clear_string(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words\n",
    "\n",
    "tokenizer_udf = pandas_udf(clear_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = data.withColumn(\"words\", tokenizer_udf(\"desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"TFFeatures\", numFeatures=10000, binary=False)\n",
    "\n",
    "hashed_data = hashingTF.transform(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"TFFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashed_data)\n",
    "idfed_data = idfModel.transform(hashed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def vectorToArray(row):\n",
    "    return row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 16 ms, total: 160 ms\n",
      "Wall time: 40.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Нормализация векторов L2, после этого для cosine_similarity будет достаточно \n",
    "# делать dot product нормализованных векторов\n",
    "from pyspark.ml.feature import Normalizer\n",
    "t = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "\n",
    "normalized_data = t.transform(idfed_data)\n",
    "# Для каждого курса посчитаем косинусное расстояние до всех остальных\n",
    "# выберем 10 самых похожих\n",
    "\n",
    "target_course_reqs = {}\n",
    "\n",
    "for course_id, lang in courses_langs:\n",
    "    course_vec = normalized_data.filter(normalized_data.id == int(course_id))\\\n",
    "             .collect()[0]['features'].toArray()\n",
    "    \n",
    "    cos_sim = f.udf(lambda x: float(x.dot(course_vec)), FloatType())\n",
    "    \n",
    "    recs = normalized_data.where((normalized_data.id != int(course_id)) & (normalized_data.lang == lang))\\\n",
    "               .withColumn('cosine_sim', cos_sim(normalized_data['features']))\\\n",
    "               .orderBy(f.desc('cosine_sim'), f.asc('name'), f.asc('id'))\\\n",
    "               .head(10)\n",
    "                           \n",
    "    list_out = [rec['id'] for rec in recs]\n",
    "    target_course_reqs.update({str(course_id): list_out})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8150': [249, 13275, 17191, 22297, 328, 6938, 7996, 11063, 7963, 18297],\n",
       " '25679': [7297, 4466, 24891, 24667, 22652, 4585, 20460, 5019, 10405, 11063],\n",
       " '7791': [26336, 26670, 7944, 18979, 17839, 21053, 10749, 10992, 23118, 23303],\n",
       " '23111': [26336,\n",
       "  26670,\n",
       "  7944,\n",
       "  21053,\n",
       "  17839,\n",
       "  23118,\n",
       "  23495,\n",
       "  18979,\n",
       "  10749,\n",
       "  10992],\n",
       " '1396': [25831, 5221, 25827, 20592, 17215, 8832, 25830, 7611, 12900, 25829],\n",
       " '1348': [5221, 20592, 25831, 8832, 25827, 12963, 7604, 22553, 7173, 7611]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_course_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab02.json', 'w') as fout:\n",
    "    fout.write(json.dumps(target_course_reqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
