## Лаба 2. Content-based рекомендательная система образовательных курсов – Spark Dataframes

#### Дедлайн

Понедельник,  08 марта, 23:59:59.

#### Дедлайн Github

Четверг, 11 марта, 23:59:59.

#### Задача

По имеющимся данным портала eclass.cc построить content-based рекомендации по образовательным курсам. Запрещено использовать библиотеки pandas, sklearn и аналогичные.

#### Описание данных

Имеются следующие данные на вход:

* набор данных о всех курсах. Датасет можно взять с HDFS по адресу: `/labs/slaba02/DO_record_per_line.json`
* id курсов, для которых надо дать рекомендации (указаны в [Личном кабинете](https://lk-spark.newprolab.com/lab/slaba02)).

Данные выглядят следующим образом:

```json
{"lang": "en", 
"name": "Accounting Cycle: The Foundation of Business Measurement and Reporting", 
"cat": "3/business_management|6/economics_finance", 
"provider": "Canvas Network", 
"id": 4, 
"desc": "This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\u2019ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate."}
```

#### Результат

Для каждого id курса из личного кабинета необходимо дать топ-10 наиболее похожих на него курсов. Рекомендованные курсы должны быть того же языка, что и курс, для которого строится рекомендация.

Выходной формат — json — должен иметь следующую структуру:

```
{
  "123": [
    5372,
    16663,
    23114,
    13079,
    13084,
    ...
  ],
  "456": [
    ...
  ],
  "789": [
    ...
  ],
  "123456": [
    ...
  ],
  "456789": [
    ...
  ],
  "987654": [
    ...
  ]
}
```

Ключи json — это id курсов, для которых строится рекомендация. Для каждого такого ключа в качестве значения задается массив рекомендованных курсов, состоящий из их id, отсортированных по убыванию метрики. При равенстве значений метрики курсы сортируются лексикографически по названию.

Также возможна очень редкая ситуация (в основном с русскоязычными курсами), когда в рекомендацию попадут два дубликата одного курса, но с разными id. Таких дубликатов очень мало относительно числа курсов, но все равно рекомендуется сортировать в следующей последовательности: по метрике (убывание) => по названию (лексикографически по возрастанию) => по возрастанию id.

Также вы можете найти так называемый submission-файл по следующему пути на мастер-ноде: `/share/submission-files/slaba02/lab02.json`. Он обладает правильной структурой и форматом, но неправильными значениями. Идея в том, что он проходит необходимые требования чекера именно по структуре, и вам не придется пытаться понять, в чем дело, почему чекер не принимает ваш файл.

#### Советы

Для подбора рекомендаций следует использовать меру TFIDF, а в качестве метрики для ранжирования — косинус угла между TFIDF-векторами для разных курсов.

Что такое TFIDF? TF — это term frequency: по сути, сколько раз слово встречается в этом документе. Если мы сделаем такой word count по каждому документу, то получим вектор, который как-то характеризует этот документ. 

```
мама - 2
мыла - 1
раму - 1
лапу - 1
роза - 2
упала - 3
```

Если мы сравним вектора, рассчитав дистанцую между ними, то получим, насколько похожи эти тексты. Назовем этот подход наивным.

Этот подход наивен, потому что мы как бы присваиваем одинаковый вес каждому слову, которое у нас есть в тексте. А что если мы попробуем как-то повысить значимость тех слов, которые часто встречаются только в этом тексте? Для этого мы посчитаем DF – document frequency: по сути, число документов, в которых есть вхождение этого слова. Мы хотим "штрафовать" слово за частое появление в документах, поэтому делаем инверсию этой величины – буква I в TFIDF. Теперь для каждого слова мы будем считать TF и делить на IDF. Так мы получим другой вектор для нашего документа. Он может быть более правильным для наших задач.

TFIDF нужно считать для описаний курсов (desc). При извлечении слов из описания словом считаем то, что состоит из латинских или кириллических букв или цифр, знаки препинания и прочие символы не учитываются.

Для поиска слов можно использовать такой код на Python (может быть проблема с распознаванием юникода). 

```
regex = re.compile(u'[\w\d]{2,}', re.U)
regex.findall(string.lower())
```

Сам TFIDF реализован в Spark, писать с нуля вычисления не требуется. При вычислении TF с помощью `HashingTF` использовалось число фичей: 10000. То есть: `tf = HashingTF(10000)`.

#### Проверка

Проверка осуществляется по результатам рекомендаций текущей рекомендательной системы на eclass.cc. Для прохождения лабораторной для каждого курса, для которого строится рекомендация, должно быть пересечение рекомендованных курсов с результатами текущей системы — **не менее 20%**.

Файл необходимо положить в свою домашнюю директорию под названием: `lab02.json`. Проверка осуществляется со страницы лабы в личном кабинете. В чекере в качестве значений для курсов указаны id и доля пересечения конкретно для каждого из курсов.

Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack.
