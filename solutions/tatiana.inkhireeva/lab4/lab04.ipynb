{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab004\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.parse\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, CountVectorizer, Normalizer, StandardScaler, IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', sep='\\t', header=True)\n",
    "\n",
    "user_schema = spark.read.json(df_train.rdd.map(lambda row: row.user_json)).schema\n",
    "\n",
    "df_train = df_train.withColumn('user_json', f.from_json('user_json', user_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.filter((f.col('gender') != '-') & (f.col('age') != '-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn('target', f.concat(f.col('gender'),f.lit('_'), f.col('age')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(t.FloatType())\n",
    "def get_hour_avg(row):\n",
    "    return float(np.mean([datetime.fromtimestamp(ts // 1000).hour for ts in row]))\n",
    "\n",
    "@f.udf(t.IntegerType())\n",
    "def get_hour_mode(row):\n",
    "    return float(stats.mode([datetime.fromtimestamp(ts // 1000).hour for ts in row])[0][0])\n",
    "\n",
    "@f.udf(t.ArrayType(t.IntegerType()))\n",
    "def get_hours(row):\n",
    "    return [datetime.fromtimestamp(ts // 1000).hour for ts in row]\n",
    "\n",
    "@f.udf(t.ArrayType(t.StringType()))\n",
    "def get_netloc(row):\n",
    "    return [urllib.parse.urlparse(url).netloc for url in row]\n",
    "\n",
    "@f.udf(t.IntegerType())\n",
    "def get_morning(row):\n",
    "    return sum([1 if (hour>=7)&(hour<=11) else 0 for hour in row])\n",
    "\n",
    "@f.udf(t.IntegerType())\n",
    "def get_day(row):\n",
    "    return sum([1 if (hour>=12)&(hour<=18) else 0 for hour in row])\n",
    "\n",
    "@f.udf(t.IntegerType())\n",
    "def get_evening(row):\n",
    "    return sum([1 if (hour>=19)&(hour<=23) else 0 for hour in row])\n",
    "\n",
    "@f.udf(t.IntegerType())\n",
    "def get_night(row):\n",
    "    return sum([1 if (hour>=0)&(hour<=6) else 0 for hour in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn('hour', get_hours(df_train.user_json.visits.timestamp))\n",
    "df_train = df_train.withColumn('hour_avg', get_hour_avg(df_train.user_json.visits.timestamp))\n",
    "df_train = df_train.withColumn('hour_mode', get_hour_mode(df_train.user_json.visits.timestamp))\n",
    "df_train = df_train.withColumn('visit_cnt', f.size(df_train.user_json.visits))\n",
    "df_train = df_train.withColumn('site', get_netloc(df_train.user_json.visits.url))\n",
    "df_train = df_train.withColumn('morning', get_morning(df_train.hour))\\\n",
    ".withColumn('day', get_day(df_train.hour))\\\n",
    ".withColumn('evening', get_evening(df_train.hour))\\\n",
    ".withColumn('night', get_night(df_train.hour))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = df_train.groupby('target').count().withColumn('target_ratio', f.col('count') / 36138).toPandas()\n",
    "\n",
    "tmp = tmp[['target', 'target_ratio']].to_dict('split')['data']\n",
    "\n",
    "tmp = dict(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.sampleBy(\"target\", fractions=tmp, seed=5757).persist()\n",
    "valid = df_train.join(train, on=[\"uid\"], how=\"leftanti\")\n",
    "\n",
    "train = train.na.fill(0)\n",
    "valid = valid.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_cnt = CountVectorizer(inputCol='site', outputCol='site_cnt')\n",
    "site_cnt_model = site_cnt.fit(train)\n",
    "train = site_cnt_model.transform(train)\n",
    "valid = site_cnt_model.transform(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['visit_cnt', 'site_cnt', 'hour_avg', 'hour_mode', 'morning', 'day', 'evening', 'night']\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "train_data = assembler.transform(train).persist()\n",
    "valid_data = assembler.transform(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indxr = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "target2num = target_indxr.fit(train_data)\n",
    "train_data = target2num.transform(train_data)\n",
    "valid_data = target2num.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler(inputCol='features', outputCol='features_scaled', withMean=True)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='label')\n",
    "logreg = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "pipeline_a = Pipeline(stages=[ \n",
    "    std_scaler,\n",
    "    logreg\n",
    "])\n",
    "\n",
    "evaluator_a = MulticlassClassificationEvaluator(labelCol=\"label\", metricName='accuracy')\n",
    "\n",
    "paramGrid_a = ParamGridBuilder().addGrid(logreg.regParam, [0.3, 0.04])\\\n",
    "                              .addGrid(logreg.elasticNetParam, [0.8, 0.1])\\\n",
    "                              .build()\n",
    "\n",
    "crossval_a = CrossValidator(estimator=pipeline_a, estimatorParamMaps=paramGrid_a,\n",
    "                              evaluator=evaluator_a, numFolds=3, parallelism=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_model_a = crossval.fit(train_data)\n",
    "\n",
    "# cv_model_a.avgMetrics\n",
    "\n",
    "# cv_model_a.getEstimatorParamMaps()[np.argmax(cv_model_a.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_model = std_scaler.fit(train_data)\n",
    "# train_data = std_model.transform(train_data)\n",
    "\n",
    "# valid_data = std_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = logreg.fit(train_data)\n",
    "predictions_valid = logreg_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator_a.evaluate(predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model.write().overwrite().save('logreg_4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(train_data)\n",
    "predictions_valid = rf_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator_a.evaluate(predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model_g.bestModel.write().overwrite().save('model_4.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2target = IndexToString(inputCol=\"prediction\", outputCol=\"prediction_labels\", labels=target2num.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\" ,\n",
    "    \"subscribe\": \"input_tatiana.inkhireeva\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"failOnDataLoss\": \"False\"\n",
    "}\n",
    "\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\" ,\n",
    "   \"topic\": \"tatiana.inkhireeva\"\n",
    "        }\n",
    "    \n",
    "    df_parsed = (\n",
    "    df\n",
    "    .withColumn(\n",
    "        'value',\n",
    "        f.col(\"value\").cast(\"string\").alias(\"value\"),\n",
    "    )\n",
    "    .select(\n",
    "        'timestamp',\n",
    "        f.json_tuple(f.col(\"value\"), \"uid\", \"visits\")\n",
    "        .alias(\"uid\", \"visits\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        'uid',\n",
    "        f.col('uid').cast(t.StringType())\n",
    "    )\n",
    "    )\n",
    "    \n",
    "    df_parsed = df_parsed.withColumn('user_json', f.from_json('user_json', user_schema)) \\\n",
    "        .withColumn('hour', get_hours(df_parsed.user_json.visits.timestamp)) \\\n",
    "        .withColumn('hour_avg', get_hour_avg(df_parsed.user_json.visits.timestamp)) \\\n",
    "        .withColumn('hour_mode', get_hour_mode(df_parsed.user_json.visits.timestamp)) \\\n",
    "        .withColumn('visit_cnt', f.size(df_parsed.user_json.visits)) \\\n",
    "        .withColumn('site', get_netloc(df_parsed.user_json.visits.url)) \\\n",
    "        .withColumn('morning', get_morning(df_parsed.hour)) \\\n",
    "        .withColumn('day', get_day(df_parsed.hour)) \\\n",
    "        .withColumn('evening', get_evening(df_parsed.hour)) \\\n",
    "        .withColumn('night', get_night(df_parsed.hour))\n",
    "\n",
    "    df_parsed = df_parsed.na.fill(0)\n",
    "    df_parsed = site_cnt_model.transform(df_parsed)\n",
    "    df_parsed = assembler.transform(df_parsed)\n",
    "\n",
    "    df_parsed = target2num.transform(df_parsed)\n",
    "\n",
    "    df_parsed = logreg_model.transform(df_parsed)\n",
    "\n",
    "    df_parsed = num2target.transform(df_parsed)\n",
    "    \n",
    "    split_col = f.split(df_parsed['prediction_labels'], '_')\n",
    "    df_parsed = df_parsed.withColumn('gender', split_col.getItem(0))\n",
    "    df_parsed = df_parsed.withColumn('age', split_col.getItem(1))\n",
    "    \n",
    "    df_parsed = df_parsed.select('uid', 'gender', 'age')\n",
    "\n",
    "    df_parsed = df_parsed.withColumn('gender', f.lit('M'))\n",
    "    df_parsed = df_parsed.withColumn('age', f.lit('25-34'))\n",
    "    df = df_parsed.select(\n",
    "        f.to_json(f.struct(['uid', 'gender', 'age'])).alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df.write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .options(**write_kafka_params) \\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\" ,\n",
    "   \"topic\": \"tatiana.inkhireeva\"\n",
    "        }\n",
    "    \n",
    "    df.write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .options(**write_kafka_params) \\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\" ,\n",
    "   \"topic\": \"tatiana.inkhireeva\"\n",
    "        }\n",
    "    \n",
    "    df_parsed = df.withColumn('value', f.col(\"value\").cast(\"string\").alias(\"value\"),)\\\n",
    "    .select('timestamp',f.json_tuple(f.col(\"value\"), \"uid\", \"visits\").alias(\"uid\", \"visits\"))/\n",
    "    .withColumn('uid', f.col('uid').cast(t.StringType()))\\\n",
    "    \n",
    "    df.write \\\n",
    "            .format(\"kafka\") \\\n",
    "            .options(**write_kafka_params) \\\n",
    "            .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fcebc326668>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sink(df):\n",
    "    return df.writeStream \\\n",
    "             .foreachBatch(foreach_batch_function) \\\n",
    "             .outputMode('append') \\\n",
    "             .option(\"checkpointLocation\", \"streaming/chk/chk_kafka_inkhireeva_tatiana_lab04\")\n",
    "\n",
    "sink = create_sink(kafka_sdf)\n",
    "\n",
    "sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_tatiana.inkhireeva]]\n"
     ]
    }
   ],
   "source": [
    "kill_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
