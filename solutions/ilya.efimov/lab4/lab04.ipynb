{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming\n",
    "\n",
    "#### Дедлайн\n",
    "\n",
    "Понедельник, 22 марта, 23:59.\n",
    "\n",
    "#### Дедлайн Github\n",
    "\n",
    "Четверг, 25 марта, 23:59\n",
    "\n",
    "#### Задача\n",
    "\n",
    "У вас имеются данные: логи посещения пользователей по разным сайтам рунета. По этим пользователям вам также известны: пол и возрастная категория. Вам нужно будет в real-time спрогнозировать эти характеристики по пользователям, о которых у вас нет этой информации, но будут все те же самые логи посещения.\n",
    "\n",
    "##### При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и прочие.\n",
    "\n",
    "#### Описание данных\n",
    "\n",
    "Обучающая выборка, с которой вы будете работать, выглядит следующим образом:\n",
    "\n",
    "```json\n",
    "gender\tage\tuid\tuser_json\n",
    "F\t18-24\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}\n",
    "```\n",
    "\n",
    "Она расположена на hdfs: `/labs/slaba04/`.\n",
    "\n",
    "Поле `gender` принимает значения `F` (женщина) и `M` (мужчина).\n",
    "\n",
    "Поле `age` принимает значения диапазона возраста: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`\n",
    "\n",
    "Поле `uid` принимает значения уникального ID пользователя (cookies): `d50192e5-c44e-4ae8-ae7a-7cfe67c8b777`.\n",
    "\n",
    "Поле `user_json` имеет внутри json со следующей схемой данных: `{\"visits\": [{\"url\": \"url1\", \"timestamp\": \"timestamp1\"}, {\"url\": \"url2\", \"timestamp\": \"timestamp2\"}]}`. В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения.\n",
    "\n",
    "Данные, по которым вам надо будет построить прогноз (тестовая выборка), хранятся в Kafka в таком виде:\n",
    "\n",
    "```json\n",
    "{\"uid\": \"bdd48781-8243-493d-8ae6-794050a4417f\",\n",
    " \"visits\": \"[{\"url\": \"http://vk.com/feed\", \"timestamp\": 1425077901001}, {\"url\": \"http://big-cards.a5ltd.com/app/html/vk.html?api_url=http://api.vk.com/api.php&api_id=1804162&api_settings=663823&viewer_id=40849488&viewer_type=2&sid=1b77e9ca975573fed6e31746ca58ea2509f3588d918e7dab2a291f7f7579ae7dab6c21058c837b74b1fbb&secret=37339dfcbf&access_token=06c53bd8f9f9a0afad23b4e166e448495396259fc5a1c5e5243474df82591bd5d0afe877d993ea4a82a81&user_id=40849488&group_id=0&is_app_user=1&auth_key=d7c91ad4bb27e20cb3ed81810f5649db&language=0&parent_language=0&ad_info=elsdcqvcsvftaqxtawjsxht b0q8htjxuvbbjrvbnwojfji2ha8h&is_secure=0&ads_app_id=1804162_903aa103fa48b1e378&referrer=menu&lc_name=2c073eed&hash=\", \"timestamp\": 1425077901000}]\"}\n",
    "```\n",
    "\n",
    "То есть все то же самое, только без поля `gender` и `age` и немного с другой схемой данных. Название топика, в который мы будем присылать данные: `input_ivan.ivanov`, где вместо `ivan.ivanov` ваш логин от личного кабинета. Данные вам будут поступать по нажатию кнопки \"Проверить\" в личном кабинете. Вам топик создавать не нужно. Он будет автоматом создаваться при записи. Чистить его тоже не надо, потому что Spark Streaming хранит оффсеты, то есть знает, что он уже считывал, что еще не считывал. \n",
    "\n",
    "Порт брокера Kafka: `6667`. Hostname: `spark-master-1.newprolab.com` или `spark-node-1.newprolab.com`.\n",
    "\n",
    "#### Результат\n",
    "\n",
    "Вам в свою очередь нужно будет, считывая данные из Kafka, делать прогноз по ним в Spark Streaming и отправлять в real-time эти прогнозные значения в свой другой топик в Kafka в формате:\n",
    "\n",
    "```json\n",
    "{\"uid\": \"fe1dba8f-3131-439f-9031-851c0da0f126\", \"gender\": \"M\", \"age\": \"25-34\"}\n",
    "{\"uid\": \"d50192e5-c44e-4ae8-ae7a-7cfe67c8b777\", \"gender\": \"F\", \"age\": \"18-24\"}\n",
    "```\n",
    "\n",
    "Название топика — ваш логин в личном кабинете: `ivan.ivanov`. После отправки данных чекер будет ждать 90 секунд, чтобы вы обработали входные данные из топика `input_ivan.ivanov` и записали результаты прогноза уже в этот топик. \n",
    "\n",
    "#### Подсказки\n",
    "\n",
    "Чтение из Kafka:\n",
    "\n",
    "```python\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ivan.ivanov\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "```\n",
    "\n",
    "Запись в Kafka:\n",
    "\n",
    "```python\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ivan.ivanov\"\n",
    "}\n",
    "batch_df.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()\n",
    "```\n",
    "  \n",
    "Бывают проблемы при чтении старых данных, которых уже нет в кафке:\n",
    "  \n",
    "```\n",
    ".option(\"failOnDataLoss\", 'False')\n",
    "```\n",
    "  \n",
    "#### Проверка\n",
    "Проверка лабы устроена следующим образом:\n",
    "1. Вы готовите свою модель\n",
    "2. Настраиваете чтение данных из одного топика и запись в другой топик.\n",
    "4. Запускаете свой скрипт в стриминговым режиме\n",
    "3. Идет подключение к топику на чтение данных.\n",
    "4. На странице лабы нажимаете кнопку \"Проверить\".\n",
    "5. Вам в топик текут данные.\n",
    "6. Модель обрабатывает входные данные.\n",
    "7. Ваш скрипт сохраняет данные в другой топик.\n",
    "8. Чекер на странице лабы выдает результат проверки.\n",
    "\n",
    "\n",
    "Проверка осуществляется автоматическим скриптом на странице лабы в личном кабинете. Наш чекер в конечном итоге подключится к вашему финальному топику в Kafka и сравнит ваш прогноз с эталонным ответом. На выходе вы получите свой score. В качестве него метрика `accuracy`: доля пользователей от общего числа, по которым вы верно угадали и пол, и возрастную категорию. Если `accuracy` будет больше **0.25**, то лаба будет засчитана.\n",
    "\n",
    "##### Особенности проверки\n",
    "1. Проверка (генерация событий, обработка вашим скриптом, чтение и проверка результатов) занимают до 90 секунд, просьба ожидать (можно нажимать кнопку \"обновить\")\n",
    "2. За один раз (одно нажание \"Проверить\") чекер присылает батч в 5000 событий. Если для отладки / повторной проверки требуется еще, надо нажимать снова\n",
    "\n",
    "####  Рекомендации по прохождению\n",
    "1. Разработанную модель (и вообще весь пайплайн) можно сохранить в HDFS и потом загрузить уже в стриминговом скрипте (методы save/load - см.документацию)\n",
    "2. Настоятельно рекомендуется сначала отладить весь процесс чтения данных, предсказания и сохранения результатов в **батчевом** режиме (сгенерить порцию событий чекером и потом считать ее в обычный DataFrame через  `spark.read.format(\"kafka\")`. И уже только после проверки полной  работоспособсности всего скрипта и корректности формата выходных данных в Кафке, переписать на стриминговый режим (благодаря единому API Spark, это не потребует больших изменений)\n",
    "\n",
    "Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 10 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Efimov Ilya lab4\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType \n",
    "from pyspark.sql.types import TimestampType, LongType, FloatType, ArrayType\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Model, Estimator\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml.param.shared import HasFeaturesCol, HasLabelCol, HasPredictionCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import IDFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender\\tage\\tuid\\tuser_json',\n",
       " 'F\\t18-24\\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile(\"/labs/slaba04/gender_age_dataset.txt\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(IntegerType())\n",
    "def encode_gender(domains):\n",
    "    mapping = {\n",
    "        '-': 0,\n",
    "        'F': 1,\n",
    "        'M': 2,\n",
    "    }\n",
    "    return domains.apply(lambda x: mapping.get(x))\n",
    "@f.pandas_udf(StringType())\n",
    "def dencode_gender(domains):\n",
    "    mapping = {\n",
    "        0:'-',\n",
    "        1:'F',\n",
    "        2:'M',\n",
    "    }\n",
    "    return domains.apply(lambda x: mapping.get(x))\n",
    "@f.pandas_udf(IntegerType())\n",
    "def encode_age(domains):\n",
    "    mapping = {\n",
    "        '-': 0,\n",
    "        '45-54': 1,\n",
    "        '35-44': 2,\n",
    "        '25-34': 3,\n",
    "        '18-24': 4,\n",
    "        '>=55': 5,\n",
    "    }\n",
    "    return domains.apply(lambda x: mapping.get(x))\n",
    "@f.pandas_udf(StringType())\n",
    "def dencode_age(domains):\n",
    "    mapping = {\n",
    "        0:'-',\n",
    "        1:'45-54',\n",
    "        2:'35-44',\n",
    "        3:'25-34',\n",
    "        4:'18-24',\n",
    "        5:'>=55',\n",
    "    }\n",
    "    return domains.apply(lambda x: mapping.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------------+--------------------+\n",
      "|gender|age|                 uid|           user_json|\n",
      "+------+---+--------------------+--------------------+\n",
      "|     1|  4|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     2|  3|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "+------+---+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"gender\", StringType()), # принимает значения F (женщина) и M (мужчина)\n",
    "    StructField(\"age\", StringType()), # принимает значения диапазона возраста: 18-24, 25-34, 35-44, 45-54, >=55\n",
    "    StructField(\"uid\", StringType()), #uid принимает значения уникального ID пользователя (cookies)\n",
    "    StructField(\"user_json\", StringType()) #содержатся непосредственно логи посещения пользователем страниц. \n",
    "])\n",
    "df = spark.read.csv(\"/labs/slaba04/gender_age_dataset.txt\", header=True, sep='\\t', schema=schema)\n",
    "df = df.withColumn(\"gender\", encode_gender(\"gender\"))\n",
    "df = df.withColumn(\"age\", encode_age(\"age\")).cache()\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.select(\"*\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp_visits_(x):\n",
    "    x=json.loads(x)\n",
    "    return np.ediff1d([i['timestamp'] for i in x['visits']]).mean()\n",
    "def parse_timestamp_visits_series(x):\n",
    "    return x.apply(parse_timestamp_visits_)\n",
    "def parse_url_visits_(x):\n",
    "    x=json.loads(x) \n",
    "    return \" \".join([urlparse(i['url'].replace(\"http://http://\",\"http://\")\n",
    "                               .replace(\"http://https://\",\"http://\")\n",
    "                               .replace(\"http://http://\",\"https://\"))\n",
    "                      .netloc.replace(\"www.\",\"\") for i in x['visits']])\n",
    "def parse_url_visits_series(x):\n",
    "    return x.apply(parse_url_visits_)\n",
    "parse_timestamp_visits = f.pandas_udf(parse_timestamp_visits_series,\"double\")\n",
    "parse_url_visits = f.pandas_udf(parse_url_visits_series,\"string\")\n",
    "# df_test[\"user_json\"].apply(parse_timestamp_visits_).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"meantime\", parse_timestamp_visits(\"user_json\")).cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"urlstring\", parse_url_visits(\"user_json\")).cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           urlstring|            features|\n",
      "+--------------------+--------------------+\n",
      "|zebra-zoya.ru new...|(2000,[943,1215,1...|\n",
      "|sweetrading.ru sw...|(2000,[38,52,63,7...|\n",
      "|ru.oriflame.com r...|(2000,[231,326,70...|\n",
      "|translate-tattoo....|(2000,[600,622,73...|\n",
      "|mail.rambler.ru n...|(2000,[105,184,22...|\n",
      "|cfire.mail.ru pet...|(2000,[392,444],[...|\n",
      "|msn.com msn.com m...|(2000,[402,425,82...|\n",
      "|gazprom.ru re-sto...|(2000,[172,609,70...|\n",
      "|lifenews.ru lifen...|(2000,[767,1252,1...|\n",
      "|google.ru films.i...|(2000,[163,568,58...|\n",
      "|muz4in.net smachn...|(2000,[22,277,307...|\n",
      "|kosmetista.ru kos...|(2000,[491,1157],...|\n",
      "|android.mobile-re...|(2000,[176,452,59...|\n",
      "|tsn.ua cfts.org.u...|(2000,[205,428,63...|\n",
      "|jobinmoscow.ru jo...|(2000,[54,498,872...|\n",
      "|      abc-people.com|(2000,[588],[4.80...|\n",
      "|easygames.biz eas...|(2000,[602],[9.31...|\n",
      "|ratanews.ru ratan...|(2000,[111,228,30...|\n",
      "|        sam-zdrav.ru|(2000,[1482],[5.8...|\n",
      "|     msn.com msn.com|(2000,[1416],[9.0...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"urlstring\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "df = idfModel.transform(featurizedData).cache() \n",
    "\n",
    "df.select(\"urlstring\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.write().overwrite().save(\"tokenizer_model\")\n",
    "hashingTF.write().overwrite().save(\"hashingTF_model\")\n",
    "idfModel.write().overwrite().save(\"idfModel_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier, RandomForestClassifier, RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = df.limit(20) \n",
    "# lr = NaiveBayes(labelCol=\"gender\", featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")\n",
    "# lr = GBTClassifier(labelCol=\"gender\", featuresCol=\"features\", maxIter=50, maxDepth=3) # 0.9003\n",
    "lr = RandomForestClassifier(labelCol=\"gender\", featuresCol=\"features\") \n",
    "model = lr.fit(df.where('gender !=0'))\n",
    "model.write().overwrite().save(\"gender_model\")\n",
    "predictions = model.transform(predictions)\n",
    "predictions = predictions.withColumn(\"gender2\", dencode_gender(\"prediction\"))\n",
    "\n",
    "columns_to_drop = ['rawPrediction', 'probability', 'prediction']\n",
    "predictions = predictions.drop(*columns_to_drop)\n",
    "\n",
    "# lr = NaiveBayes(labelCol=\"age\", featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")\n",
    "# lr = GBTClassifier(labelCol=\"age\", featuresCol=\"features\", maxIter=50, maxDepth=3) # 0.9003\n",
    "lr = RandomForestClassifier(labelCol=\"age\", featuresCol=\"features\") \n",
    "model = lr.fit(df.where('age !=0'))\n",
    "model.write().overwrite().save(\"age_model\")\n",
    "predictions = model.transform(predictions)\n",
    "predictions = predictions.withColumn(\"age2\", dencode_age(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age2</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>25-34</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   age2  gender gender2\n",
       "0    2  25-34       2       M\n",
       "1    3  25-34       2       M\n",
       "2    3  25-34       2       M\n",
       "3    3  25-34       2       M\n",
       "4    3  25-34       2       M\n",
       "5    3  25-34       2       M\n",
       "6    3  25-34       1       F\n",
       "7    3  25-34       2       M\n",
       "8    3  25-34       1       M\n",
       "9    4  25-34       1       M"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.toPandas()[[\"age\", \"age2\", \"gender\", \"gender2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load(\"tokenizer_model\")\n",
    "hashingTF = HashingTF.load(\"hashingTF_model\")\n",
    "idfModel = IDFModel.load(\"idfModel_model\")\n",
    "age_model = RandomForestClassificationModel.load(\"age_model\")\n",
    "gender_model = RandomForestClassificationModel.load(\"gender_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "INPUT_KAFKA_TOPIC = 'input_ilya.efimov'\n",
    "OUTPUT_KAFKA_TOPIC = 'ilya.efimov'\n",
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': INPUT_KAFKA_TOPIC,\n",
    "#     'startingOffsets': 'earliest', # убрать в стриме\n",
    "    \"startingOffsets\": \"latest\", #Добавить в стриме\n",
    "}\n",
    "# kafka_sdf = (\n",
    "#     spark\n",
    "#     .read\n",
    "#     .format('kafka')\n",
    "#     .options(**read_kafka_params)\n",
    "#     .load()\n",
    "#     .cache()\n",
    "# )\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "kafka_sdf.printSchema()\n",
    "# print('count',kafka_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = kafka_sdf.selectExpr(\"CAST(value AS STRING)\")#.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url_visits_(x):\n",
    "    \n",
    "    x = json.loads(x)\n",
    "    x = json.loads(x['visits'])\n",
    "#     return(str(x))\n",
    "#     try:\n",
    "    return \" \".join([urlparse(i['url'].replace(\"http://http://\",\"http://\")\n",
    "                               .replace(\"http://https://\",\"http://\")\n",
    "                               .replace(\"http://http://\",\"https://\"))\n",
    "                      .netloc.replace(\"www.\",\"\") for i in x])\n",
    "def parse_url_visits_series(x):\n",
    "    return x.apply(parse_url_visits_) \n",
    "parse_url_visits = f.pandas_udf(parse_url_visits_series,\"string\") \n",
    "\n",
    "kafka_sdf = kafka_sdf.withColumn(\"urlstring\", parse_url_visits(\"value\"))#.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData = tokenizer.transform(kafka_sdf)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "kafka_sdf = idfModel.transform(featurizedData) \n",
    "\n",
    "kafka_sdf = gender_model.transform(kafka_sdf)\n",
    "kafka_sdf = kafka_sdf.withColumn(\"gender\", dencode_gender(\"prediction\"))\n",
    "\n",
    "columns_to_drop = ['rawPrediction', 'probability', 'prediction']\n",
    "kafka_sdf = kafka_sdf.drop(*columns_to_drop)\n",
    "\n",
    "kafka_sdf = age_model.transform(kafka_sdf)\n",
    "kafka_sdf = kafka_sdf.withColumn(\"age\", dencode_age(\"prediction\"))\n",
    "\n",
    "columns_to_drop = ['rawPrediction', 'probability', 'prediction']\n",
    "kafka_sdf = kafka_sdf.drop(*columns_to_drop)#.cache() \n",
    "# kafka_sdf.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_uid_(x):\n",
    "    x = json.loads(x)\n",
    "    return x['uid']\n",
    "\n",
    "def parse_uid_series(x):\n",
    "    return x.apply(parse_uid_) \n",
    "parse_uid = f.pandas_udf(parse_uid_series,\"string\") \n",
    "\n",
    "kafka_sdf = kafka_sdf.withColumn(\"uid\", parse_uid(\"value\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['value', 'urlstring', 'words', 'rawFeatures', 'features']\n",
    "kafka_sdf = kafka_sdf.drop(*columns_to_drop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ilya.efimov\"\n",
    "}\n",
    "\n",
    "doc = to_json(struct(*[col(c) for c in kafka_sdf.columns]))\n",
    "\n",
    "sq = kafka_sdf.select(doc.alias(\"value\")).writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option('checkpointLocation', 'streaming/chk/chk_kafka_ilya_efimov_lab04')\\\n",
    "    .outputMode(\"append\").start()\n",
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for next trigger',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, to_json\n",
    "# для обычного режима\n",
    "def write_kafka(topic, data):\n",
    "    kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ilya.efimov\"\n",
    "}\n",
    "    kafka_doc = to_json(struct(*[col(c) for c in data.columns]))\n",
    "    data.select(kafka_doc.alias(\"value\")) \\\n",
    "        .withColumn(\"topic\", f.lit(kafka_params[\"topic\"])) \\\n",
    "        .write.format(\"kafka\") \\\n",
    "        .options(**kafka_params)\\\n",
    "        .save()\n",
    "# write_kafka(\"test_topic0\", kafka_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_ilya.efimov]]\n"
     ]
    }
   ],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))\n",
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 - 0.2426, all - 0.197, 20000 - 0.1832, 1000 - 0.2454, 700 - 0.2444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим наличие данных в целевом топике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "count 0\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "INPUT_KAFKA_TOPIC = 'input_ilya.efimov'\n",
    "OUTPUT_KAFKA_TOPIC = 'ilya.efimov'\n",
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': OUTPUT_KAFKA_TOPIC,\n",
    "#     'startingOffsets': 'earliest', # убрать в стриме\n",
    "#     \"startingOffsets\": \"latest\", #Заменить в стриме\n",
    "}\n",
    "sdf2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .load()\n",
    "    .cache()\n",
    ") \n",
    "\n",
    "sdf2.printSchema()\n",
    "print('count',sdf2.count())\n",
    "sdf2.selectExpr(\"CAST(value AS STRING)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "40868"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
