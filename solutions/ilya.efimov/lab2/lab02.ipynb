{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 2. Content-based рекомендательная система образовательных курсов – Spark Dataframes\n",
    "\n",
    "#### Дедлайн\n",
    "\n",
    "Понедельник,  08 марта, 23:59:59.\n",
    "\n",
    "#### Дедлайн Github\n",
    "\n",
    "Четверг, 11 марта, 23:59:59.\n",
    "\n",
    "#### Задача\n",
    "\n",
    "По имеющимся данным портала eclass.cc построить content-based рекомендации по образовательным курсам. Запрещено использовать библиотеки pandas, sklearn и аналогичные.\n",
    "\n",
    "#### Описание данных\n",
    "\n",
    "Имеются следующие данные на вход:\n",
    "\n",
    "* набор данных о всех курсах. Датасет можно взять с HDFS по адресу: `/labs/slaba02/DO_record_per_line.json`\n",
    "* id курсов, для которых надо дать рекомендации (указаны в [Личном кабинете](https://lk-spark.newprolab.com/lab/slaba02)).\n",
    "\n",
    "Данные выглядят следующим образом:\n",
    "\n",
    "```json\n",
    "{\"lang\": \"en\", \n",
    "\"name\": \"Accounting Cycle: The Foundation of Business Measurement and Reporting\", \n",
    "\"cat\": \"3/business_management|6/economics_finance\", \n",
    "\"provider\": \"Canvas Network\", \n",
    "\"id\": 4, \n",
    "\"desc\": \"This course introduces the basic financial statements used by most businesses, as well as the essential tools used to prepare them. This course will serve as a resource to help business students succeed in their upcoming university-level accounting classes, and as a refresher for upper division accounting students who are struggling to recall elementary concepts essential to more advanced accounting topics. Business owners will also benefit from this class by gaining essential skills necessary to organize and manage information pertinent to operating their business. At the conclusion of the class, students will understand the balance sheet, income statement, and cash flow statement. They will be able to differentiate between cash basis and accrual basis techniques, and know when each is appropriate. They\\u2019ll also understand the accounting equation, how to journalize and post transactions, how to adjust and close accounts, and how to prepare key financial reports. All material for this class is written and delivered by the professor, and can be previewed here. Students must have access to a spreadsheet program to participate.\"}\n",
    "```\n",
    "\n",
    "#### Результат\n",
    "\n",
    "Для каждого id курса из личного кабинета необходимо дать топ-10 наиболее похожих на него курсов. Рекомендованные курсы должны быть того же языка, что и курс, для которого строится рекомендация.\n",
    "\n",
    "Выходной формат — json — должен иметь следующую структуру:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"123\": [\n",
    "    5372,\n",
    "    16663,\n",
    "    23114,\n",
    "    13079,\n",
    "    13084,\n",
    "    ...\n",
    "  ],\n",
    "  \"456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"123456\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"456789\": [\n",
    "    ...\n",
    "  ],\n",
    "  \"987654\": [\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Ключи json — это id курсов, для которых строится рекомендация. Для каждого такого ключа в качестве значения задается массив рекомендованных курсов, состоящий из их id, отсортированных по убыванию метрики. При равенстве значений метрики курсы сортируются лексикографически по названию.\n",
    "\n",
    "Также возможна очень редкая ситуация (в основном с русскоязычными курсами), когда в рекомендацию попадут два дубликата одного курса, но с разными id. Таких дубликатов очень мало относительно числа курсов, но все равно рекомендуется сортировать в следующей последовательности: по метрике (убывание) => по названию (лексикографически по возрастанию) => по возрастанию id.\n",
    "\n",
    "Также вы можете найти так называемый submission-файл по следующему пути на мастер-ноде: `/share/submission-files/slaba02/lab02.json`. Он обладает правильной структурой и форматом, но неправильными значениями. Идея в том, что он проходит необходимые требования чекера именно по структуре, и вам не придется пытаться понять, в чем дело, почему чекер не принимает ваш файл.\n",
    "\n",
    "#### Советы\n",
    "\n",
    "Для подбора рекомендаций следует использовать меру TFIDF, а в качестве метрики для ранжирования — косинус угла между TFIDF-векторами для разных курсов.\n",
    "\n",
    "Что такое TFIDF? TF — это term frequency: по сути, сколько раз слово встречается в этом документе. Если мы сделаем такой word count по каждому документу, то получим вектор, который как-то характеризует этот документ. \n",
    "\n",
    "```\n",
    "мама - 2\n",
    "мыла - 1\n",
    "раму - 1\n",
    "лапу - 1\n",
    "роза - 2\n",
    "упала - 3\n",
    "```\n",
    "\n",
    "Если мы сравним вектора, рассчитав дистанцую между ними, то получим, насколько похожи эти тексты. Назовем этот подход наивным.\n",
    "\n",
    "Этот подход наивен, потому что мы как бы присваиваем одинаковый вес каждому слову, которое у нас есть в тексте. А что если мы попробуем как-то повысить значимость тех слов, которые часто встречаются только в этом тексте? Для этого мы посчитаем DF – document frequency: по сути, число документов, в которых есть вхождение этого слова. Мы хотим \"штрафовать\" слово за частое появление в документах, поэтому делаем инверсию этой величины – буква I в TFIDF. Теперь для каждого слова мы будем считать TF и делить на IDF. Так мы получим другой вектор для нашего документа. Он может быть более правильным для наших задач.\n",
    "\n",
    "TFIDF нужно считать для описаний курсов (desc). При извлечении слов из описания словом считаем то, что состоит из латинских или кириллических букв или цифр, знаки препинания и прочие символы не учитываются.\n",
    "\n",
    "Для поиска слов можно использовать такой код на Python (может быть проблема с распознаванием юникода). \n",
    "\n",
    "```\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "regex.findall(string.lower())\n",
    "```\n",
    "\n",
    "Сам TFIDF реализован в Spark, писать с нуля вычисления не требуется. При вычислении TF с помощью `HashingTF` использовалось число фичей: 10000. То есть: `tf = HashingTF(10000)`.\n",
    "\n",
    "#### Проверка\n",
    "\n",
    "Проверка осуществляется по результатам рекомендаций текущей рекомендательной системы на eclass.cc. Для прохождения лабораторной для каждого курса, для которого строится рекомендация, должно быть пересечение рекомендованных курсов с результатами текущей системы — **не менее 20%**.\n",
    "\n",
    "Файл необходимо положить в свою домашнюю директорию под названием: `lab02.json`. Проверка осуществляется со страницы лабы в личном кабинете. В чекере в качестве значений для курсов указаны id и доля пересечения конкретно для каждого из курсов.\n",
    "\n",
    "Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab02 Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('/labs/slaba02/DO_record_per_line.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Курсы, по которым нужно выдать решение\n",
    "given_courses =  [\n",
    " [23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    " [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    " [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    " [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    " [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    " [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[23126, 'en', 'Compass - powerful SASS library that makes your life easier'],\n",
       " [21617, 'en', 'Preparing for the AP* Computer Science A Exam — Part 2'],\n",
       " [16627, 'es', 'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'],\n",
       " [11556,\n",
       "  'es',\n",
       "  'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'],\n",
       " [16704, 'ru', 'Программирование на Lazarus'],\n",
       " [13702, 'ru', 'Математическая экономика']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_langs = [(course[0], course[1]) for course in given_courses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF + TFIDF + dot product + l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import re\n",
    "\n",
    "def clear_string(series):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    words = series.str.findall(regex)\n",
    "    return words\n",
    "\n",
    "tokenizer_udf = pandas_udf(clear_string, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = data.withColumn(\"words\", tokenizer_udf(\"desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"TFFeatures\", numFeatures=10000, binary=False)\n",
    "\n",
    "hashed_data = hashingTF.transform(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"TFFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashed_data)\n",
    "idfed_data = idfModel.transform(hashed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def vectorToArray(row):\n",
    "    return row.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 178 ms, total: 698 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Нормализация векторов L2, после этого для cosine_similarity будет достаточно \n",
    "# делать dot product нормализованных векторов\n",
    "from pyspark.ml.feature import Normalizer\n",
    "t = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "\n",
    "normalized_data = t.transform(idfed_data)\n",
    "# Для каждого курса посчитаем косинусное расстояние до всех остальных\n",
    "# выберем 10 самых похожих\n",
    "\n",
    "target_course_reqs = {}\n",
    "\n",
    "for course_id, lang in courses_langs:\n",
    "    course_vec = normalized_data.filter(normalized_data.id == int(course_id))\\\n",
    "             .collect()[0]['features'].toArray()\n",
    "    \n",
    "    cos_sim = f.udf(lambda x: float(x.dot(course_vec)), FloatType())\n",
    "    \n",
    "    recs = normalized_data.where((normalized_data.id != int(course_id)) & (normalized_data.lang == lang))\\\n",
    "               .withColumn('cosine_sim', cos_sim(normalized_data['features']))\\\n",
    "               .orderBy(f.desc('cosine_sim'), f.asc('name'), f.asc('id'))\\\n",
    "               .head(10)\n",
    "                           \n",
    "    list_out = [rec['id'] for rec in recs]\n",
    "    target_course_reqs.update({str(course_id): list_out})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'23126': [13665, 11063, 23257, 6938, 3919, 3819, 11616, 25782, 10950, 12465],\n",
       " '21617': [16971, 17221, 6776, 380, 7597, 19848, 22297, 116, 8110, 21081],\n",
       " '16627': [26336,\n",
       "  26670,\n",
       "  7944,\n",
       "  17839,\n",
       "  21053,\n",
       "  10749,\n",
       "  23303,\n",
       "  18979,\n",
       "  23118,\n",
       "  11575],\n",
       " '11556': [26336, 26670, 7944, 16929, 21053, 17839, 10749, 18979, 3660, 8098],\n",
       " '16704': [1247, 5221, 1234, 20319, 8203, 1365, 8217, 7604, 25830, 22553],\n",
       " '13702': [28074, 864, 28075, 25830, 22553, 19648, 5221, 7611, 7607, 8313]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_course_reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../lab02.json', 'w') as fout:\n",
    "    fout.write(json.dumps(target_course_reqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
