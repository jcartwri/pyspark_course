{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 3. Рекомендательная система видеоконтента с implicit feedback – Spark ML\n",
    "\n",
    "#### Дедлайн\n",
    "\n",
    "Понедельник, 15 марта, 23:59\n",
    "\n",
    "#### Дедлайн Github\n",
    "\n",
    "Четверг, 18 марта, 23:59\n",
    "\n",
    "#### Задача\n",
    "\n",
    "В вашем распоряжении имеется уже предобработанный и очищенный датасет с фактами покупок абонентами телепередач от компании E-Contenta. По доступным вам данным нужно предсказать вероятность покупки других передач этими, а, возможно, и другими абонентами. \n",
    "При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и другие. \n",
    "Если scikit-learn (например, но и другие тоже) обернут в классы Transformer и Estimator, то их можно использовать.\n",
    "\n",
    "#### Описание данных\n",
    "\n",
    "Для выполнения работы вам следует взять все файлы из папки на HDFS `/labs/slaba03/`. \n",
    "\n",
    "Давайте посмотрим, что у нас есть:\n",
    "\n",
    "```\n",
    "$ hdfs dfs -ls /labs/slaba03/\n",
    "Found 4 items\n",
    "-rw-r--r--   3 hdfs hdfs   91066524 2019-03-17 21:07 /labs/slaba03/laba03_items.csv\n",
    "-rw-r--r--   3 hdfs hdfs   29965581 2019-03-17 21:07 /labs/slaba03/laba03_test.csv\n",
    "-rw-r--r--   3 hdfs hdfs   74949368 2019-03-17 21:07 /labs/slaba03/laba03_train.csv\n",
    "-rw-r--r--   3 hdfs hdfs  871302535 2019-03-17 21:07 /labs/slaba03/laba03_views_programmes.csv\n",
    "```\n",
    "\n",
    "- В `laba03_train.csv` содержатся факты покупки (колонка `purchase`) пользователями (колонка `user_id`) телепередач (колонка `item_id`). Такой формат файла вам уже знаком.\n",
    "\n",
    "- `laba03_items.csv` — дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "\n",
    "  - `item_id` — primary key. Соответствует `item_id` в предыдущем файле.\n",
    "  - `content_type` — тип телепередачи (`1` — платная, `0` — бесплатная). Вас интересуют платные передачи.\n",
    "  - `title` — название передачи, текстовое поле.\n",
    "  - `year` — год выпуска передачи, число.\n",
    "  - `genres` — поле с жанрами передачи, разделёнными через запятую.\n",
    "\n",
    "- `laba03_test.csv` — тестовый датасет без указанного целевого признака `purchase`, который вам и предстоит предсказать.\n",
    "\n",
    "- Дополнительный файл `laba03_views_programmes.csv` по просмотрам передач с полями:\n",
    "\n",
    "  - `ts_start` — время начала просмотра.\n",
    "\n",
    "  - `ts_end` — время окончания просмотра.\n",
    "\n",
    "  - `item_type` — тип просматриваемого контента:\n",
    "\n",
    "    - `live` — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "    - `pvr` — просмотр в записи, после показа контента в эфире.\n",
    "\n",
    "#### Результат\n",
    "\n",
    "Предсказание целевой переменной \"купит/не купит\" — хорошо знакомая вам задача бинарной классификации. Поскольку нам важны именно вероятности отнесения пары `(пользователь, товар)` к классу \"купит\" (`1`), то, на самом деле, вы можете подойти к проблеме с разных сторон:\n",
    "\n",
    "1. Как к разработке рекомендательной системы: рекомендовать пользователю `user_id` топ-N лучших телепередач, которые были найдены по методике user-user / item-item коллаборативной фильтрации.\n",
    "2. Как к задаче факторизации матриц: алгоритмы SVD, ALS, FM/FFM.\n",
    "3. Как просто к задаче бинарной классификации. У вас есть два датасета, которые можно каким-то образом объединить, дополнительно обработать и сделать предсказания классификаторами (Spark ML).\n",
    "4. Как к задаче регрессии. Поскольку от вас требуется предсказать не факт покупки, а его *вероятность*, то можно перевести задачу в регрессионную и решать её соответствующим образом.\n",
    "\n",
    "#### Советы\n",
    "\n",
    "1. На качество прогноза в большей степени влияет _качество признаков_, которые вы сможете придумать из имеющихся данных, нежели выбор и _сложность алгоритма_.\n",
    "2. Качество входных данных также имеет сильное значени. Существует фраза \"garbage in – garbage out\". Мусор на входе – мусор на выходе. Потратьте время на подготовку и предобработку данных.\n",
    "Путь к успеху в третьей лабораторной:\n",
    "1. Сосредоточьтесь на формировании следующих фичей: по файлу \n",
    "laba03_train.csv сформируйте признаки, характеризирующие как интенсивно\n",
    "покупает пользователь и \"покупаемость\" item'ов\n",
    "2. возьмите достаточно мощную модель (например GBTRegressor из pyspark'а)\n",
    "\n",
    "#### Проверка\n",
    "\n",
    "Эта лаба проходит в формате соревнования. Для вас оно начинается, когда вы успешно пройдёте минимальный порог  — **AUC должен составить не менее 0.79**. После этого вы увидите лидерборд и сможете следить за результатами других участников.\n",
    "\n",
    "Как уже было сказано, мы будем оценивать ваш алгоритм по метрике ROC AUC. Чекеру требуются *вероятности* в диапазоне `[0.0, 1.0]` отнесения пары `(пользователь, товар)` в тестовой выборке к классу \"1\" (купит).\n",
    "\n",
    "**Важно!** Для точной проверки не забудьте отсортировать полученный файл по возрастанию идентификаторов пользователей (`user_id`), а затем — по возрастанию идентификаторов передач (`item_id`).\n",
    "```\n",
    ",user_id,item_id,purchase\n",
    "0,1654,336,0.021805684684958027\n",
    "1,1654,678,0.021805684684958027\n",
    "2,1654,691,0.021805684684958027\n",
    "3,1654,696,0.021805684684958027\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Результат следует сохранить в файл `lab03.csv` в своей домашней директории.\n",
    "\n",
    "Проверка осуществляется автоматическим скриптом из [Личного кабинета](http://lk-spark.newprolab.com/lab/slaba03).\n",
    "\n",
    "Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"natasha pritykovskaya Spark RDD app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType \n",
    "from pyspark.sql.types import TimestampType, LongType, FloatType, ArrayType\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Model, Estimator\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml.param.shared import HasFeaturesCol, HasLabelCol, HasPredictionCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laba03_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " user_id  | 1654  \n",
      " item_id  | 74107 \n",
      " purchase | 0.0   \n",
      "-RECORD 1---------\n",
      " user_id  | 1654  \n",
      " item_id  | 89249 \n",
      " purchase | 0.0   \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()), # cодержатся факты покупки\n",
    "    StructField(\"item_id\", IntegerType()), # пользователями\n",
    "    StructField(\"purchase\", FloatType()) #телепередач\n",
    "])\n",
    "df_train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", header=True, schema=schema).cache() \n",
    "df_train.show(2,False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laba03_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " user_id  | 1654  \n",
      " item_id  | 94814 \n",
      " purchase | null  \n",
      "-RECORD 1---------\n",
      " user_id  | 1654  \n",
      " item_id  | 93629 \n",
      " purchase | null  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()), # cодержатся факты покупки\n",
    "    StructField(\"item_id\", IntegerType()), # пользователями\n",
    "    StructField(\"purchase\", FloatType()) #телепередач\n",
    "])\n",
    "df_test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", header=True, schema=schema).cache()  \n",
    "df_test.show(2,False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------+\n",
      "|summary|           user_id|          item_id|purchase|\n",
      "+-------+------------------+-----------------+--------+\n",
      "|  count|           2156840|          2156840|       0|\n",
      "|   mean| 869652.3733920922|66896.00283609354|    null|\n",
      "| stddev|60706.516163349734|35227.83130704647|    null|\n",
      "|    min|              1654|              326|    null|\n",
      "|    25%|            846164|            65668|    null|\n",
      "|    50%|            885247|            79856|    null|\n",
      "|    75%|            908588|            93606|    null|\n",
      "|    max|            941450|           104165|    null|\n",
      "+-------+------------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_test.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laba03_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------\n",
      " item_id                     | 65667                                                    \n",
      " channel_id                  | null                                                     \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                      \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                      \n",
      " datetime_show_start         | null                                                     \n",
      " datetime_show_stop          | null                                                     \n",
      " content_type                | 1                                                        \n",
      " title                       | на пробах только девушки (all girl auditions)            \n",
      " year                        | 2013.0                                                   \n",
      " genres                      | Эротика                                                  \n",
      " region_id                   | null                                                     \n",
      "-RECORD 1-------------------------------------------------------------------------------\n",
      " item_id                     | 65669                                                    \n",
      " channel_id                  | null                                                     \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                      \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                      \n",
      " datetime_show_start         | null                                                     \n",
      " datetime_show_stop          | null                                                     \n",
      " content_type                | 1                                                        \n",
      " title                       | скуби ду: эротическая пародия (scooby doo: a xxx parody) \n",
      " year                        | 2011.0                                                   \n",
      " genres                      | Эротика                                                  \n",
      " region_id                   | null                                                     \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"item_id\", IntegerType()), # primary key, Соответствует item_id в предыдущем файле\n",
    "    StructField(\"channel_id\", FloatType()),\n",
    "    StructField(\"datetime_availability_start\", TimestampType()),\n",
    "    StructField(\"datetime_availability_stop\", TimestampType()),\n",
    "    StructField(\"datetime_show_start\", TimestampType()),\n",
    "    StructField(\"datetime_show_stop\", TimestampType()),\n",
    "    StructField(\"content_type\", IntegerType()), # — тип телепередачи (1 — платная, 0 — бесплатная).\n",
    "    StructField(\"title\", StringType()), # — название передачи, текстовое поле\n",
    "    StructField(\"year\", FloatType()), # — год выпуска передачи, число\n",
    "    StructField(\"genres\", StringType()),# genres — поле с жанрами передачи, разделёнными через запятую.\n",
    "    StructField(\"region_id\", FloatType())])\n",
    "\n",
    "df_items = spark.read.csv(\"/labs/slaba03/laba03_items.csv\", header=True, sep=\"\\t\", schema=schema).cache()  \n",
    "df_items.show(2,False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------\n",
      " summary      | count                                  \n",
      " item_id      | 635568                                 \n",
      " channel_id   | 631864                                 \n",
      " content_type | 635568                                 \n",
      " title        | 635568                                 \n",
      " year         | 3700                                   \n",
      " genres       | 635535                                 \n",
      " region_id    | 273304                                 \n",
      "-RECORD 1----------------------------------------------\n",
      " summary      | mean                                   \n",
      " item_id      | 6791742.332593838                      \n",
      " channel_id   | 1787.092608536014                      \n",
      " content_type | 0.005827857916068776                   \n",
      " title        | 675.2237762237762                      \n",
      " year         | 2005.0624324324324                     \n",
      " genres       | null                                   \n",
      " region_id    | 28.877857623744987                     \n",
      "-RECORD 2----------------------------------------------\n",
      " summary      | stddev                                 \n",
      " item_id      | 660968.3283445426                      \n",
      " channel_id   | 10148.139275424435                     \n",
      " content_type | 0.07611769245242281                    \n",
      " title        | 852.6240475572829                      \n",
      " year         | 15.207695883567666                     \n",
      " genres       | null                                   \n",
      " region_id    | 9.02763901499916                       \n",
      "-RECORD 3----------------------------------------------\n",
      " summary      | min                                    \n",
      " item_id      | 326                                    \n",
      " channel_id   | 5.0                                    \n",
      " content_type | 0                                      \n",
      " title        | \"\"\"90 лет досааф\"\". юбилейный концерт\" \n",
      " year         | 1916.0                                 \n",
      " genres       | General                                \n",
      " region_id    | 15.0                                   \n",
      "-RECORD 4----------------------------------------------\n",
      " summary      | 25%                                    \n",
      " item_id      | 6489422                                \n",
      " channel_id   | 13.0                                   \n",
      " content_type | 0                                      \n",
      " title        | 99.0                                   \n",
      " year         | 2006.0                                 \n",
      " genres       | null                                   \n",
      " region_id    | 22.0                                   \n",
      "-RECORD 5----------------------------------------------\n",
      " summary      | 50%                                    \n",
      " item_id      | 6758861                                \n",
      " channel_id   | 33.0                                   \n",
      " content_type | 0                                      \n",
      " title        | 99.0                                   \n",
      " year         | 2011.0                                 \n",
      " genres       | null                                   \n",
      " region_id    | 26.0                                   \n",
      "-RECORD 6----------------------------------------------\n",
      " summary      | 75%                                    \n",
      " item_id      | 7124678                                \n",
      " channel_id   | 116.0                                  \n",
      " content_type | 0                                      \n",
      " title        | 1984.0                                 \n",
      " year         | 2014.0                                 \n",
      " genres       | null                                   \n",
      " region_id    | 36.0                                   \n",
      "-RECORD 7----------------------------------------------\n",
      " summary      | max                                    \n",
      " item_id      | 7610659                                \n",
      " channel_id   | 100227.0                               \n",
      " content_type | 1                                      \n",
      " title        | ёлки-3                                 \n",
      " year         | 2017.0                                 \n",
      " genres       | Юмористические,Передачи,Детские        \n",
      " region_id    | 48.0                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.summary().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laba03_views_programmes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " user_id   | 0          \n",
      " item_id   | 7101053    \n",
      " ts_start  | 1491409931 \n",
      " ts_end    | 1491411600 \n",
      " item_type | live       \n",
      "-RECORD 1---------------\n",
      " user_id   | 0          \n",
      " item_id   | 7101054    \n",
      " ts_start  | 1491412481 \n",
      " ts_end    | 1491451571 \n",
      " item_type | live       \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", IntegerType()), # время начала просмотра.\n",
    "    StructField(\"ts_end\", StringType()), # время окончания просмотра.\n",
    "    StructField(\"item_type\", StringType()) \n",
    "    # тип просматриваемого контента: # live — просмотр \"вживую\", в момент показа контента в эфире. \n",
    "                                  # pvr — просмотр в записи, после показа контента в эфире.\n",
    "])\n",
    "\n",
    "df_views = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", header=True, schema=schema).cache()   \n",
    "df_views.show(2,False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_views.summary().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random solution\n",
    "Важно! Для точной проверки не забудьте отсортировать полученный файл по возрастанию идентификаторов пользователей (user_id), а затем — по возрастанию идентификаторов передач (item_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torand_func(a: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.random.uniform(0,1,a.shape[0]))\n",
    "torand = pandas_udf(torand_func, returnType=FloatType())\n",
    "\n",
    "df_test = df_test.withColumn('purchase',torand(df_test['purchase']))\n",
    "\n",
    "df_test.toPandas()[[\"user_id\",\"item_id\",\"purchase\"]].sort_values([\"user_id\",\"item_id\"]).to_csv('../lab03.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = f.udf(ith_, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_type\n",
    "df_train=df_train.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])\n",
    "df_test=df_test.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.withColumn(\"genres\", f.split(f.col(\"genres\"), \",\")).select(\"genres.*\").show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean join\n",
    "df_mean_user_purch=df_train.groupBy(\"user_id\").mean(\"purchase\")\n",
    "df_mean_item_purch=df_train.groupBy(\"item_id\").mean(\"purchase\")\n",
    "\n",
    "df_train=df_train.join(df_mean_user_purch, [\"user_id\"])\n",
    "df_train=df_train.withColumnRenamed(\"avg(purchase)\",\"mean_user_purch\")\n",
    "df_train=df_train.join(df_mean_item_purch, [\"item_id\"])\n",
    "df_train=df_train.withColumnRenamed(\"avg(purchase)\",\"mean_item_purch\")\n",
    "\n",
    "df_test=df_test.join(df_mean_user_purch, [\"user_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_user_purch\")\n",
    "df_test=df_test.join(df_mean_item_purch, [\"item_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_item_purch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"purchase\", featuresCol=\"features\", maxIter=50, maxDepth=3) # 0.9003\n",
    "lr = LogisticRegression(labelCol=\"purchase\", featuresCol=\"features\") # 0.889905372368\n",
    "\n",
    "features_col = [\"mean_user_purch\", \"mean_item_purch\", \"content_type\"] # \"year\"\n",
    "        # \"count_user_purch\", \"count_item_purch\", \n",
    "assembler = VectorAssembler(inputCols=features_col, outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "qualification_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, gbt]) #  \n",
    "model = pipeline.fit(df_train)\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "predictions.select(\"user_id\",\"item_id\", ith(\"probability\", f.lit(1)).alias(\"purchase\")).toPandas()\\\n",
    "        .sort_values([\"user_id\",\"item_id\"]).to_csv('../lab03.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
