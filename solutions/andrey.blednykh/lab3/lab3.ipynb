{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.instances\", \"6\")\n",
    "conf.set(\"spark.executor.cores\", \"3\")\n",
    "conf.set(\"spark.executor.memory\", \"9g\")\n",
    "conf.set(\"spark.driver.cores\", \"3\")\n",
    "conf.set(\"spark.driver.memory\", \"6g\")\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf = conf)\\\n",
    "    .appName(\"Lab 03 Model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"user_id\", IntegerType()),\n",
    "                    StructField(\"item_id\", IntegerType()),\n",
    "                    StructField(\"purchase\", DoubleType())])\n",
    "\n",
    "df_train = spark.read.csv('/labs/slaba03/laba03_train.csv', header= True, schema=schema)\n",
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header= True, schema=schema)\n",
    "\n",
    "df_views = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', header= True)\n",
    "df_items = spark.read.csv('/labs/slaba03/laba03_items.csv', header= True, sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df_train.join(train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = spark.read.csv('/labs/laba03/lab10_test.csv', header= True, schema=schema)\n",
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header= True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_purchases = train\\\n",
    "    .groupBy('user_id')\\\n",
    "    .sum()\\\n",
    "    .select(col(\"sum(purchase)\").alias(\"user_purchases\"), col(\"user_id\"))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_purchases = train\\\n",
    "    .groupBy('item_id')\\\n",
    "    .sum()\\\n",
    "    .select(col(\"sum(purchase)\").alias(\"item_purchases\"), col(\"item_id\"))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train\\\n",
    "    .join(train_purchases, on='user_id', how='left')\\\n",
    "    .join(item_purchases, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid\\\n",
    "    .join(train_purchases, on='user_id', how='left')\\\n",
    "    .join(item_purchases, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test\\\n",
    "    .join(train_purchases, on='user_id', how='left')\\\n",
    "    .join(item_purchases, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_attempts = train\\\n",
    "    .groupBy('user_id')\\\n",
    "    .count()\\\n",
    "    .select(col(\"count\").alias(\"user_attempts\"), col(\"user_id\"))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_attempts = train\\\n",
    "    .groupBy('item_id')\\\n",
    "    .count()\\\n",
    "    .select(col(\"count\").alias(\"item_attempts\"), col(\"item_id\"))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train\\\n",
    "    .join(train_user_attempts, on='user_id', how='left')\\\n",
    "    .join(train_item_attempts, on='item_id', how='left')\\\n",
    "    .withColumn('user_addict', col('user_purchases') / col('user_attempts'))\\\n",
    "    .withColumn('item_addict', col('item_purchases') / col('item_attempts'))\\\n",
    "    .na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid\\\n",
    "    .join(train_user_attempts, on='user_id', how='left')\\\n",
    "    .join(train_item_attempts, on='item_id', how='left')\\\n",
    "    .withColumn('user_addict', col('user_purchases') / col('user_attempts'))\\\n",
    "    .withColumn('item_addict', col('item_purchases') / col('item_attempts'))\\\n",
    "    .na.fill(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test\\\n",
    "    .join(train_user_attempts, on='user_id', how='left')\\\n",
    "    .join(train_item_attempts, on='item_id', how='left')\\\n",
    "    .withColumn('user_addict', col('user_purchases') / col('user_attempts'))\\\n",
    "    .withColumn('item_addict', col('item_purchases') / col('item_attempts'))\\\n",
    "    .na.fill(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_attempts: bigint, item_id: int]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_purchases.unpersist()\n",
    "item_purchases.unpersist()\n",
    "train_user_attempts.unpersist()\n",
    "train_item_attempts.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['item_purchases', 'user_purchases', 'user_addict', 'item_addict']\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = assembler.transform(train).cache()\n",
    "valid_data = assembler.transform(valid)\n",
    "test_data = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"purchase\")\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [3, 4])\\\n",
    "                              .addGrid(gbt.minInstancesPerNode, [2, 3])\\\n",
    "                              .addGrid(gbt.maxBins, [50, 55])\\\n",
    "                              .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator, numFolds=3, parallelism=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_valid = cv_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, user_id: int, purchase: double, user_purchases: double, item_purchases: double, user_attempts: bigint, item_attempts: bigint, user_addict: double, item_addict: double, features: vector]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"purchase\", maxDepth=4, minInstancesPerNode=3, maxBins=50)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "predictions_valid = gbt_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "predictions_pd = predictions_valid.select(\"user_id\", \"item_id\", col(\"probability\").alias(\"purchase\")).toPandas()\n",
    "predictions_pd = predictions_pd.sort_values(by=['user_id', 'item_id'])\n",
    "predictions_pd['purchase'] = predictions_pd['purchase'].apply(lambda x: x[1])\n",
    "predictions_pd.to_csv('/data/home/andrey.blednykh/lab03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
