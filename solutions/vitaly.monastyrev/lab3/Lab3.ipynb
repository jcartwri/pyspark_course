{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Lab3\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "\n",
    "# Function to convert JSON array string to a list\n",
    "def parse_json(array_str):\n",
    "    json_obj = json.loads(array_str)\n",
    "    for item in json_obj[\"visits\"]:\n",
    "        yield (item[\"url\"], item[\"timestamp\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Lab3\") \n",
    "conf.set(\"spark.driver.memory\", \"16g\") \n",
    "conf.set(\"spark.driver.memoryOverhead\", \"32g\") \n",
    "conf.set(\"spark.executor.memory\", \"4g\") \n",
    "conf.set(\"spark.executor.instances\", \"2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "items = spark.read.option(\"delimiter\", \"\\t\").csv(\"/labs/slaba03/laba03_items.csv\", header = True)\n",
    "test = spark.read.option(\"delimiter\", \",\").csv(\"/labs/slaba03/laba03_test.csv\", header = True)\n",
    "train = spark.read.option(\"delimiter\", \",\").csv(\"/labs/slaba03/laba03_train.csv\", header = True)\n",
    "views_programmes = spark.read.option(\"delimiter\", \",\").csv(\"/labs/slaba03/laba03_views_programmes.csv\", header = True)\n",
    "\n",
    "train = train.withColumn(\"item_id\", F.col(\"item_id\").cast(IntegerType()))\n",
    "train = train.withColumn(\"user_id\", F.col(\"user_id\").cast(IntegerType()))\n",
    "train = train.withColumn(\"purchase\", F.col(\"purchase\").cast(IntegerType()))\n",
    "\n",
    "test = test.withColumn(\"item_id\", F.col(\"item_id\").cast(IntegerType()))\n",
    "test = test.withColumn(\"user_id\", F.col(\"user_id\").cast(IntegerType()))\n",
    "test = test.drop(F.col(\"purchase\"))\n",
    "\n",
    "als = ALS(maxIter=10, regParam=0.01, nonnegative = False, implicitPrefs=True, rank = 7,\n",
    "          userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"purchase\")\n",
    "\n",
    "model = als.fit(train)\n",
    "predictions = model.transform(test.fillna(0))\n",
    "\n",
    "\n",
    "predictions = predictions.withColumnRenamed(\"prediction\", \"purchase\")\n",
    "\n",
    "\n",
    "predictions = predictions.toPandas()\n",
    "predictions = predictions.sort_values(by=['user_id', 'item_id'])\n",
    "print(predictions)\n",
    "predictions.to_csv('lab03.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
