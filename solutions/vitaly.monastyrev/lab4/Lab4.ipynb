{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Lab3\") \n",
    "conf.set(\"spark.driver.memory\", \"16g\") \n",
    "conf.set(\"spark.driver.memoryOverhead\", \"32g\") \n",
    "conf.set(\"spark.executor.memory\", \"4g\") \n",
    "conf.set(\"spark.executor.instances\", \"2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "\n",
    "# Function to convert JSON array string to a list\n",
    "def parse_json(array_str):\n",
    "    json_obj = json.loads(array_str)\n",
    "    for item in json_obj[\"visits\"]:\n",
    "        yield (item[\"url\"], item[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").csv(\"/labs/slaba04/gender_age_dataset.txt\", header = True)\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "json_schema = ArrayType(StructType([StructField('url', StringType(), nullable=False), StructField('timestamp', StringType(), nullable=False)]))\n",
    "# Define udf\n",
    "udf_parse_json = udf(lambda str: parse_json(str), json_schema)\n",
    "\n",
    "\n",
    "# Generate a new data frame with the expected schema\n",
    "df = df.withColumn(\"visits\", udf_parse_json(F.col(\"user_json\")))\n",
    "df = df.select(F.col(\"gender\"), F.col(\"age\"), F.col(\"uid\"), F.col(\"visits\"))\n",
    "df = df.withColumn(\"visits\", F.explode(F.col(\"visits\")))\n",
    "df = df.withColumn(\"url\", F.col(\"visits.url\"))\n",
    "df = df.drop(F.col(\"visits\"))\n",
    "\n",
    "df = df.withColumn(\"url\", F.lower(F.expr(\"parse_url(url, 'HOST')\"))).withColumn(\"url\", F.regexp_replace(F.col(\"url\"), \"www.\", \"\")).withColumn(\"url\", F.regexp_replace(F.col(\"url\"), \"[.]\", \"-\"))\n",
    "df = df.filter(F.col(\"url\").isNotNull())\n",
    "df = df.withColumn(\"gender_age\", F.concat(F.col(\"gender\"), F.lit(\":\"), F.col(\"age\")))\n",
    "\n",
    "df = df.groupBy(F.col(\"gender_age\"), F.col(\"uid\")).agg(F.collect_list(F.col(\"url\")).alias(\"domains\"))\n",
    "\n",
    "df = df.filter(df.gender_age != F.lit(\"-:-\"))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"domains\", outputCol=\"features\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"gender_age\", outputCol=\"label\")\n",
    "labels = indexer.fit(df).labels\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20, \n",
    "    maxBins=32, \n",
    "    maxDepth=15,\n",
    "    seed=37)\n",
    "\n",
    "converter = IndexToString(inputCol=\"prediction\", labels=labels, outputCol=\"res\")\n",
    "\n",
    "pipeline = Pipeline(stages=[cv, indexer, rf, converter])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "model.write().overwrite().save(\"Lab_4_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"Lab_4_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-2.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_vitaly.monastyrev\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "dfInput = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "df = dfInput.selectExpr(\"CAST(value AS STRING)\")\n",
    "schema = StructType([\n",
    "  StructField(\"uid\", StringType(), True),\n",
    "  StructField(\"visits\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = df.withColumn(\"jsonData\", F.from_json(F.col(\"value\"), schema)).select(\"jsonData.*\")\n",
    "df = df.withColumn(\"visits\", F.concat(F.lit(\"{\\\"visits\\\": \"), F.col(\"visits\"), F.lit(\"}\")))\n",
    "# Define the schema\n",
    "json_schema = ArrayType(StructType([StructField('url', StringType(), nullable=False), StructField('timestamp', StringType(), nullable=False)]))\n",
    "# Define udf\n",
    "udf_parse_json = udf(lambda str: parse_json(str), json_schema)\n",
    "\n",
    "\n",
    "# Generate a new data frame with the expected schema\n",
    "df = df.withColumn(\"visits\", udf_parse_json(F.col(\"visits\")))\n",
    "\n",
    "df = df.withColumn(\"visits\", F.explode(F.col(\"visits\")))\n",
    "df = df.withColumn(\"url\", F.col(\"visits.url\"))\n",
    "df = df.drop(F.col(\"visits\"))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"url\", F.lower(F.expr(\"parse_url(url, 'HOST')\"))).withColumn(\"url\", F.regexp_replace(F.col(\"url\"), \"www.\", \"\")).withColumn(\"url\", F.regexp_replace(F.col(\"url\"), \"[.]\", \"-\"))\n",
    "\n",
    "df = df.groupBy(F.col(\"uid\")).agg(F.collect_list(F.col(\"url\")).alias(\"domains\"))\n",
    "model = PipelineModel.load(\"Lab_4_model\")\n",
    "df = model.transform(df)\n",
    "df = df.select(F.col(\"uid\"), F.col(\"res\").alias(\"gender_age\"))\n",
    "\n",
    "split_col = F.split(F.col(\"gender_age\"), ':')\n",
    "df = df.withColumn('gender', split_col.getItem(0))\n",
    "df = df.withColumn('age', split_col.getItem(1))\n",
    "df = df.select(F.col(\"uid\"), F.col(\"gender\"), F.col(\"age\"))\n",
    "\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-2.newprolab.com:6667',\n",
    "   \"topic\": \"vitaly.monastyrev\"\n",
    "}\n",
    "query = df.selectExpr(\"CAST(uid AS STRING) AS key\", \"to_json(struct(*)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .option(\"maxOffsetsPerTrigger\", 200)\\\n",
    "    .outputMode(\"update\").start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
