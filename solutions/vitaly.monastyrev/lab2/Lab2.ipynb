{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Lab2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"23126\": [14760, 13665, 13782, 20638, 24419, 15909, 2724, 25782, 13348, 17499], \"21617\": [21609, 21608, 21616, 21492, 21703, 21624, 21700, 21623, 21508, 21506], \"16627\": [11431, 12247, 5687, 17964, 11575, 13021, 17961, 25010, 16694, 12660], \"11556\": [16488, 13461, 468, 23357, 7833, 9289, 19330, 16929, 22710, 5750], \"16704\": [1247, 20288, 1273, 8203, 1365, 1236, 1233, 20645, 1164, 1426], \"13702\": [864, 1052, 8082, 1216, 8313, 17017, 13057, 19613, 21079, 20105]}\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType, FloatType\n",
    "    \n",
    "targetIds = [23126, 21617, 16627, 11556, 16704, 13702]\n",
    "targetIds\n",
    "\n",
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")\n",
    "\n",
    "sentenceData = df.withColumnRenamed(\"desc\", \"sentence\")\n",
    "sentenceData = sentenceData.select(\"sentence\", \"id\", \"lang\")\n",
    "sentenceData = sentenceData.withColumn(\"sentence\",F.regexp_replace(F.col(\"sentence\"), \"[^A-Za-zА-Яа-я ]\", \"\"))\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words_first\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words_first\", outputCol=\"words\")\n",
    "wordsData = remover.transform(wordsData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "res = rescaledData.select(\"features\", \"id\", \"lang\")\n",
    "\n",
    "res.cache()\n",
    "# res.show()\n",
    "\n",
    "resJson = \"{}\"\n",
    "for targetId in targetIds:\n",
    "    targetDf = res.filter(F.col(\"id\") == targetId)\n",
    "    lang = targetDf.select(F.col(\"lang\")).collect()[0][0]\n",
    "    \n",
    "    def to_array(col):\n",
    "        def to_array_(v):\n",
    "            return v.toArray().tolist()\n",
    "        # Important: asNondeterministic requires Spark 2.3 or later\n",
    "        # It can be safely removed i.e.\n",
    "        # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "        # but at the cost of decreased performance\n",
    "        return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n",
    "\n",
    "    targetDf = targetDf.select(F.col(\"features\"))\n",
    "#     targetDf = targetDf.select(to_array(F.col(\"features\")).alias(\"features\"))\n",
    "#     targetVec = targetDf.select(F.col(\"features\")).collect()[0]#[0]\n",
    "    targetRes = res.filter(F.col(\"lang\") == lang)\n",
    "    \n",
    "    targetDf = targetDf.withColumnRenamed(\"features\", \"features2\")\n",
    "    targetRes = targetRes.crossJoin(targetDf)\n",
    "    \n",
    "    \n",
    "    @udf\n",
    "    def sim_cos(v1, v2):\n",
    "        try:\n",
    "            p = 2\n",
    "            return float(v1.dot(v2))/float(v1.norm(p)*v2.norm(p))\n",
    "        except:\n",
    "            return 0\n",
    "     \n",
    "    targetRes = targetRes.withColumn(\"cos\", sim_cos(F.col(\"features\"), F.col(\"features2\")))\n",
    "\n",
    "    targetRes = targetRes.withColumn(\"cos\", F.col(\"cos\").cast(FloatType()))\n",
    "    targetRes = targetRes.filter(F.col(\"cos\").isNotNull())\n",
    "    targetRes = targetRes.filter(F.col(\"id\") != targetId)\n",
    "    targetRes = targetRes.select(F.col(\"id\"), F.col(\"cos\"), F.col(\"lang\")).orderBy(F.col(\"cos\").desc()).limit(10)\n",
    "    \n",
    "    predicts = targetRes.select(F.col(\"id\")).collect()\n",
    "\n",
    "    tempJson = json.dumps({targetId: [predicts[0][0], \n",
    "                                predicts[1][0], \n",
    "                                predicts[2][0], \n",
    "                                predicts[3][0], \n",
    "                                predicts[4][0], \n",
    "                                predicts[5][0], \n",
    "                                predicts[6][0], \n",
    "                                predicts[7][0], \n",
    "                                predicts[8][0], \n",
    "                                predicts[9][0]]})\n",
    "    \n",
    "    dictA = json.loads(resJson)\n",
    "    dictB = json.loads(tempJson)\n",
    "\n",
    "    dictA.update(dictB)\n",
    "    resJson = json.dumps(dictA)\n",
    "\n",
    "print(resJson)\n",
    "f = open(\"lab02.json\", \"a\")\n",
    "f.write(resJson)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
