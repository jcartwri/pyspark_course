{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, CountVectorizer\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType \n",
    "from pyspark.sql.types import TimestampType, LongType, FloatType, ArrayType\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Model, Estimator\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml.param.shared import HasFeaturesCol, HasLabelCol, HasPredictionCol\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf()\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=spark_config)\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/labs/slaba03/laba03_train.csv'\n",
    "test_path = '/labs/slaba03/laba03_test.csv'\n",
    "items_path = '/labs/slaba03/laba03_items.csv'\n",
    "users_path = '/labs/slaba03/laba03_views_programmes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                              StructField('item_id', IntegerType()),\n",
    "                                              StructField('purchase', IntegerType(), nullable=True), \n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "                                       StructField('channel_id', IntegerType()),\n",
    "                                       StructField('datetime_availability_start', StringType()),\n",
    "                                       StructField('datetime_availability_stop', StringType()),\n",
    "                                       StructField('datetime_show_start', StringType()),\n",
    "                                       StructField('datetime_show_stop', StringType()),\n",
    "                                       StructField('content_type', IntegerType()),\n",
    "                                       StructField('title', StringType(), nullable=True),\n",
    "                                       StructField('year', FloatType(), nullable=True),\n",
    "                                       StructField('genres', StringType()),\n",
    "                                       StructField('region_id', IntegerType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                       StructField('item_id', IntegerType()),\n",
    "                                       StructField('ts_start', IntegerType()),\n",
    "                                       StructField('ts_end', IntegerType()),\n",
    "                                       StructField('item_type', StringType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(main_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(train_path)\\\n",
    "                .cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(main_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(test_path)\\\n",
    "                 .cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(items_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"delimiter\", \"\\\\t\")\\\n",
    "                .load(items_path)\\\n",
    "                 .select(['item_id', 'content_type', 'year', 'genres'])\\\n",
    "                 .na.fill({'year': -999, 'genres': 'unknown'}).cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users =  spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(users_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(users_path)\\\n",
    "                .cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_user_purchase=df_train.groupBy(\"user_id\").mean(\"purchase\")\n",
    "df_mean_item_purchase=df_train.groupBy(\"item_id\").mean(\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_user_purchase=df_mean_user_purchase.withColumnRenamed(\"avg(purchase)\",\"mean_user_purchase\")\n",
    "df_mean_item_purchase=df_mean_item_purchase.withColumnRenamed(\"avg(purchase)\",\"mean_item_purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- mean_user_purchase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mean_user_purchase.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- mean_item_purchase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mean_item_purchase.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.join(df_mean_user_purchase, [\"user_id\"])\n",
    "df_train=df_train.join(df_mean_item_purchase, [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- content_type: integer (nullable = true)\n",
      " |-- year: float (nullable = false)\n",
      " |-- genres: string (nullable = false)\n",
      " |-- mean_user_purchase: double (nullable = true)\n",
      " |-- mean_item_purchase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.join(df_mean_user_purchase, [\"user_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_user_purchase\")\n",
    "df_test=df_test.join(df_mean_item_purchase, [\"item_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_item_purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"purchase\", featuresCol=\"features\", maxIter=50, maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"purchase\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"purchase\", featuresCol=\"features\", maxIter=100, regParam=0.01, elasticNetParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = [\"mean_user_purchase\", \"mean_item_purchase\", \"content_type\", \"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=features_col, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "qualification_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, lr]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_1 = predictions.select(['user_id','item_id','probability'])\\\n",
    "                    .withColumnRenamed(\"probability\",\"purchase\")\\\n",
    "                        .orderBy(['user_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|            purchase|\n",
      "+-------+-------+--------------------+\n",
      "|   1654|    336|[0.99869163353666...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_df_1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "total_df = predictions.select(['user_id','item_id','probability'])\\\n",
    "                        .orderBy(['user_id','item_id']).toPandas()\n",
    "total_df['purchase'] = total_df['probability'].apply(lambda x: x[1])\n",
    "total_df = total_df.drop('probability', axis=1)\n",
    "total_df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
