{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, CountVectorizer\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf()\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=spark_config)\\\n",
    "                    .appName(\"nazim_lab03\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_interactions_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                              StructField('item_id', IntegerType()),\n",
    "                                              StructField('purchase', IntegerType(), nullable=True), \n",
    "                                             ])\n",
    "\n",
    "read_items_schema = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "                                       StructField('channel_id', IntegerType()),\n",
    "                                       StructField('datetime_availability_start', StringType()),\n",
    "                                       StructField('datetime_availability_stop', StringType()),\n",
    "                                       StructField('datetime_show_start', StringType()),\n",
    "                                       StructField('datetime_show_stop', StringType()),\n",
    "                                       StructField('content_type', IntegerType()),\n",
    "                                       StructField('title', StringType(), nullable=True),\n",
    "                                       StructField('year', FloatType(), nullable=True),\n",
    "                                       StructField('genres', StringType()),\n",
    "                                       StructField('region_id', IntegerType()),\n",
    "                                      ])\n",
    "\n",
    "read_users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                       StructField('item_id', IntegerType()),\n",
    "                                       StructField('ts_start', IntegerType()),\n",
    "                                       StructField('ts_end', IntegerType()),\n",
    "                                       StructField('item_type', StringType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/labs/slaba03/laba03_train.csv'\n",
    "test_path = '/labs/slaba03/laba03_test.csv'\n",
    "items_path = '/labs/slaba03/laba03_items.csv'\n",
    "users_path = '/labs/slaba03/laba03_views_programmes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(read_interactions_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(train_path)\n",
    "\n",
    "test_sdf = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(read_interactions_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(test_path)\n",
    "\n",
    "items_sdf = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(read_items_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"delimiter\", \"\\\\t\")\\\n",
    "                .load(items_path)\n",
    "\n",
    "items_sdf = items_sdf.select(['item_id', 'content_type', 'year', 'genres'])\\\n",
    "                        .na.fill({'year': -999, 'genres': 'unknown'})\n",
    "\n",
    "users_sdf =  spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(read_users_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(users_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPurchaseByUser(Estimator):\n",
    "    def __init__(self):\n",
    "        Transformer.__init__(self)\n",
    "        self.user_mean_purchase = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.user_mean_purchase = X.select(['user_id', 'purchase'])\\\n",
    "                                    .groupBy('user_id')\\\n",
    "                                    .agg(f.mean('purchase').alias('user_purchase_mean'),\n",
    "                                         f.sum('purchase').alias('user_purchase_count'))\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.join(self.user_mean_purchase, on='user_id', how='inner')\n",
    "\n",
    "    \n",
    "class MeanPurchaseByItem(Estimator):\n",
    "    def __init__(self):\n",
    "        Transformer.__init__(self)\n",
    "        self.item_mean_purchase = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.item_mean_purchase = X.select(['item_id', 'purchase'])\\\n",
    "                                    .groupBy('item_id')\\\n",
    "                                    .agg(f.mean('purchase').alias('item_purchase_mean'), \n",
    "                                         f.sum('purchase').alias('item_purchase_count'))\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.join(self.item_mean_purchase, on='item_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = Pipeline(stages=[MeanPurchaseByUser(),\n",
    "                                   MeanPurchaseByItem()\n",
    "                                  ])\n",
    "target_features_model = target_features.fit(train_sdf)\n",
    "X_train = target_features_model.transform(train_sdf)\n",
    "X_test = target_features_model.transform(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemFeaturesJoin(Transformer):\n",
    "    def transform(self, X):\n",
    "        return X.join(items_sdf, on='item_id', how='left')\n",
    "    \n",
    "class GenresVectorizer(Transformer):\n",
    "    def transform(self, X):\n",
    "        X = X.withColumn(\"genres_array\", f.split(f.col(\"genres\"), ','))\n",
    "        return CountVectorizer(inputCol=\"genres_array\", outputCol=\"genres_vector\").fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = Pipeline(stages=[ItemFeaturesJoin(),\n",
    "                                 GenresVectorizer()\n",
    "                                ])\n",
    "X_train = item_features.fit(X_train).transform(X_train)\n",
    "X_test = item_features.fit(X_test).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserWatchedTime(Estimator):\n",
    "    def __init__(self):\n",
    "        Transformer.__init__(self)\n",
    "        self.user_watched_time = None\n",
    "    \n",
    "    def fit(self, users_features):\n",
    "        users_features = users_features.withColumn('watched_time', f.col('ts_end') - f.col('ts_start'))\n",
    "        watched_time = users_features.groupBy(['user_id']).pivot('item_type')\\\n",
    "                                            .agg(f.sum('watched_time').alias('sum_watched'),\n",
    "                                                 f.mean('watched_time').alias('mean_watched'))\n",
    "                                        \n",
    "        self.user_watched_time = watched_time.na.fill(0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.join(self.user_watched_time, on='user_id', how='left').na.fill(0)\n",
    "        X = X.withColumn('watched_time', f.col('live_sum_watched') + f.col('pvr_sum_watched'))\n",
    "        X = X.withColumn('watched_time_mean', (f.col('live_mean_watched') + f.col('pvr_mean_watched')) / 2.0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = Pipeline(stages=[UserWatchedTime()\n",
    "                                ])\n",
    "user_features_model = user_features.fit(users_sdf)\n",
    "X_train = user_features_model.transform(X_train).cache()\n",
    "X_test = user_features_model.transform(X_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['user_id', 'item_id', 'year', 'genres_vector', 'content_type',\n",
    "                 'user_purchase_mean', 'item_purchase_mean', 'user_purchase_count', 'item_purchase_count',\n",
    "                 'watched_time', 'watched_time_mean',\n",
    "                 'live_sum_watched', 'live_mean_watched', 'pvr_sum_watched',  'pvr_mean_watched']\n",
    "\n",
    "features_assembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol='purchase', seed=42,\n",
    "                   maxIter=50, maxDepth=3, minInstancesPerNode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(stages=[features_assembler,gbt])\n",
    "  \n",
    "model = model_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "answer_df = predictions.select(['user_id','item_id','probability'])\\\n",
    "                        .orderBy(['user_id','item_id']).toPandas()\n",
    "answer_df['purchase'] = answer_df['probability'].apply(lambda x: x[1])\n",
    "answer_df = answer_df.drop('probability', axis=1)\n",
    "answer_df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
