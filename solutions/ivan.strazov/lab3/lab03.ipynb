{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Библиотеки и настройки сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/labs/slaba03/\"\n",
    "TRAIN_FILE = DATA_PATH + \"laba03_train.csv\"\n",
    "TEST_FILE = DATA_PATH + \"laba03_test.csv\"\n",
    "ITEMS_FILE = DATA_PATH + \"laba03_items.csv\"\n",
    "VIEWS_FILE = DATA_PATH + \"laba03_views_programmes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"ivan.strazov - lab03\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"ivan.strazov - lab03\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Normalizer, OneHotEncoder, StringIndexer, Tokenizer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2onehot(data, column):\n",
    "    \"\"\"\n",
    "    Transform categorical column to binary vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pyspark.DataFrame) - base dataframe.\n",
    "        column (str) - name of column.\n",
    "    \n",
    "    Return:\n",
    "        new_data () - transformed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    indexer = StringIndexer(inputCol=column, outputCol=\"index\").setHandleInvalid(\"keep\")\n",
    "    encoder = OneHotEncoder(dropLast=True, inputCol=\"index\", outputCol=column+\"_onehot\")\n",
    "    \n",
    "    pipe = Pipeline(stages = [indexer, encoder])\n",
    "    \n",
    "    new_data = pipe\\\n",
    "                .fit(data)\\\n",
    "                .transform(data) \\\n",
    "                .drop(column, \"index\")\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2tfidf(data, column):\n",
    "    \"\"\"\n",
    "    Transform text column to normalized TF-IDF.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pyspark.DataFrame) - base dataframe.\n",
    "        column (str) - name of column.\n",
    "    \n",
    "    Return:\n",
    "        new_data () - transformed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=column, outputCol=\"token\")\n",
    "    tokenized = tokenizer.transform(data)\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"token\",\n",
    "                          outputCol=\"tf\",\n",
    "                          numFeatures=10000)\n",
    "    tf = hashingTF.transform(tokenized)\n",
    "\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "    tfidf = idf.transform(tf)\n",
    "\n",
    "    normalizer = Normalizer(inputCol=\"tfidf\",\n",
    "                            outputCol=column+\"_tfidf\",\n",
    "                            p=2)\n",
    "    new_data = normalizer \\\n",
    "                    .transform(tfidf) \\\n",
    "                    .drop(column, \"token\", \"tf\", \"tfidf\")                \n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "В файле содержатся факты покупки (колонка purchase) пользователями (колонка user_id) телепередач (колонка item_id). Такой формат файла вам уже знаком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n",
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "dataset = spark.read \\\n",
    "            .schema(schema) \\\n",
    "            .format(\"csv\") \\\n",
    "            .load(TRAIN_FILE, header=\"true\")\n",
    "dataset.printSchema()\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|user_id|users_buys|      users_per_buys|\n",
      "+-------+----------+--------------------+\n",
      "| 867850|         1|3.829950210647261...|\n",
      "| 870928|         2|7.674597083653108E-4|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buys4users = dataset \\\n",
    "                .groupBy(\"user_id\") \\\n",
    "                .agg(f.sum(\"purchase\").alias(\"users_buys\"),\n",
    "                     (f.sum(\"purchase\") / f.count(\"item_id\")).alias(\"users_per_buys\"))\n",
    "buys4users.cache()\n",
    "buys4users.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|item_id|items_buys|      items_per_buys|\n",
      "+-------+----------+--------------------+\n",
      "|   8638|         2|0.001450326323422...|\n",
      "|  95940|         1|   7.097232079489E-4|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buys4items = dataset \\\n",
    "                .groupBy(\"item_id\") \\\n",
    "                .agg(f.sum(\"purchase\").alias(\"items_buys\"),\n",
    "                     (f.sum(\"purchase\") / f.count(\"user_id\")).alias(\"items_per_buys\"))\n",
    "buys4items.cache()\n",
    "buys4items.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Тестовый датасет без указанного целевого признака purchase, который вам и предстоит предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n",
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "result = spark.read \\\n",
    "            .schema(schema) \\\n",
    "            .format(\"csv\") \\\n",
    "            .load(TEST_FILE, header=\"true\")\n",
    "result.printSchema()\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items\n",
    "\n",
    "Дополнительные данные по items. В данном файле много лишней или ненужной информации, так что задача её фильтрации и отбора ложится на вас. Поля в файле, на которых хотелось бы остановиться:\n",
    "- item_id — primary key. Соответствует item_id в предыдущем файле.\n",
    "- content_type — тип телепередачи (1 — платная, 0 — бесплатная). Вас интересуют платные передачи.\n",
    "- title — название передачи, текстовое поле.\n",
    "- year — год выпуска передачи, число.\n",
    "- genres — поле с жанрами передачи, разделёнными через запятую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- channel_id: string (nullable = true)\n",
      " |-- datetime_availability_start: string (nullable = true)\n",
      " |-- datetime_availability_stop: string (nullable = true)\n",
      " |-- datetime_show_start: string (nullable = true)\n",
      " |-- datetime_show_stop: string (nullable = true)\n",
      " |-- content_type: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- region_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>datetime_availability_start</th>\n",
       "      <th>datetime_availability_stop</th>\n",
       "      <th>datetime_show_start</th>\n",
       "      <th>datetime_show_stop</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genres</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65667</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-01T00:00:00Z</td>\n",
       "      <td>2018-01-01T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>на пробах только девушки (all girl auditions)</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Эротика</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65669</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-01T00:00:00Z</td>\n",
       "      <td>2018-01-01T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>скуби ду: эротическая пародия (scooby doo: a x...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Эротика</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65668</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-01T00:00:00Z</td>\n",
       "      <td>2018-01-01T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>горячие девочки для горячих девочек (hot babes...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Эротика</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65671</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-01T00:00:00Z</td>\n",
       "      <td>2018-01-01T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>соблазнительницы женатых мужчин (top heavy hom...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Эротика</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65670</td>\n",
       "      <td>None</td>\n",
       "      <td>1970-01-01T00:00:00Z</td>\n",
       "      <td>2018-01-01T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>секретные секс-материалы ii: темная секс парод...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Эротика</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id channel_id datetime_availability_start datetime_availability_stop  \\\n",
       "0    65667       None        1970-01-01T00:00:00Z       2018-01-01T00:00:00Z   \n",
       "1    65669       None        1970-01-01T00:00:00Z       2018-01-01T00:00:00Z   \n",
       "2    65668       None        1970-01-01T00:00:00Z       2018-01-01T00:00:00Z   \n",
       "3    65671       None        1970-01-01T00:00:00Z       2018-01-01T00:00:00Z   \n",
       "4    65670       None        1970-01-01T00:00:00Z       2018-01-01T00:00:00Z   \n",
       "\n",
       "  datetime_show_start datetime_show_stop  content_type  \\\n",
       "0                None               None             1   \n",
       "1                None               None             1   \n",
       "2                None               None             1   \n",
       "3                None               None             1   \n",
       "4                None               None             1   \n",
       "\n",
       "                                               title    year   genres  \\\n",
       "0      на пробах только девушки (all girl auditions)  2013.0  Эротика   \n",
       "1  скуби ду: эротическая пародия (scooby doo: a x...  2011.0  Эротика   \n",
       "2  горячие девочки для горячих девочек (hot babes...  2011.0  Эротика   \n",
       "3  соблазнительницы женатых мужчин (top heavy hom...  2011.0  Эротика   \n",
       "4  секретные секс-материалы ii: темная секс парод...  2010.0  Эротика   \n",
       "\n",
       "  region_id  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", StringType()),\n",
    "    StructField(\"datetime_availability_start\", StringType()),\n",
    "    StructField(\"datetime_availability_stop\", StringType()),\n",
    "    StructField(\"datetime_show_start\", StringType()),\n",
    "    StructField(\"datetime_show_stop\", StringType()),\n",
    "    StructField(\"content_type\", IntegerType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", DoubleType()),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", StringType()),\n",
    "])\n",
    "\n",
    "items = spark.read \\\n",
    "            .schema(schema) \\\n",
    "            .format(\"csv\") \\\n",
    "            .load(ITEMS_FILE, sep=\"\\t\", header=\"true\")\n",
    "items.printSchema()\n",
    "items.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, content_type: int, title: string, year: double, genres: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = items \\\n",
    "            .select(\"item_id\", \"content_type\", \"title\", \"year\", \"genres\")\n",
    "items.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, content_type: int, title: string, year: double, genres_onehot: vector]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"genres\", outputCol=\"index\").setHandleInvalid(\"keep\")\n",
    "encoder = OneHotEncoder(dropLast=True, inputCol=\"index\", outputCol=\"genres\"+\"_onehot\")\n",
    "\n",
    "pipe = Pipeline(stages = [indexer, encoder])\n",
    "\n",
    "items = pipe\\\n",
    "            .fit(items)\\\n",
    "            .transform(items) \\\n",
    "            .drop(\"genres\", \"index\")\n",
    "items.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, content_type: int, year: double, genres_onehot: vector, title_tfidf: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"token\")\n",
    "tokenized = tokenizer.transform(items)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"token\",\n",
    "                      outputCol=\"tf\",\n",
    "                      numFeatures=10000)\n",
    "tf = hashingTF.transform(tokenized)\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"tfidf\",\n",
    "                        outputCol=\"title\"+\"_tfidf\",\n",
    "                        p=2)\n",
    "items = normalizer \\\n",
    "                .transform(tfidf) \\\n",
    "                .drop(\"title\", \"token\", \"tf\", \"tfidf\")\n",
    "items.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+----------------+--------------------+\n",
      "|item_id|content_type|  year|   genres_onehot|         title_tfidf|\n",
      "+-------+------------+------+----------------+--------------------+\n",
      "|  65667|           1|2013.0|(1076,[6],[1.0])|(10000,[665,1029,...|\n",
      "|  65669|           1|2011.0|(1076,[6],[1.0])|(10000,[1147,1299...|\n",
      "|  65668|           1|2011.0|(1076,[6],[1.0])|(10000,[966,2231,...|\n",
      "|  65671|           1|2011.0|(1076,[6],[1.0])|(10000,[164,266,3...|\n",
      "|  65670|           1|2010.0|(1076,[6],[1.0])|(10000,[697,1147,...|\n",
      "+-------+------------+------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Views\n",
    "\n",
    "Дополнительный файл по просмотрам передач с полями:\n",
    "- ts_start — время начала просмотра.\n",
    "- ts_end — время окончания просмотра.\n",
    "- item_type — тип просматриваемого контента:\n",
    "    - live — просмотр \"вживую\", в момент показа контента в эфире.\n",
    "    - pvr — просмотр в записи, после показа контента в эфире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- ts_start: string (nullable = true)\n",
      " |-- ts_end: string (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n",
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|  ts_start|    ts_end|item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|      0|7101053|1491409931|1491411600|     live|\n",
      "|      0|7101054|1491412481|1491451571|     live|\n",
      "|      0|7101054|1491411640|1491412481|     live|\n",
      "|      0|6184414|1486191290|1486191640|     live|\n",
      "|    257|4436877|1490628499|1490630256|     live|\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", StringType()),\n",
    "    StructField(\"ts_end\", StringType()),\n",
    "    StructField(\"item_type\", StringType()),\n",
    "])\n",
    "\n",
    "views = spark.read \\\n",
    "            .schema(schema) \\\n",
    "            .format(\"csv\") \\\n",
    "            .load(VIEWS_FILE, header=\"true\")\n",
    "views.printSchema()\n",
    "views.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(IntegerType())\n",
    "def item_type_live(x):\n",
    "    if x == \"live\":\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "@f.udf(IntegerType())\n",
    "def item_type_pvr(x):\n",
    "    if x == \"pvr\":\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------+-------------+\n",
      "|user_id|item_id|ts_delta|item_type_live|item_type_pvr|\n",
      "+-------+-------+--------+--------------+-------------+\n",
      "|      0|7101053|  1669.0|             1|            0|\n",
      "|      0|7101054| 39090.0|             1|            0|\n",
      "|      0|7101054|   841.0|             1|            0|\n",
      "|      0|6184414|   350.0|             1|            0|\n",
      "|    257|4436877|  1757.0|             1|            0|\n",
      "+-------+-------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views = views \\\n",
    "            .select(\"user_id\",\n",
    "                    \"item_id\",\n",
    "                    (col(\"ts_end\") - col(\"ts_start\")).alias(\"ts_delta\"),\n",
    "                    item_type_live(\"item_type\").alias(\"item_type_live\"),\n",
    "                    item_type_pvr(\"item_type\").alias(\"item_type_pvr\"))\n",
    "views.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------+------------------+--------------------+\n",
      "|user_id|    ts_delta_mean|      ts_delta_std|film_counts|    item_type_live|       item_type_pvr|\n",
      "+-------+-----------------+------------------+-----------+------------------+--------------------+\n",
      "| 819569|584.3846153846154|52.239095291965754|         13|               1.0|                 0.0|\n",
      "| 820786|3700.068508287293|3875.9279351057703|        905|0.5756906077348066|  0.4243093922651934|\n",
      "| 820904|       5029.84375| 6562.616136416318|        224|0.9419642857142857| 0.05803571428571429|\n",
      "| 821554|      4846.078125| 4378.617966999981|        128|               1.0|                 0.0|\n",
      "| 821806|4411.917594654788| 4821.358965756061|        449|0.9955456570155902|0.004454342984409799|\n",
      "+-------+-----------------+------------------+-----------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views4users = views \\\n",
    "            .groupBy(\"user_id\") \\\n",
    "            .agg(f.mean(\"ts_delta\").alias(\"ts_delta_mean\"),\n",
    "                 f.stddev(\"ts_delta\").alias(\"ts_delta_std\"),\n",
    "                 f.count(\"item_id\").alias(\"film_counts\"),\n",
    "                 (f.sum(\"item_type_live\") / f.count(\"item_id\")).alias(\"item_type_live\"),\n",
    "                 (f.sum(\"item_type_pvr\") / f.count(\"item_id\")).alias(\"item_type_pvr\")) \\\n",
    "            .cache()\n",
    "views4users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------------+\n",
      "|item_id|view_count|item_type|    mean_time_view|\n",
      "+-------+----------+---------+------------------+\n",
      "|6622608|        98|        0| 3180.030612244898|\n",
      "|6467228|       186|        0|4231.8494623655915|\n",
      "|7371253|        74|        0|1454.1486486486488|\n",
      "|6922406|      1558|        0|3287.0346598202823|\n",
      "|6826302|      3773|        0| 4285.416114497747|\n",
      "+-------+----------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views4items = views \\\n",
    "                .groupBy(\"item_id\") \\\n",
    "                .agg(f.count(\"user_id\").alias(\"view_count\"),\n",
    "                     f.min(\"item_type_live\").alias(\"item_type\"),\n",
    "                     f.mean(\"ts_delta\").alias(\"mean_time_view\"))\\\n",
    "                .cache()\n",
    "views4items.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Сборка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " item_id        | 74107                \n",
      " user_id        | 1654                 \n",
      " purchase       | 0                    \n",
      " users_buys     | 5                    \n",
      " users_per_buys | 0.001947040498442... \n",
      " items_buys     | 1                    \n",
      " items_per_buys | 7.575757575757576E-4 \n",
      " ts_delta_mean  | 1844.3989898989898   \n",
      " ts_delta_std   | 2691.822888743697    \n",
      " film_counts    | 198                  \n",
      " item_type_live | 0.6313131313131313   \n",
      " item_type_pvr  | 0.3686868686868687   \n",
      " view_count     | 0                    \n",
      " item_type      | 0                    \n",
      " mean_time_view | 0.0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.join(buys4users, on=\"user_id\", how=\"left\")\n",
    "dataset = dataset.join(buys4items, on=\"item_id\", how=\"left\")\n",
    "#dataset = dataset.join(items.drop(\"tfidf\"), on=\"item_id\", how=\"left\")\n",
    "dataset = dataset.join(views4users, on=\"user_id\", how=\"left\")\n",
    "dataset = dataset.join(views4items, on=\"item_id\", how=\"left\")\n",
    "dataset = dataset.na.fill(0)\n",
    "dataset.cache()\n",
    "dataset.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train&Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- users_buys: long (nullable = true)\n",
      " |-- users_per_buys: double (nullable = false)\n",
      " |-- items_buys: long (nullable = true)\n",
      " |-- items_per_buys: double (nullable = false)\n",
      " |-- ts_delta_mean: double (nullable = false)\n",
      " |-- ts_delta_std: double (nullable = false)\n",
      " |-- film_counts: long (nullable = true)\n",
      " |-- item_type_live: double (nullable = false)\n",
      " |-- item_type_pvr: double (nullable = false)\n",
      " |-- view_count: long (nullable = true)\n",
      " |-- item_type: integer (nullable = true)\n",
      " |-- mean_time_view: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = dataset.randomSplit([0.7, 0.3], seed=42)\n",
    "train = train.drop(\"user_id\", \"item_id\").cache()\n",
    "test = train.drop(\"user_id\", \"item_id\").cache()\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_buys\n",
      "users_per_buys\n",
      "items_buys\n",
      "items_per_buys\n",
      "ts_delta_mean\n",
      "ts_delta_std\n",
      "film_counts\n",
      "item_type_live\n",
      "item_type_pvr\n",
      "view_count\n",
      "item_type\n",
      "mean_time_view\n"
     ]
    }
   ],
   "source": [
    "feat_cols = [col for col in train.columns if col not in (\"purchase\")]\n",
    "print(*feat_cols, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\").setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"purchase\", maxIter=15)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.767638722841348"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7677539915790177"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977932531321091"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GBTClassifier(labelCol=\"purchase\", maxIter=15, maxDepth=10)\n",
    "\n",
    "pipeline_gb = Pipeline(stages=[\n",
    "    assembler,\n",
    "    gb\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-3f257f6bfc96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_gb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_gb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline_gb_model = pipeline_gb.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну как-то слишком долго......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_gb_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N. Сохранение результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " item_id        | 94814                \n",
      " user_id        | 1654                 \n",
      " purchase       | 0                    \n",
      " users_buys     | 5                    \n",
      " users_per_buys | 0.001947040498442... \n",
      " items_buys     | 1                    \n",
      " items_per_buys | 7.246376811594203E-4 \n",
      " ts_delta_mean  | 1844.3989898989898   \n",
      " ts_delta_std   | 2691.822888743697    \n",
      " film_counts    | 198                  \n",
      " item_type_live | 0.6313131313131313   \n",
      " item_type_pvr  | 0.3686868686868687   \n",
      " view_count     | 0                    \n",
      " item_type      | 0                    \n",
      " mean_time_view | 0.0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = result.join(buys4users, on=\"user_id\", how=\"left\")\n",
    "result = result.join(buys4items, on=\"item_id\", how=\"left\")\n",
    "#result = result.join(items.drop(\"tfidf\"), on=\"item_id\", how=\"left\")\n",
    "result = result.join(views4users, on=\"user_id\", how=\"left\")\n",
    "result = result.join(views4items, on=\"item_id\", how=\"left\")\n",
    "result = result.na.fill(0)\n",
    "result.cache()\n",
    "result.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " user_id  | 1654                 \n",
      " item_id  | 336                  \n",
      " purchase | [0.99862074167450... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = pipeline_model \\\n",
    "            .transform(result) \\\n",
    "            .select(\"user_id\",\n",
    "                    \"item_id\",\n",
    "                    col(\"probability\").alias(\"purchase\")) \\\n",
    "            .orderBy([\"user_id\", \"item_id\"])\n",
    "predict.cache()\n",
    "predict.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654</td>\n",
       "      <td>336</td>\n",
       "      <td>[0.9986207416745089, 0.0013792583254909833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654</td>\n",
       "      <td>678</td>\n",
       "      <td>[0.9986207416745089, 0.0013792583254909833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654</td>\n",
       "      <td>691</td>\n",
       "      <td>[0.9986207416745089, 0.0013792583254909833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654</td>\n",
       "      <td>696</td>\n",
       "      <td>[0.9984350433045214, 0.0015649566954786577]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654</td>\n",
       "      <td>763</td>\n",
       "      <td>[0.9985319878300942, 0.0014680121699059228]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id                                     purchase\n",
       "0     1654      336  [0.9986207416745089, 0.0013792583254909833]\n",
       "1     1654      678  [0.9986207416745089, 0.0013792583254909833]\n",
       "2     1654      691  [0.9986207416745089, 0.0013792583254909833]\n",
       "3     1654      696  [0.9984350433045214, 0.0015649566954786577]\n",
       "4     1654      763  [0.9985319878300942, 0.0014680121699059228]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = predict.toPandas()\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df[\"purchase\"] = predict_df[\"purchase\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654</td>\n",
       "      <td>336</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654</td>\n",
       "      <td>678</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654</td>\n",
       "      <td>691</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654</td>\n",
       "      <td>696</td>\n",
       "      <td>0.001565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654</td>\n",
       "      <td>763</td>\n",
       "      <td>0.001468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  purchase\n",
       "0     1654      336  0.001379\n",
       "1     1654      678  0.001379\n",
       "2     1654      691  0.001379\n",
       "3     1654      696  0.001565\n",
       "4     1654      763  0.001468"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id       int32\n",
       "item_id       int32\n",
       "purchase    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.sort_values([\"user_id\", \"item_id\"]).to_csv(\"lab03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
