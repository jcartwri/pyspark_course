{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 41138\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', sep='\\t', header=True)\n",
    "print('Data size: {}'. format(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract data from json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = spark.read.json(data.rdd.map(lambda r: r.user_json))\n",
    "\n",
    "parsed_json = json_data.select('visits.url', 'visits.timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 url|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|[http://zebra-zoy...|[1419688144068, 1...|\n",
      "|[http://sweetradi...|[1419717886224, 1...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_json.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use only one feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "parsed_json = parsed_json.withColumn(\"sites_cnt\", size(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "data = data.withColumn(\"id\", monotonically_increasing_id())\n",
    "parsed_json = parsed_json.withColumn(\"id\", monotonically_increasing_id())\n",
    "df = data.join(parsed_json, \"id\", \"outer\").drop(\"id\")\n",
    "# print(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('uid', 'gender', 'age', 'sites_cnt'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.age != '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('target', \n",
    "                    F.concat(F.col('gender'),F.lit('_'), F.col('age')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=['sites_cnt'\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['sites_cnt'\n",
    "              ],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_all = assembler.transform(df)\n",
    "df_all = df_all.select(['uid', 'target', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.withColumn('target', F.when(col('target') == 'M_25-34', 1).\\\n",
    "                  when(col('target') == 'M_35-44', 2).\\\n",
    "                       when(col('target') == 'M_18-24', 3).\\\n",
    "                            when(col('target') == 'M_45-54', 4).\\\n",
    "                                 when(col('target') == 'M_>=55', 5).\\\n",
    "                   when(col('target') == 'F_25-34', 6).\\\n",
    "                  when(col('target') == 'F_35-44', 7).\\\n",
    "                       when(col('target') == 'F_18-24', 8).\\\n",
    "                            when(col('target') == 'F_45-54', 9).\\\n",
    "                                 when(col('target') == 'F_>=55', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_all.sampleBy(\"target\", fractions={1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8,\n",
    "                                         6: 0.8, 7: 0.8, 8: 0.8, 9: 0.8, 10: 0.8}, seed=5757)\n",
    "val = df_all.join(train, on=['uid'], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|                 uid|target|features|\n",
      "+--------------------+------+--------+\n",
      "|d5288ed8-8253-45d...|     1|  [19.0]|\n",
      "|d52ab244-0a9c-434...|     1|  [20.0]|\n",
      "+--------------------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"target\", featuresCol=\"features\", numTrees=500, \n",
    "                            maxDepth=2, seed=2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 292 ms, sys: 149 ms, total: 441 ms\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model = rf_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23748251748251747\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_model)\n",
    "print(accuracy)\n",
    "# print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save('rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.23552447552447553  == 0.248 LB\n",
    "0.24125874125874125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To submit file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "predicted_labels = predictions_model.withColumn('prediction', F.when(col('prediction') == 1, 'M_25-34').\\\n",
    "                  when(col('prediction') == 2, 'M_35-44').\\\n",
    "                       when(col('prediction') == 3, 'M_18-24').\\\n",
    "                            when(col('prediction') == 4, 'M_45-54').\\\n",
    "                                 when(col('prediction') == 5, 'M_>=55').\\\n",
    "                   when(col('prediction') == 6, 'F_25-34').\\\n",
    "                  when(col('prediction') == 7, 'F_35-44').\\\n",
    "                       when(col('prediction') == 8, 'F_18-24').\\\n",
    "                            when(col('prediction') == 9, 'F_45-54').\\\n",
    "                                 when(col('prediction') == 10, 'F_>=55')).select('uid', 'prediction')\n",
    "\n",
    "split_col = f.split(predicted_labels['prediction'], '_')\n",
    "predicted_labels = predicted_labels.withColumn('gender', split_col.getItem(0))\n",
    "predicted_labels = predicted_labels.withColumn('age', split_col.getItem(1)).select('uid', 'gender', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-19 16:27 streaming/chk/chk_kafka/commits\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov         45 2021-03-19 16:27 streaming/chk/chk_kafka/metadata\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-23 09:35 streaming/chk/chk_kafka/offsets\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-19 16:27 streaming/chk/chk_kafka/sources\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls streaming/chk/chk_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov        438 2021-03-19 16:27 streaming/chk/chk_kafka/offsets/0\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov        441 2021-03-23 09:35 streaming/chk/chk_kafka/offsets/1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls streaming/chk/chk_kafka/offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read only one topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_params = {\n",
    "#     \"kafka.bootstrap.servers\": \"spark-master-1.newprolab.com:6667\",\n",
    "#     \"kafka.bootstrap.servers\": \"spark-node-1.newprolab.com:6667\",\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-master-1.newprolab.com:6667\",\n",
    "    \"subscribe\": \"input_dmitry.ulogov\",\n",
    "\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "\n",
    "#         \"startingOffsets\": \"\"\" { \"test_topic0\": { \"0\": 6 } } \"\"\",\n",
    "#     \"endingOffsets\": \"\"\" { \"test_topic0\": { \"0\": 9 } }  \"\"\"\n",
    "    \n",
    "    \"checkpointLocation\": \"streaming/chk/chk_kafka_ulogov_d_lab_04\"\n",
    "}\n",
    "\n",
    "\n",
    "# sdf = spark.read.format(\"kafka\").option(\"failOnDataLoss\", 'False').options(**kafka_params).load()\n",
    "sdf = spark.read.format(\"kafka\").options(**kafka_params).load()\n",
    "sdf.printSchema()\n",
    "sdf = sdf.select(             \n",
    "#     col(\"key\").cast(\"string\"), \\\n",
    "                              col(\"value\").cast(\"string\"),\\\n",
    "#                               col(\"topic\"), \\\n",
    "#                               col(\"partition\"), \\\n",
    "#                               col(\"offset\"), \\\n",
    "#                              col(\"timestamp\"), \\\n",
    "#                               col(\"timestampType\")\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, LongType\n",
    "json_schema = StructType(\n",
    "    [\n",
    "    StructField('uid', StringType()),\n",
    "    StructField('visits', StringType())])\n",
    "\n",
    "json_schema_vis = ArrayType(StructType([\n",
    "                                StructField('url', StringType()),\n",
    "                                StructField('timestamp', LongType())\n",
    "                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = sdf.withColumn('weblog', F.from_json(col('value').cast('string'), json_schema))\\\n",
    "                        .select('weblog.*')\\\n",
    "                        .select('uid', F.from_json(col('visits'), json_schema_vis).alias('visits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 uid|              visits|\n",
      "+--------------------+--------------------+\n",
      "|bd7a30e1-a25d-4cb...|[[http://www.inte...|\n",
      "|bd7a6f52-45db-49b...|[[https://www.pac...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_parsed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = sdf_parsed.select('uid','visits.url', 'visits.timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "sdf_parsed = sdf_parsed.withColumn(\"sites_cnt\", size(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = sdf_parsed.fillna(0, subset=['sites_cnt', \n",
    "#                           'days_passed', 'month_between', 'sites_per_day', 'year_equal_flg'\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = assembler.transform(sdf_parsed)\n",
    "sdf_parsed = sdf_parsed.select(['uid', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                 uid|features|\n",
      "+--------------------+--------+\n",
      "|bd7a30e1-a25d-4cb...|[2000.0]|\n",
      "|bd7a6f52-45db-49b...|[1284.0]|\n",
      "+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_parsed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kafka = rf_model.transform(sdf_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|                 uid|features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "|bd7a30e1-a25d-4cb...|[2000.0]|[0.0,121.27558947...|[0.0,0.2425511789...|       1.0|\n",
      "|bd7a6f52-45db-49b...|[1284.0]|[0.0,121.27558947...|[0.0,0.2425511789...|       1.0|\n",
      "+--------------------+--------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_kafka.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "pred_kafka = pred_kafka.withColumn(\"prediction\", pred_kafka[\"prediction\"].cast(FloatType()))\n",
    "pred_kafka = pred_kafka.withColumn(\"prediction\", pred_kafka[\"prediction\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "pred_kafka = pred_kafka.withColumn('prediction', F.when(col('prediction') == 1, 'M_25-34').\\\n",
    "                  when(col('prediction') == 2, 'M_35-44').\\\n",
    "                       when(col('prediction') == 3, 'M_18-24').\\\n",
    "                            when(col('prediction') == 4, 'M_45-54').\\\n",
    "                                 when(col('prediction') == 5, 'M_>=55').\\\n",
    "                   when(col('prediction') == 6, 'F_25-34').\\\n",
    "                  when(col('prediction') == 7, 'F_35-44').\\\n",
    "                       when(col('prediction') == 8, 'F_18-24').\\\n",
    "                            when(col('prediction') == 9, 'F_45-54').\\\n",
    "                                 when(col('prediction') == 10, 'F_>=55')).select('uid', 'prediction')\n",
    "\n",
    "split_col = f.split(pred_kafka['prediction'], '_')\n",
    "predicted_labels = pred_kafka.withColumn('gender', split_col.getItem(0))\n",
    "predicted_labels = predicted_labels.withColumn('age', split_col.getItem(1)).select('uid', 'gender', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|                 uid|gender|  age|\n",
      "+--------------------+------+-----+\n",
      "|bd7a30e1-a25d-4cb...|     M|25-34|\n",
      "|bd7a6f52-45db-49b...|     M|25-34|\n",
      "+--------------------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "finish = predicted_labels.select(to_json(struct(\"uid\",\"gender\",\"age\"))\\\n",
    "                                        .alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|value                                                                    |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"uid\":\"bd7a30e1-a25d-4cbf-a03f-61748cbe540e\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7a7fd9-ab06-42f5-bf0f-1cbb0463004c\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7c5d7a-0def-41d1-895f-fdb96c56c2d4\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7e54a2-0215-45cb-a869-9efebf250e38\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7e9797-4cdb-46e1-a540-f3ea010605ad\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7e9ec7-fb67-45eb-8ad3-209d01d15ae6\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd8056df-cc25-4b63-bc12-a46f888baa49\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd818690-73d2-445d-be5d-5c8f748dbb19\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd81e006-f059-4cdd-b716-3467c78d1312\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finish.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finish.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-de-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"dmitry.ulogov\"\n",
    "}\n",
    "# finish.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "#     .option(\"checkpointLocation\", \"streaming/chk/chk_kafka_ulogov_d_lab_04\")\\\n",
    "#     .outputMode(\"append\").start()\n",
    "\n",
    "finish.write.format(\"kafka\").options(**write_kafka_params).save()\n",
    "#     .outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотреть данные в кафке**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-de-master-1.newprolab.com:6667'\n",
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "INPUT_KAFKA_TOPIC = 'input_dmitry.ulogov'\n",
    "OUTPUT_KAFKA_TOPIC = 'dmitry.ulogov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': OUTPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'earliest'\n",
    "}\n",
    "kafka_sdf = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .load()\n",
    "    .cache()\n",
    ")\n",
    "kafka_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " value | [7B 22 75 69 64 22 3A 22 62 64 37 61 33 30 65 31 2D 61 32 35 64 2D 34 63 62 66 2D 61 30 33 66 2D 36 31 37 34 38 63 62 65 35 34 30 65 22 2C 22 67 65 6E 64 65 72 22 3A 22 4D 22 2C 22 61 67 65 22 3A 22 32 35 2D 33 34 22 7D] \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " value | [7B 22 75 69 64 22 3A 22 62 64 37 61 36 66 35 32 2D 34 35 64 62 2D 34 39 62 66 2D 39 30 66 32 2D 61 33 62 30 37 61 39 62 37 62 63 64 22 2C 22 67 65 6E 64 65 72 22 3A 22 4D 22 2C 22 61 67 65 22 3A 22 32 35 2D 33 34 22 7D] \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_sdf.select('value').show(2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 10000\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "| key|               value|        topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|dmitry.ulogov|        0| 10000|2021-03-23 14:30:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|dmitry.ulogov|        0| 10001|2021-03-23 14:30:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|dmitry.ulogov|        0| 10002|2021-03-23 14:30:...|            0|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('count',kafka_sdf.count())\n",
    "kafka_sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|value                                                                    |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"uid\":\"bd7a30e1-a25d-4cbf-a03f-61748cbe540e\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "|{\"uid\":\"bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd\",\"gender\":\"M\",\"age\":\"25-34\"}|\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_sdf.select( col(\"value\").cast(\"string\")).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    return df \\\n",
    "            .writeStream \\\n",
    "            .format(\"console\") \\\n",
    "            .trigger(processingTime=\"5 seconds\") \\\n",
    "            .option(\"truncate\", \"false\") \\\n",
    "            .option(\"numRows\", \"20\")\\\n",
    "#             .option(\"checkpointLocation\", \"streaming/chk/chk_kafka_ulogov_d_lab_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-19 16:27 streaming/chk/chk_kafka\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-23 12:28 streaming/chk/chk_kafka_ulogov_d_lab_04\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls streaming/chk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-de-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_dmitry.ulogov\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "\n",
    "parsed_sdf = kafka_sdf.select(\n",
    "#     col(\"key\").cast(\"string\"), \\\n",
    "                              col(\"value\").cast(\"string\"),\\\n",
    "#                               col(\"topic\"), \\\n",
    "#                               col(\"partition\"), \\\n",
    "#                               col(\"offset\"), \\\n",
    "#                              col(\"timestamp\"), \\\n",
    "#                               col(\"timestampType\")\n",
    ")                            \n",
    "\n",
    "\n",
    "parsed_sdf.printSchema()\n",
    "\n",
    "sink = create_console_sink(parsed_sdf)\n",
    "\n",
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-de-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_dmitry.ulogov\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "parsed_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = parsed_sdf.withColumn('weblog', F.from_json(col('value').cast('string'), json_schema))\\\n",
    "                        .select('weblog.*')\\\n",
    "                        .select('uid', F.from_json(col('visits'), json_schema_vis).alias('visits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = sdf_parsed.select('uid','visits.url', 'visits.timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "sdf_parsed = sdf_parsed.withColumn(\"sites_cnt\", size(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = sdf_parsed.fillna(0, subset=['sites_cnt', \n",
    "#                           'days_passed', 'month_between', 'sites_per_day', 'year_equal_flg'\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_parsed = assembler.transform(sdf_parsed)\n",
    "sdf_parsed = sdf_parsed.select(['uid', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kafka = rf_model.transform(sdf_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "pred_kafka = pred_kafka.withColumn(\"prediction\", pred_kafka[\"prediction\"].cast(FloatType()))\n",
    "pred_kafka = pred_kafka.withColumn(\"prediction\", pred_kafka[\"prediction\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "pred_kafka = pred_kafka.withColumn('prediction', F.when(col('prediction') == 1, 'M_25-34').\\\n",
    "                  when(col('prediction') == 2, 'M_35-44').\\\n",
    "                       when(col('prediction') == 3, 'M_18-24').\\\n",
    "                            when(col('prediction') == 4, 'M_45-54').\\\n",
    "                                 when(col('prediction') == 5, 'M_>=55').\\\n",
    "                   when(col('prediction') == 6, 'F_25-34').\\\n",
    "                  when(col('prediction') == 7, 'F_35-44').\\\n",
    "                       when(col('prediction') == 8, 'F_18-24').\\\n",
    "                            when(col('prediction') == 9, 'F_45-54').\\\n",
    "                                 when(col('prediction') == 10, 'F_>=55')).select('uid', 'prediction')\n",
    "\n",
    "split_col = f.split(pred_kafka['prediction'], '_')\n",
    "predicted_labels = pred_kafka.withColumn('gender', split_col.getItem(0))\n",
    "predicted_labels = predicted_labels.withColumn('age', split_col.getItem(1)).select('uid', 'gender', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "finish = predicted_labels.select(to_json(struct(\"uid\",\"gender\",\"age\"))\\\n",
    "                                        .alias(\"column\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sq.isActive\n",
    "\n",
    "sq.status\n",
    "\n",
    "sq.lastProgress\n",
    "\n",
    "kill_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5f603287f0>"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"dmitry.ulogov\"\n",
    "}\n",
    "finish.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
