{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().get(\"spark.executor.instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Engineering\n",
    "2. Feature correlation\n",
    "-  Feature Importance\n",
    "3. Model Evaluation\n",
    "4. HyperParameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 5032624\n",
      "test size: 2156840\n",
      "items size: 635568\n",
      "views size: 20845607\n"
     ]
    }
   ],
   "source": [
    "train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", header=True)\n",
    "print('train size: {}'. format(train.count()))\n",
    "\n",
    "test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", header=True)\n",
    "print('test size: {}'. format(test.count()))\n",
    "\n",
    "items = spark.read.options(delimiter='\\t').csv(\"/labs/slaba03/laba03_items.csv\", header=True)\n",
    "print('items size: {}'. format(items.count()))\n",
    "\n",
    "views = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", header=True)\n",
    "print('views size: {}'. format(views.count()))\n",
    "\n",
    "# merge to one dataframe (train+test)\n",
    "df_all = train.union(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_all.join(items, on=['item_id'], how='left').cache()\n",
    "df_all = tmp.join(views, on=['item_id', 'user_id'], how='left').cache()\n",
    "del tmp\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "df_all = df_all.withColumn(\"purchase\", df_all[\"purchase\"].cast(IntegerType()))\n",
    "df_all = df_all.withColumn(\"year\", df_all[\"year\"].cast(IntegerType()))\n",
    "\n",
    "df_all = df_all.select(['item_id', 'user_id', 'purchase', 'year', 'genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Доля покупок конкретного фильма\n",
    "- Доля покупок у конкретного пользователя\n",
    "- Доля покупок пользователя по категориям\n",
    "- Доля покупок пользователя по годам\n",
    "- категориальные признаки из жанров 1,2,3\n",
    "- Доля покупок пользователя по категориям - для genre 1, 2, 3\n",
    "- Доля покупок пользователя по годам - разбить фильмы по бинам в годах\n",
    "- Посчитать статистики для бинов по возрасту фильма\n",
    "- флаг 0,1 - покупал ли этот пользователь хотя бы раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split \n",
    "\n",
    "df_all = df_all.withColumn(\"genre_1\", split(col(\"genres\"), \",\")\\\n",
    "                     .getItem(0))\n",
    "df_all = df_all.withColumn(\"genre_2\", split(col(\"genres\"), \",\")\\\n",
    "                     .getItem(1))\n",
    "df_all = df_all.withColumn(\"genre_3\", split(col(\"genres\"), \",\")\\\n",
    "                     .getItem(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate target 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1 = df_all.filter(df_all.purchase == '1')\n",
    "df_all_0 = df_all.filter(df_all.purchase == '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля покупок конкретного фильма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_sales = df_all_1.groupBy(\"item_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"sale_count\")\\\n",
    "                .orderBy(\"sale_count\", ascending=False)\n",
    "\n",
    "film_shows = df_all_0.groupBy(\"item_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"show_count\")\\\n",
    "                .orderBy(\"show_count\", ascending=False)\n",
    "\n",
    "films_voronka = film_sales.join(film_shows, on=['item_id'], how='left').cache()\n",
    "\n",
    "films_voronka = films_voronka.withColumn('film_percent_sales', \\\n",
    "                         films_voronka['sale_count']  / films_voronka['show_count'] )\\\n",
    "                        .select(['item_id', 'film_percent_sales'])\n",
    "\n",
    "df_all = df_all.join(films_voronka, on=['item_id'], how='left').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля покупок у конкретного пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_sales = df_all_1.groupBy(\"user_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"sale_count\")\\\n",
    "                .orderBy(\"sale_count\", ascending=False)\n",
    "\n",
    "person_shows = df_all_0.groupBy(\"user_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"show_count\")\\\n",
    "                .orderBy(\"show_count\", ascending=False)\n",
    "\n",
    "person_voronka = person_sales.join(person_shows, on=['user_id'], how='left').cache()\n",
    "\n",
    "person_voronka = person_voronka.withColumn('user_percent_sales', \\\n",
    "                         person_voronka['sale_count']  / person_voronka['show_count'] )\\\n",
    "                        .select(['user_id', 'user_percent_sales'])\n",
    "\n",
    "df_all = df_all.join(person_voronka, on=['user_id'], how='left').cache()\n",
    "\n",
    "df_all = df_all.select(['user_id', 'item_id', 'purchase', 'year', 'genres', 'film_percent_sales', 'user_percent_sales',\n",
    "               'genre_1', 'genre_2', 'genre_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Доля покупок фильмов пользователем по годам выпуска**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year\").agg(count(\"*\")).alias('cnt_year_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year\"), col('count(1)').alias('year_cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year\").agg(count(\"*\")).alias('cnt_year_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year\"), col('count(1)').alias('year_cnt_1'))\n",
    "\n",
    "film_users_voronka = temp_0.join(temp_1, on=['user_id', 'year'], how='left').cache()\n",
    "\n",
    "film_users_voronka = film_users_voronka.withColumn('user_percent_years', \\\n",
    "                         film_users_voronka['year_cnt_1']  / film_users_voronka['year_cnt_0'] )\\\n",
    "                        .select(['user_id', 'year', 'user_percent_years'])\n",
    "\n",
    "df_all = df_all.join(film_users_voronka, on=['user_id', 'year'], how='left').cache()\n",
    "\n",
    "del temp_0, temp_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Доля покупок фильмов пользователем в зависимости от жанра**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_0 = df_all_0.groupBy(\"user_id\", \"genre_1\").agg(count(\"*\")).alias('cnt_genre_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"genre_1\"), col('count(1)').alias('year_cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"genre_1\").agg(count(\"*\")).alias('cnt_genre_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"genre_1\"), col('count(1)').alias('year_genre_1'))\n",
    "\n",
    "genres_users_voronka = temp_0.join(temp_1, on=['user_id', 'genre_1'], how='left').cache()\n",
    "\n",
    "genres_users_voronka = genres_users_voronka.withColumn('user_percent_genres', \\\n",
    "                         genres_users_voronka['year_genre_1']  / genres_users_voronka['year_cnt_0'] )\\\n",
    "                        .select(['user_id', 'genre_1', 'user_percent_genres'])\n",
    "\n",
    "df_all = df_all.join(genres_users_voronka, on=['user_id', 'genre_1'], how='left').cache()\n",
    "\n",
    "del temp_0, temp_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_0 = df_all_0.groupBy(\"user_id\", \"genre_2\").agg(count(\"*\")).alias('cnt_genre_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"genre_2\"), col('count(1)').alias('cnt_genre_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"genre_2\").agg(count(\"*\")).alias('cnt_genre_2')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"genre_2\"), col('count(1)').alias('cnt_genre_1'))\n",
    "\n",
    "genres_users_voronka = temp_0.join(temp_1, on=['user_id', 'genre_2'], how='left').cache()\n",
    "\n",
    "genres_users_voronka_2 = genres_users_voronka.withColumn('user_percent_genres_2', \\\n",
    "                         genres_users_voronka['cnt_genre_0']  / genres_users_voronka['cnt_genre_1'] )\\\n",
    "                        .select(['user_id', 'genre_2', 'user_percent_genres_2'])\n",
    "\n",
    "df_all = df_all.join(genres_users_voronka_2, on=['user_id', 'genre_2'], how='left').cache()\n",
    "\n",
    "del temp_0, temp_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_0 = df_all_0.groupBy(\"user_id\", \"genre_3\").agg(count(\"*\")).alias('cnt_genre_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"genre_3\"), col('count(1)').alias('cnt_genre_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"genre_3\").agg(count(\"*\")).alias('cnt_genre_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"genre_3\"), col('count(1)').alias('cnt_genre_1'))\n",
    "\n",
    "genres_users_voronka = temp_0.join(temp_1, on=['user_id', 'genre_3'], how='left').cache()\n",
    "\n",
    "genres_users_voronka_3 = genres_users_voronka.withColumn('user_percent_genres_3', \\\n",
    "                         genres_users_voronka['cnt_genre_0']  / genres_users_voronka['cnt_genre_1'] )\\\n",
    "                        .select(['user_id', 'genre_3', 'user_percent_genres_3'])\n",
    "\n",
    "df_all = df_all.join(genres_users_voronka_3, on=['user_id', 'genre_3'], how='left').cache()\n",
    "\n",
    "del temp_0, temp_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bins"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "old < 1950\n",
    "old_middle 1950-1970\n",
    "middle 1970-1990\n",
    "middle_new 1990-2000\n",
    "new 2000-2010\n",
    "newest >2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_all = df_all.withColumn('year_bin', F.when(col('year') <= 1950, 'old').\\\n",
    "                  when((col('year') > 1950) & (col('year') <= 1970), 'old_middle').\\\n",
    "                  when((col('year') > 1970) & (col('year') <= 1990), 'middle').\\\n",
    "                  when((col('year') >1990) & (col('year') <= 2000), 'middle_new').\\\n",
    "                  when((col('year') > 2000) & (col('year') <= 2010), 'new').\\\n",
    "                  when(col('year') > 2010, 'newest')\\\n",
    "                      .otherwise('hz'))#.select(['year', 'year_bin']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Статистика покупок фильмов по годам**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "film_types:\n",
    "old\n",
    "old_middle\n",
    "middle\n",
    "middle_new\n",
    "new\n",
    "newest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_type = 'old'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_1 = film_bin_stat.withColumn('film_bin_stat_1', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_1'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "film_type = 'old_middle'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_2 = film_bin_stat.withColumn('film_bin_stat_2', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_2'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "film_type = 'middle'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_3 = film_bin_stat.withColumn('film_bin_stat_3', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_3'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "film_type = 'middle_new'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_4 = film_bin_stat.withColumn('film_bin_stat_4', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_4'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "film_type = 'new'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_5 = film_bin_stat.withColumn('film_bin_stat_5', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_5'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "film_type = 'newest'\n",
    "df_all_1 = df_all.filter((df_all.purchase == '1') & (df_all.year_bin == film_type))\n",
    "df_all_0 = df_all.filter((df_all.purchase == '0') & (df_all.year_bin == film_type))\n",
    "\n",
    "temp_0 = df_all_0.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_0')\n",
    "temp_0 = temp_0.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_0'))\n",
    "\n",
    "temp_1 = df_all_1.groupBy(\"user_id\", \"year_bin\").agg(count(\"*\")).alias('cnt_1')\n",
    "temp_1 = temp_1.select(col(\"user_id\"), col(\"year_bin\"), col('count(1)').alias('cnt_1'))\n",
    "\n",
    "film_bin_stat = temp_0.join(temp_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "\n",
    "film_bin_stat_6 = film_bin_stat.withColumn('film_bin_stat_6', \\\n",
    "                         film_bin_stat['cnt_1']  / film_bin_stat['cnt_0'] )\\\n",
    "                        .select(['user_id', 'year_bin', 'film_bin_stat_6'])\n",
    "\n",
    "del temp_0, temp_1\n",
    "\n",
    "df_all = df_all.join(film_bin_stat_1, on=['user_id', 'year_bin'], how='left').cache()\n",
    "df_all = df_all.join(film_bin_stat_2, on=['user_id', 'year_bin'], how='left').cache()\n",
    "df_all = df_all.join(film_bin_stat_3, on=['user_id', 'year_bin'], how='left').cache()\n",
    "df_all = df_all.join(film_bin_stat_4, on=['user_id', 'year_bin'], how='left').cache()\n",
    "df_all = df_all.join(film_bin_stat_5, on=['user_id', 'year_bin'], how='left').cache()\n",
    "df_all = df_all.join(film_bin_stat_6, on=['user_id', 'year_bin'], how='left').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Флаг, что пользователь покупал хотя бы один раз**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1 = df_all.filter(df_all.purchase == '1').select(['user_id', 'purchase'])\n",
    "df_all_1 = df_all_1.dropDuplicates(['user_id'])\n",
    "user_sell_flags = df_all_1.withColumnRenamed('purchase', 'sell_flag')\n",
    "\n",
    "df_all = df_all.join(user_sell_flags, on=['user_id'], how='left').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как интенсивно покупает пользователь**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1 = df_all.filter(df_all.purchase == '1').select(['user_id', 'purchase'])\n",
    "\n",
    "sale_user_count = df_all_1.groupBy(\"user_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"sale_user_count\")\n",
    "df_all = df_all.join(sale_user_count, on=['user_id'], how='left').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Покупаемость item_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1 = df_all.filter(df_all.purchase == '1').select(['item_id', 'purchase'])\n",
    "\n",
    "sale_item_count = df_all_1.groupBy(\"item_id\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"sale_item_count\")\n",
    "df_all = df_all.join(sale_item_count, on=['item_id'], how='left').cache()\n",
    "\n",
    "# df_all = df_all.select(['user_id', 'item_id', 'purchase', 'year', 'film_percent_sales', 'user_percent_sales'])\n",
    "\n",
    "df_all = df_all.select(['user_id','item_id','purchase', 'year_bin', 'genre_3', 'genre_2', 'genre_1', \n",
    "                        'year',  'film_percent_sales', 'user_percent_sales', 'user_percent_years', \n",
    "                        'user_percent_genres', 'user_percent_genres_2', 'user_percent_genres_3', \n",
    "                        'film_bin_stat_1', 'film_bin_stat_2', 'film_bin_stat_3', 'film_bin_stat_4', \n",
    "                        'film_bin_stat_5', 'film_bin_stat_6', 'sell_flag', 'sale_user_count', 'sale_item_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done_1\n",
      "done_2\n",
      "done_3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"genre_1\", outputCol=\"genre_1_cat\")\n",
    "df_all = indexer.setHandleInvalid(\"keep\").fit(df_all).transform(df_all)\n",
    "print('done_1')\n",
    "indexer = StringIndexer(inputCol=\"genre_2\", outputCol=\"genre_2_cat\")\n",
    "df_all = indexer.setHandleInvalid(\"keep\").fit(df_all).transform(df_all)\n",
    "print('done_2')\n",
    "indexer = StringIndexer(inputCol=\"genre_3\", outputCol=\"genre_3_cat\")\n",
    "df_all = indexer.setHandleInvalid(\"keep\").fit(df_all).transform(df_all)\n",
    "print('done_3')\n",
    "indexer = StringIndexer(inputCol=\"year_bin\", outputCol=\"year_bin_cat\")\n",
    "df_all = indexer.setHandleInvalid(\"keep\").fit(df_all).transform(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = df_all.fillna(0, subset=['year', 'film_percent_sales', 'user_percent_sales'])\n",
    "df_all = df_all.fillna(0, subset=['year_bin_cat', 'genre_3_cat', 'genre_2_cat', 'genre_1_cat', \n",
    "                        'year',  'film_percent_sales', 'user_percent_sales', 'user_percent_years', \n",
    "                        'user_percent_genres', 'user_percent_genres_2', 'user_percent_genres_3', \n",
    "                        'film_bin_stat_1', 'film_bin_stat_2', 'film_bin_stat_3', 'film_bin_stat_4', \n",
    "                        'film_bin_stat_5', 'film_bin_stat_6', 'sell_flag', 'sale_user_count', 'sale_item_count'])\n",
    "\n",
    "df_all = df_all.select(['user_id','item_id','purchase', 'year_bin_cat', 'genre_3_cat', 'genre_2_cat', 'genre_1_cat', \n",
    "                        'year',  'film_percent_sales', 'user_percent_sales', 'user_percent_years', \n",
    "                        'user_percent_genres', 'user_percent_genres_2', 'user_percent_genres_3', \n",
    "                        'film_bin_stat_1', 'film_bin_stat_2', 'film_bin_stat_3', 'film_bin_stat_4', \n",
    "                        'film_bin_stat_5', 'film_bin_stat_6', 'sell_flag', 'sale_user_count', 'sale_item_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df_all = df_all.withColumn(\"year_bin_cat\", df_all[\"year_bin_cat\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"genre_3_cat\", df_all[\"genre_3_cat\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"genre_2_cat\", df_all[\"genre_2_cat\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"genre_1_cat\", df_all[\"genre_1_cat\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_percent_sales\", df_all[\"film_percent_sales\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"user_percent_sales\", df_all[\"user_percent_sales\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"user_percent_years\", df_all[\"user_percent_years\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"user_percent_genres\", df_all[\"user_percent_genres\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"user_percent_genres_2\", df_all[\"user_percent_genres_2\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"user_percent_genres_3\", df_all[\"user_percent_genres_3\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_1\", df_all[\"film_bin_stat_1\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_2\", df_all[\"film_bin_stat_2\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_3\", df_all[\"film_bin_stat_3\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_4\", df_all[\"film_bin_stat_4\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_5\", df_all[\"film_bin_stat_5\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_6\", df_all[\"film_bin_stat_6\"].cast(FloatType()))\n",
    "\n",
    "df_all = df_all.withColumn(\"sale_user_count\", df_all[\"sale_user_count\"].cast(FloatType()))\n",
    "df_all = df_all.withColumn(\"sale_item_count\", df_all[\"sale_item_count\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "df_all = df_all.withColumn(\"year_bin_cat\", func.round(df_all[\"year_bin_cat\"], 6))\n",
    "df_all = df_all.withColumn(\"genre_3_cat\", func.round(df_all[\"genre_3_cat\"], 6))\n",
    "df_all = df_all.withColumn(\"genre_2_cat\", func.round(df_all[\"genre_2_cat\"], 6))\n",
    "df_all = df_all.withColumn(\"genre_1_cat\", func.round(df_all[\"genre_1_cat\"], 6))\n",
    "df_all = df_all.withColumn(\"film_percent_sales\", func.round(df_all[\"film_percent_sales\"], 6))\n",
    "df_all = df_all.withColumn(\"user_percent_sales\", func.round(df_all[\"user_percent_sales\"], 6))\n",
    "df_all = df_all.withColumn(\"user_percent_years\", func.round(df_all[\"user_percent_years\"], 6))\n",
    "df_all = df_all.withColumn(\"user_percent_genres\", func.round(df_all[\"user_percent_genres\"], 6))\n",
    "df_all = df_all.withColumn(\"user_percent_genres_2\", func.round(df_all[\"user_percent_genres_2\"], 6))\n",
    "df_all = df_all.withColumn(\"user_percent_genres_3\", func.round(df_all[\"user_percent_genres_3\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_1\", func.round(df_all[\"film_bin_stat_1\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_2\", func.round(df_all[\"film_bin_stat_2\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_3\", func.round(df_all[\"film_bin_stat_3\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_4\", func.round(df_all[\"film_bin_stat_4\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_5\", func.round(df_all[\"film_bin_stat_5\"], 6))\n",
    "df_all = df_all.withColumn(\"film_bin_stat_6\", func.round(df_all[\"film_bin_stat_6\"], 6))\n",
    "\n",
    "df_all = df_all.withColumn(\"sale_user_count\", func.round(df_all[\"sale_user_count\"], 6))\n",
    "df_all = df_all.withColumn(\"sale_item_count\", func.round(df_all[\"sale_item_count\"], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['year_bin_cat', 'genre_3_cat', 'genre_2_cat', 'genre_1_cat', \n",
    "                        'year',  'film_percent_sales', 'user_percent_sales', 'user_percent_years', \n",
    "                        'user_percent_genres', 'user_percent_genres_2', 'user_percent_genres_3', \n",
    "                        'film_bin_stat_1', 'film_bin_stat_2', 'film_bin_stat_3', 'film_bin_stat_4', \n",
    "                        'film_bin_stat_5', 'film_bin_stat_6', 'sell_flag', 'sale_user_count', 'sale_item_count'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_all_transformed = assembler.transform(df_all)\n",
    "\n",
    "\n",
    "df_all_transformed = df_all_transformed.select(['user_id', 'item_id', 'purchase','features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "normalized_data = normalizer.transform(df_all_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df_all_transformed.filter(df_all_transformed.purchase.isNotNull())\n",
    "# test = df_all_transformed.filter(df_all_transformed.purchase.isNull())\n",
    "\n",
    "train = normalized_data.filter(normalized_data.purchase.isNotNull())\n",
    "test = normalized_data.filter(normalized_data.purchase.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- normFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train df to check model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=5757)\n",
    "val_sample = train.join(train_sample, on=['user_id', 'item_id'], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GBDT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbt_model = gbt.fit(train_sample)\n",
    "# gbt_model = gbt.fit(train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(18, {0: 0.0089, 1: 0.0611, 2: 0.0551, 3: 0.0257, 4: 0.0269, 5: 0.1776, 6: 0.129, 7: 0.1504, 8: 0.1004, 9: 0.1712, 10: 0.0896, 11: 0.0011, 12: 0.0004, 14: 0.0012, 16: 0.0015})"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt = gbt_model.transform(val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \\\n",
    "                                          labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions_gbt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.9945205654506661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol=\"normFeatures\", labelCol='purchase', maxIter=15, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 7.4 ms, total: 137 ms\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = lr.fit(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = lr_model.transform(val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636491532290061"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='LogisticRegression_a19ae7bef07a', name='maxIter', doc='maximum number of iterations (>= 0)')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.maxIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9544210565111111\n",
    "\n",
    "0.951874561786903\n",
    "\n",
    "0.9473850805864773"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol='purchase', maxIter=15)\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \\\n",
    "                                          labelCol=\"purchase\", metricName='areaUnderROC'),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LR - short 3 features = 0.9399063570059675\n",
    "LR - short many features = 0.9515689510977573\n",
    "\n",
    "GBT - short many features = 0.9945205654506661\n",
    "GBT - short many features = 0.751009186711 (LB)\n",
    "\n",
    "LR - short many features = 0.9544210565111111\n",
    "LR - short many features =  (LB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Make prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_test = gbt_model.transform(test)\n",
    "predictions_test = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|purchase|            features|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+----------+\n",
      "| 867363|  73483|    null|(18,[0,1,2,3,4,5,...|[1.54369627086296...|[0.95636969378535...|       0.0|\n",
      "| 867363|  10558|    null|(18,[0,1,2,3,4,5,...|[1.54369627086296...|[0.95636969378535...|       0.0|\n",
      "+-------+-------+--------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------------------------------+\n",
      "|user_id|item_id|probability                               |\n",
      "+-------+-------+------------------------------------------+\n",
      "|871154 |100140 |[0.9988080672995057,0.0011919327004943977]|\n",
      "|878352 |100140 |[0.9987286834814678,0.001271316518532219] |\n",
      "+-------+-------+------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.select(['user_id', 'item_id', 'probability']).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "lastelement=f.udf(lambda v:float(v[1]),FloatType())\n",
    "predictions_test = predictions_test.withColumn(\"purchase\", lastelement(\"probability\")).\\\n",
    "                    select(['user_id', 'item_id', 'purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- purchase: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_sorted = predictions_test.orderBy([\"user_id\", \"item_id\"], ascending=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_sorted_repart = predictions_test_sorted.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_test_sorted.write.csv('lab03.csv')\n",
    "predictions_test_sorted_repart.write.csv('lab03_13_03_1500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move submission from hdfs to local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\r\n",
      "drwx------   - dmitry.ulogov dmitry.ulogov          0 2021-03-13 15:33 /user/dmitry.ulogov/.Trash\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-13 11:40 /user/dmitry.ulogov/.sparkStaging\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 17:31 /user/dmitry.ulogov/gbt_model\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov         84 2021-02-26 21:44 /user/dmitry.ulogov/lab01.json\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov        458 2021-03-05 15:32 /user/dmitry.ulogov/lab02.json\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 20:36 /user/dmitry.ulogov/lab03\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-13 15:32 /user/dmitry.ulogov/lab03_13_03_1500\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/dmitry.ulogov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/13 15:33:36 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/dmitry.ulogov/lab03.csv' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/dmitry.ulogov/.Trash/Current/user/dmitry.ulogov/lab03.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/dmitry.ulogov/lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/dmitry.ulogov/lab03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov          0 2021-03-13 15:32 /user/dmitry.ulogov/lab03_13_03_1500/_SUCCESS\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov   55481663 2021-03-13 15:32 /user/dmitry.ulogov/lab03_13_03_1500/lab03.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/dmitry.ulogov/lab03_13_03_1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mv /user/dmitry.ulogov/lab03_13_03_1500/part-00000-177647e9-5f42-4b7c-9d43-e38b68d94505-c000.csv /user/dmitry.ulogov/lab03_13_03_1500/lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -get /user/dmitry.ulogov/lab03_13_03_1500/lab03.csv /data/home/dmitry.ulogov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/dmitry.ulogov/lab03_13_03_1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sub = pd.read_csv('lab03.csv', header=None)\n",
    "sub.columns = ['user_id', 'item_id', 'purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.sort_values(by=['user_id', 'item_id'])\n",
    "sub.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov          0 2021-03-12 20:24 /user/dmitry.ulogov/lab03/_SUCCESS\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov   53341201 2021-03-12 20:24 /user/dmitry.ulogov/lab03/part-00000-1a57cf5d-6e25-445c-ab6e-6a08e160ea04-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/dmitry.ulogov/lab03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mv /user/dmitry.ulogov/lab03/part-00000-1a57cf5d-6e25-445c-ab6e-6a08e160ea04-c000.csv /user/dmitry.ulogov/lab03/lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov          0 2021-03-12 20:24 /user/dmitry.ulogov/lab03/_SUCCESS\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov   53341201 2021-03-12 20:24 /user/dmitry.ulogov/lab03/lab03.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/dmitry.ulogov/lab03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cp /user/dmitry.ulogov/lab03/lab03.csv /user/dmitry.ulogov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\r\n",
      "drwx------   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 15:32 /user/dmitry.ulogov/.Trash\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 18:50 /user/dmitry.ulogov/.sparkStaging\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 17:31 /user/dmitry.ulogov/gbt_model\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov         84 2021-02-26 21:44 /user/dmitry.ulogov/lab01.json\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov        458 2021-03-05 15:32 /user/dmitry.ulogov/lab02.json\r\n",
      "drwxr-xr-x   - dmitry.ulogov dmitry.ulogov          0 2021-03-12 20:36 /user/dmitry.ulogov/lab03\r\n",
      "-rw-r--r--   3 dmitry.ulogov dmitry.ulogov   53341201 2021-03-12 20:39 /user/dmitry.ulogov/lab03.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/dmitry.ulogov/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_sub = pd.read_csv('lab03.csv',header=None)\n",
    "my_sub.columns = ['user_id', 'item_id', 'purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sub = my_sub.sort_values(by=['user_id', 'item_id'], ascending = True)\n",
    "my_sub.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
