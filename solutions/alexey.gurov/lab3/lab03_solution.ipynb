{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"alexey gurov lab3\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.ml.feature import HashingTF\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import NaN\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, \\\n",
    "                              IntegerType, DateType, FloatType, ArrayType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4055\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>alexey gurov lab3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5a292ccdd8>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def split(x):\n",
    "    return list(x.split(' '))\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_type(x):\n",
    "    return type(x)\n",
    "\n",
    "@udf(returnType=ArrayType(DoubleType()))\n",
    "def get_features(a, b, c, d, e):\n",
    "    return list([float(a), float(b), float(c)]) + list(map(float, Vectors.dense(a).toArray())) + list(map(float, Vectors.dense(b).toArray()))\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def add_sparce(a, b):\n",
    "    return list(map(float, Vectors.dense(a).toArray())) + list(map(float, Vectors.dense(b).toArray()))\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(DoubleType()))\n",
    "def add_features(a, b, c):\n",
    "    return list([float(a), float(b), float(c)])\n",
    "\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def to_int(x):\n",
    "    return int(float(x))\n",
    "\n",
    "my_func = f.udf(lambda a, b, c, d, e: Vectors.sparse(list([float(a), float(b), float(c)]) + list(map(float, Vectors.dense(a).toArray())) + list(map(float, Vectors.dense(b).toArray()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id\tchannel_id\tdatetime_availability_start\tdatetime_availability_stop\tdatetime_show_start\tdatetime_show_stop\tcontent_type\ttitle\tyear\tgenres\tregion_id\r\n",
      "65667\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tна пробах только девушки (all girl auditions)\t2013.0\tЭротика\t\r\n",
      "65669\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tскуби ду: эротическая пародия (scooby doo: a xxx parody)\t2011.0\tЭротика\t\r\n",
      "65668\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tгорячие девочки для горячих девочек (hot babes 4 hot babes)\t2011.0\tЭротика\t\r\n",
      "65671\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tсоблазнительницы женатых мужчин (top heavy homewreckers)\t2011.0\tЭротика\t\r\n",
      "65670\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tсекретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody)\t2010.0\tЭротика\t\r\n",
      "65809\t\t1970-01-01T00:00:00Z\t2099-12-31"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /labs/slaba03/laba03_items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = spark.read.csv('/labs/slaba03/laba03_train.csv', schema=data_train_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = spark.read.csv('/labs/slaba03/laba03_test.csv', schema=data_test_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2156840, 2)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.toPandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|item_id|\n",
      "+-------+-------+\n",
      "|   1654|  94814|\n",
      "|   1654|  93629|\n",
      "|   1654|   9980|\n",
      "|   1654|  95099|\n",
      "|   1654|  11265|\n",
      "+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", IntegerType()),\n",
    "    StructField(\"datetime_availability_start\", DateType()),\n",
    "    StructField(\"datetime_availability_stop\", DateType()),\n",
    "    StructField(\"datetime_show_start\", DateType()),\n",
    "    StructField(\"datetime_show_stop\", DateType()),\n",
    "    StructField(\"content_type\", IntegerType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", IntegerType()),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items = spark.read.csv('/labs/slaba03/laba03_items.csv', header=True, sep='\\t')\\\n",
    "                .withColumnRenamed('item_id', 'items_item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "|items_item_id|channel_id|datetime_availability_start|datetime_availability_stop|datetime_show_start|datetime_show_stop|content_type|title                                                                                 |year  |genres |region_id|\n",
      "+-------------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "|65667        |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |на пробах только девушки (all girl auditions)                                         |2013.0|Эротика|null     |\n",
      "|65669        |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |скуби ду: эротическая пародия (scooby doo: a xxx parody)                              |2011.0|Эротика|null     |\n",
      "|65668        |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |горячие девочки для горячих девочек (hot babes 4 hot babes)                           |2011.0|Эротика|null     |\n",
      "|65671        |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |соблазнительницы женатых мужчин (top heavy homewreckers)                              |2011.0|Эротика|null     |\n",
      "|65670        |null      |1970-01-01T00:00:00Z       |2018-01-01T00:00:00Z      |null               |null              |1           |секретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody)|2010.0|Эротика|null     |\n",
      "+-------------+----------+---------------------------+--------------------------+-------------------+------------------+------------+--------------------------------------------------------------------------------------+------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|title                                                                                 |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|на пробах только девушки (all girl auditions)                                         |\n",
      "|скуби ду: эротическая пародия (scooby doo: a xxx parody)                              |\n",
      "|горячие девочки для горячих девочек (hot babes 4 hot babes)                           |\n",
      "|соблазнительницы женатых мужчин (top heavy homewreckers)                              |\n",
      "|секретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody)|\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_items.select('title').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_views_programmes = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|  ts_start|    ts_end|item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|      0|7101053|1491409931|1491411600|     live|\n",
      "|      0|7101054|1491412481|1491451571|     live|\n",
      "|      0|7101054|1491411640|1491412481|     live|\n",
      "|      0|6184414|1486191290|1486191640|     live|\n",
      "|    257|4436877|1490628499|1490630256|     live|\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_views_programmes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_united = data_train.join(data_items, f.col('item_id') == f.col('items_item_id'))\\\n",
    "                        .select('user_id', 'item_id', 'content_type', 'title', 'year', 'genres', 'purchase')\\\n",
    "                        .filter((f.col('content_type') == 1))\\\n",
    "                        .fillna({'genres': '', 'year': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_united = data_test.join(data_items, f.col('item_id') == f.col('items_item_id'), 'left_outer')\\\n",
    "                        .select('user_id', 'item_id', 'content_type', 'title', 'year', 'genres')\\\n",
    "                        .filter(f.col('content_type') == 1)\\\n",
    "                        .fillna({'genres': '', 'year': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+--------------------+------+--------------------+--------+\n",
      "|user_id|item_id|content_type|               title|  year|              genres|purchase|\n",
      "+-------+-------+------------+--------------------+------+--------------------+--------+\n",
      "| 520446|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 556825|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 566701|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 613775|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 619378|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 625678|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 632495|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 636572|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 639612|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 668112|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 703514|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 711308|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 717302|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 719149|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 725821|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 728960|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 729785|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 731490|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 732411|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "| 735387|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|       0|\n",
      "+-------+-------+------------+--------------------+------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train_united.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+--------------------+------+--------------------+\n",
      "|user_id|item_id|content_type|               title|  year|              genres|\n",
      "+-------+-------+------------+--------------------+------+--------------------+\n",
      "|   1654|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 510087|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 517612|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 522798|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 523860|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 529632|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 566758|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 575248|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 588378|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 625638|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 627053|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 642397|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 651811|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 659698|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 706816|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 728909|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 740405|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 740836|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 742048|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "| 742986|   8389|           1|пес в сапогах (су...|1981.0|Мультфильмы,Детск...|\n",
      "+-------+-------+------------+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test_united.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_train_united.groupBy('genres').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1075, 2)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ht = HashingTF(inputCol='title_split', outputCol=\"title_features\", numFeatures=100)\n",
    "genres_ht = HashingTF(inputCol='genres_split', outputCol=\"genres_features\", numFeatures=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_train_united.select('user_id', \n",
    "                             'item_id', \n",
    "                             to_int(data_train_united.year).alias('year'), \n",
    "                             split(data_train_united.title).alias('title_split'), \n",
    "                             split(data_train_united.genres).alias('genres_split'), \n",
    "                             'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_united_transformed = genres_ht.transform(title_ht.transform(d))\\\n",
    "                                        .select('user_id', 'item_id', 'year', 'title_features', 'genres_features', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_test_united.select('user_id', \n",
    "                            'item_id', \n",
    "                            to_int(data_test_united.year).alias('year'), \n",
    "                            split(data_test_united.title).alias('title_split'), \n",
    "                            split(data_test_united.genres).alias('genres_split'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_united_transformed = genres_ht.transform(title_ht.transform(d))\\\n",
    "                                        .select('user_id', 'item_id', 'year', 'title_features', 'genres_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+--------------------+---------------+--------+\n",
      "|user_id|item_id|year|      title_features|genres_features|purchase|\n",
      "+-------+-------+----+--------------------+---------------+--------+\n",
      "| 520446|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 556825|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 566701|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 613775|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 619378|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 625678|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 632495|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 636572|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 639612|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 668112|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 703514|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 711308|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 717302|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 719149|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 725821|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 728960|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 729785|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 731490|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 732411|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "| 735387|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|       0|\n",
      "+-------+-------+----+--------------------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train_united_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+--------------------+---------------+\n",
      "|user_id|item_id|year|      title_features|genres_features|\n",
      "+-------+-------+----+--------------------+---------------+\n",
      "| 869272|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 869294|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 869465|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 869669|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 869892|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 869909|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 870514|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 870906|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 870998|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871082|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871154|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871348|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871411|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871442|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871456|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871503|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 871876|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 872063|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 872536|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "| 872797|   8389|1981|(100,[20,58,84,87...|(100,[5],[1.0])|\n",
      "+-------+-------+----+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test_united_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Дополнительные параметры для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_item_purchase = data_train.groupBy('item_id')\\\n",
    "                        .sum('purchase')\\\n",
    "                        .withColumnRenamed('sum(purchase)', 'sum_item_purchased').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_user_purchase = data_train.groupBy('user_id')\\\n",
    "                        .sum('purchase')\\\n",
    "                        .withColumnRenamed('sum(purchase)', 'sum_user_purchased').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_item_watch_time = data_views_programmes\\\n",
    "                        .select('item_id', (f.col('ts_end') - f.col('ts_start')).alias('item_watch_time'))\\\n",
    "                        .groupBy('item_id')\\\n",
    "                        .sum('item_watch_time')\\\n",
    "                        .withColumnRenamed('sum(item_watch_time)', 'sum_item_watch_time')\\\n",
    "                        .cache()\n",
    "\n",
    "sum_item_watch_time = sum_item_watch_time.select('item_id', to_int(sum_item_watch_time.sum_item_watch_time)\\\n",
    "                                                 .alias('sum_item_watch_time')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_user_watch_time = data_views_programmes\\\n",
    "                        .select('user_id', (f.col('ts_end') - f.col('ts_start')).alias('user_watch_time'))\\\n",
    "                        .groupBy('user_id')\\\n",
    "                        .sum('user_watch_time')\\\n",
    "                        .withColumnRenamed('sum(user_watch_time)', 'sum_user_watch_time')\\\n",
    "                        .cache()\n",
    "\n",
    "sum_user_watch_time = sum_user_watch_time.select('user_id', to_int(sum_user_watch_time.sum_user_watch_time)\\\n",
    "                                                 .alias('sum_user_watch_time')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train_united_transformed\\\n",
    "            .join(sum_item_purchase, on='item_id', how='left_outer')\\\n",
    "            .join(sum_user_purchase, on='user_id', how='left_outer')\\\n",
    "            .join(sum_item_watch_time, on='item_id', how='left_outer')\\\n",
    "            .join(sum_user_watch_time, on='user_id', how='left_outer')\\\n",
    "            .select('user_id', \n",
    "                    'item_id', \n",
    "                    'year', \n",
    "                    'genres_features', \n",
    "                    'sum_item_purchased', \n",
    "                    'sum_user_purchased', \n",
    "                    'sum_item_watch_time', \n",
    "                    'sum_user_watch_time', \n",
    "                    'purchase')\\\n",
    "            .fillna({'sum_item_purchased': 0, \n",
    "                     'sum_user_purchased': 0, \n",
    "                     'sum_item_watch_time': 0, \n",
    "                     'sum_user_watch_time': 0}).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test_united_transformed\\\n",
    "            .join(sum_item_purchase, on='item_id', how='left_outer')\\\n",
    "            .join(sum_user_purchase, on='user_id', how='left_outer')\\\n",
    "            .join(sum_item_watch_time, on='item_id', how='left_outer')\\\n",
    "            .join(sum_user_watch_time, on='user_id', how='left_outer')\\\n",
    "            .select('user_id', \n",
    "                    'item_id', \n",
    "                    'year', \n",
    "                    'genres_features', \n",
    "                    'sum_item_purchased', \n",
    "                    'sum_user_purchased', \n",
    "                    'sum_item_watch_time', \n",
    "                    'sum_user_watch_time')\\\n",
    "            .fillna({'sum_item_purchased': 0, \n",
    "                     'sum_user_purchased': 0, \n",
    "                     'sum_item_watch_time': 0, \n",
    "                     'sum_user_watch_time': 0}).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['user_id', \n",
    "                'item_id', \n",
    "                'year', \n",
    "                'genres_features', \n",
    "                'sum_item_purchased', \n",
    "                'sum_user_purchased', \n",
    "                'sum_item_watch_time', \n",
    "                'sum_user_watch_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_assembler = VectorAssembler(inputCols=feature_list, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol='features', labelCol='purchase', seed=42, maxIter=40, maxDepth=4, minInstancesPerNode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(stages=[feature_assembler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем предсказания и записываем в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+---------------+------------------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|user_id|item_id|year|genres_features|sum_item_purchased|sum_user_purchased|sum_item_watch_time|sum_user_watch_time|            features|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+----+---------------+------------------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   1654|   8389|1981|(100,[5],[1.0])|                 8|                 5|                  0|             365191|(107,[0,1,2,8,103...|[1.78690582979145...|[0.97271652947498...|       0.0|\n",
      "| 510087|   8389|1981|(100,[5],[1.0])|                 8|                 6|                  0|            2337212|(107,[0,1,2,8,103...|[1.77074800513445...|[0.97184567426074...|       0.0|\n",
      "| 517612|   8389|1981|(100,[5],[1.0])|                 8|                 1|                  0|             287400|(107,[0,1,2,8,103...|[1.79554745981580...|[0.97317148301086...|       0.0|\n",
      "| 522798|   8389|1981|(100,[5],[1.0])|                 8|                 3|                  0|            6229271|(107,[0,1,2,8,103...|[1.79533049771612...|[0.97316015146701...|       0.0|\n",
      "| 523860|   8389|1981|(100,[5],[1.0])|                 8|                34|                  0|           14186858|(107,[0,1,2,8,103...|[0.87588401759261...|[0.85217566413895...|       0.0|\n",
      "+-------+-------+----+---------------+------------------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "answer_df = predictions.select(['user_id','item_id','probability']).orderBy(['user_id','item_id']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df['purchase'] = answer_df['probability'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df = answer_df.drop('probability', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_df.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
