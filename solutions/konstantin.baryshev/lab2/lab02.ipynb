{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"baryshev konstantin\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"baryshev konstantin\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Разбиение описания на слова"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Шаги:\n",
    "    1) Разобьем описание на массив слов (с помощью рег.выражений)\n",
    "    2) Посчитает HashingTF \n",
    "    3) Для IDF есть собственный класс (он так и называется IDF). \n",
    "Применение метода fit над датасетом возвращает объект IDFModel, в который нужно передать результат HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (text - array of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def get_words(description):\n",
    "    return description.apply(lambda x: pattern.findall(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создадим колонку с массивом слов\n",
    "data = data.withColumn(\"words\", get_words(\"desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (Hashing TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|               words|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|[this, course, in...|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|[this, online, co...|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=10000, binary=False, inputCol=\"words\", outputCol=\"word_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizedData = hasher.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|               words|         word_vector|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|[this, course, in...|(10000,[36,63,138...|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|[this, online, co...|(10000,[32,222,36...|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show(n=2)\n",
    "#featurizedData.select(\"desc\", \"word_vector\").show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"word_vector\", outputCol=\"features_vec\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Найдем схожие курсы по косинусной мере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Нормировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализуем вектор\n",
    "normalizer = Normalizer(inputCol=\"features_vec\", outputCol=\"normFeatures\", p=2.0)\n",
    "NormData = normalizer.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|               words|         word_vector|        features_vec|        normFeatures|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|[this, course, in...|(10000,[36,63,138...|(10000,[36,63,138...|(10000,[36,63,138...|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NormData.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Соберем в dataframe id и desc по фильмам, для которых нужно сделать рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_coursers = NormData[['id','lang','normFeatures']]\\\n",
    "              .filter(f.col(\"id\").isin(23126,21617,16627,11556,16704,13702))\\\n",
    "              .withColumnRenamed(\"id\", \"id_\")\\\n",
    "              .withColumnRenamed(\"normFeatures\", \"normFeatures_\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|  id_|lang|       normFeatures_|\n",
      "+-----+----+--------------------+\n",
      "|11556|  es|(10000,[249,522,5...|\n",
      "|13702|  ru|(10000,[310,942,2...|\n",
      "|16627|  es|(10000,[55,76,192...|\n",
      "|16704|  ru|(10000,[381,1144,...|\n",
      "|21617|  en|(10000,[17,128,16...|\n",
      "|23126|  en|(10000,[87,91,128...|\n",
      "+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_coursers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross = my_coursers[['id_','lang','normFeatures_']]\\\n",
    "            .join(NormData[['id','lang','name','normFeatures']],\n",
    "                  on=['lang'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+---+--------------------+--------------------+\n",
      "|lang|  id_|       normFeatures_| id|                name|        normFeatures|\n",
      "+----+-----+--------------------+---+--------------------+--------------------+\n",
      "|  en|21617|(10000,[17,128,16...|  4|Accounting Cycle:...|(10000,[36,63,138...|\n",
      "|  en|21617|(10000,[17,128,16...|  5|American Counter ...|(10000,[32,222,36...|\n",
      "+----+-----+--------------------+---+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cross.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удалим c одинаковым id\n",
    "df_cross = df_cross.where(f.col(\"id_\") != f.col(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#посчитаем для каждого нормированного признака косинусоное расстояние\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "dotProductUdf = udf(lambda v1, v2: float(v1.dot(v2)), DoubleType())\n",
    "df_cross = df_cross.withColumn('cos_dist', dotProductUdf('normFeatures_', 'normFeatures'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|lang|  id_|       normFeatures_| id|                name|        normFeatures|            cos_dist|\n",
      "+----+-----+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|  en|23126|(10000,[87,91,128...|  4|Accounting Cycle:...|(10000,[36,63,138...|0.021360168184388236|\n",
      "|  en|21617|(10000,[17,128,16...|  4|Accounting Cycle:...|(10000,[36,63,138...| 0.07692421791027074|\n",
      "+----+-----+--------------------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cross.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Выберем топ-10 и отсортируем в нужной последов-ти"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "по метрике (убывание) => по названию (лексикографически по возрастанию) => по возрастанию id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "n = 10\n",
    "w = Window().partitionBy(\"id_\").orderBy(col(\"cos_dist\").desc(), col(\"name\").asc(), col(\"id\").asc())\n",
    "result = (\n",
    "    df_cross.withColumn(\"rn\", row_number().over(w))\n",
    "    .where(col(\"rn\") <= n)\n",
    "    .select(\"id_\", \"id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  id_|   id|\n",
      "+-----+-----+\n",
      "|23126|14760|\n",
      "|23126|13665|\n",
      "|23126|13782|\n",
      "|23126|20638|\n",
      "|23126|24419|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Загрузим в json и отправим на проверку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list\n",
    "\n",
    "collected = result.groupBy(\"id_\").agg(\n",
    "    collect_list(col(\"id\")).alias(\"list_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|  id_|             list_id|\n",
      "+-----+--------------------+\n",
      "|23126|[14760, 13665, 13...|\n",
      "|16627|[11431, 11575, 12...|\n",
      "|13702|[864, 21079, 8313...|\n",
      "|16704|[1236, 1247, 1365...|\n",
      "|11556|[16488, 468, 1346...|\n",
      "|21617|[21609, 21616, 21...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_: bigint, list_id: array<bigint>]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 96.8 ms, total: 228 ms\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_rec = {'23126': collected.where(col(\"id_\") == '23126').select(\"list_id\").collect()[0][0],\n",
    "             '16627': collected.where(col(\"id_\") == '16627').select(\"list_id\").collect()[0][0],\n",
    "             '13702': collected.where(col(\"id_\") == '13702').select(\"list_id\").collect()[0][0],\n",
    "             '16704': collected.where(col(\"id_\") == '16704').select(\"list_id\").collect()[0][0],\n",
    "             '11556': collected.where(col(\"id_\") == '11556').select(\"list_id\").collect()[0][0],\n",
    "             '21617': collected.where(col(\"id_\") == '21617').select(\"list_id\").collect()[0][0]            \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/home/konstantin.baryshev/lab02.json', 'w') as f:\n",
    "    json.dump(dict_rec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test\n",
    "with open('/data/home/konstantin.baryshev/lab02.json', 'r') as f:\n",
    "    d = json.load(f)\n",
    "    print(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
