{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"baryshev konstantin\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"baryshev konstantin\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>baryshev konstantin</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe5845d0828>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", IntegerType(), True),\n",
    "        StructField(\"item_id\", IntegerType(), True),\n",
    "        StructField(\"purchase\", IntegerType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"sep\", \",\")\n",
    "    .schema(schema)\n",
    "    .load(\"/labs/slaba03/laba03_train.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"sep\", \",\")\n",
    "    .schema(schema)\n",
    "    .load(\"/labs/slaba03/laba03_test.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"item_id\", IntegerType(), True),\n",
    "        StructField(\"channel_id\", IntegerType(), True),\n",
    "        StructField(\"datetime_availability_start\", TimestampType(), True),\n",
    "        StructField(\"datetime_availability_stop\", TimestampType(), True),\n",
    "        StructField(\"datetime_show_start\", TimestampType(), True),\n",
    "        StructField(\"datetime_show_stop\", TimestampType(), True),\n",
    "        StructField(\"content_type\", IntegerType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"year\", DoubleType(), True),\n",
    "        StructField(\"genres\", StringType(), True),\n",
    "        StructField(\"region_id\", IntegerType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .schema(schema)\n",
    "    .load(\"/labs/slaba03/laba03_items.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------\n",
      " item_id                     | 65667                \n",
      " channel_id                  | null                 \n",
      " datetime_availability_start | 1970-01-01 03:00:00  \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00  \n",
      " datetime_show_start         | null                 \n",
      " datetime_show_stop          | null                 \n",
      " content_type                | 1                    \n",
      " title                       | на пробах только ... \n",
      " year                        | 2013.0               \n",
      " genres                      | Эротика              \n",
      " region_id                   | null                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.show(1, truncate=True, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", IntegerType(), True),\n",
    "        StructField(\"item_id\", IntegerType(), True),\n",
    "        StructField(\"ts_start\", IntegerType(), True),\n",
    "        StructField(\"ts_end\", IntegerType(), True),\n",
    "        StructField(\"item_type\", StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views_programmes = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"sep\", \",\")\n",
    "    .schema(schema)\n",
    "    .load(\"/labs/slaba03/laba03_views_programmes.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------+\n",
      "|user_id|avg_time_view|sum_time_view|\n",
      "+-------+-------------+-------------+\n",
      "| 561425|       7539.8|        37699|\n",
      "| 612390|       4406.0|         4406|\n",
      "+-------+-------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views_programmes.show(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train - факт покупок\n",
    "df_items - доп.инфа по фильмам\n",
    "df_views_programmes - длительность просмотра\n",
    "\n",
    "df_test - датасет для пол-ей, по которым необходимо сделать предсказания (не забудь отсортировать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ = ['user_id',\n",
    " 'item_id',\n",
    " 'purchase',\n",
    " 'avg_cnt_buy_item',\n",
    " 'cnt_buys_item',\n",
    " 'flag_buys_item',\n",
    " 'year',\n",
    " 'cnt_buys_user',\n",
    " 'flag_buys_user',\n",
    " 'avg_cnt_buy_user',\n",
    " 'avg_time_view',\n",
    " 'sum_time_view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#подсчет nan значений в dataframes (есть nan в years)\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "\n",
    "result = df_train_full.select([count(when(isnan(c), c)).alias(c) for c in features_])\n",
    "result = df_train_full.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in features_]\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Фичи инжиниринг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "flag_convert = udf(\n",
    "    lambda x: 1 if x > 0 else 0,\n",
    "    IntegerType(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " user_id          | 1654                   \n",
      " item_id          | 74107                  \n",
      " purchase         | 0                      \n",
      " avg_cnt_buy_item | 7.575757575757576E-4   \n",
      " cnt_buys_item    | 1                      \n",
      " flag_buys_item   | 1                      \n",
      " year             | 2011.0                 \n",
      " genres_array     | [Драмы, Зарубежные]    \n",
      " cnt_buys_user    | 5                      \n",
      " flag_buys_user   | 1                      \n",
      " avg_cnt_buy_user | 0.0019470404984423676  \n",
      " avg_time_view    | 1844.3989898989898     \n",
      " sum_time_view    | 365191                 \n",
      " genres_vector    | (80,[32,45],[1.0,1.0]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_full.where(f.col(\"item_id\")==74107).where(f.col(\"user_id\")==1654).show(1,False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 ITEMS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число уник-ых пользователей в train: 1941\n"
     ]
    }
   ],
   "source": [
    "print(f\"Число уник-ых пользователей в train: {df_train.select('user_id').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#как часто покупали item из всех item-ов, которые купили хотя бы один раз\n",
    "avg_buys_items=df_train.groupby(\"item_id\").agg({\"purchase\":'avg'}) \\\n",
    "                                                  .withColumnRenamed('avg(purchase)', \"avg_cnt_buy_item\")\n",
    "\n",
    "cnt_buys_items=df_train.groupby(\"item_id\").sum(\"purchase\").withColumnRenamed('sum(purchase)', \"cnt_buys_item\")\n",
    "cnt_buys_items=cnt_buys_items.withColumn(\"flag_buys_item\", flag_convert(cnt_buys_items.cnt_buys_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.na.fill({'year': -999, 'genres': 'unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#преобразуем файл с просмотрами: массив жанров разделим запятыми и поместим в массив\n",
    "df_items = df_items.withColumn(\"genres_array\", f.split(f.col(\"genres\"), ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_features = avg_buys_items.join(cnt_buys_items, on=['item_id'])\\\n",
    "                               .join(df_items.select('item_id', 'year', 'genres_array'), on=['item_id'])# , 'genres_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "|item_id|    avg_cnt_buy_item|cnt_buys_item|flag_buys_item|  year|        genres_array|\n",
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "|   8389|0.005979073243647235|            8|             1|1981.0|[Мультфильмы, Дет...|\n",
      "|   8638|0.001450326323422...|            2|             1|2012.0|[Ужасы, Комедии, ...|\n",
      "|  10817|7.380073800738007E-4|            1|             1|2013.0|[Документальные, ...|\n",
      "|  72820|7.390983000739098E-4|            1|             1|2016.0|[Драмы, Мелодрамы...|\n",
      "|  74757|7.358351729212656E-4|            1|             1|2012.0|[Мистические, Ужа...|\n",
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_buys_users=df_train.groupby(\"user_id\").agg({\"purchase\":'avg'}).withColumnRenamed('avg(purchase)', \"avg_cnt_buy_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_buys_users=df_train.groupby(\"user_id\").sum(\"purchase\").withColumnRenamed('sum(purchase)', 'cnt_buys_user')\n",
    "cnt_buys_users=cnt_buys_users.withColumn(\"flag_buys_user\", flag_convert(cnt_buys_users.cnt_buys_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views_programmes = df_views_programmes.withColumn('ts_delta', f.col(\"ts_end\")-f.col(\"ts_start\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_views_programmes = df_views_programmes.groupby('user_id')\\\n",
    "                                           .agg(f.mean('ts_delta').alias('avg_time_view'),\\\n",
    "                                                f.sum('ts_delta').alias('sum_time_view')\n",
    "                                                ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_features=cnt_buys_users.join(avg_buys_users, [\"user_id\"])\\\n",
    "                             .join(df_views_programmes, [\"user_id\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "|user_id|cnt_buys_user|flag_buys_user|    avg_cnt_buy_user|     avg_time_view|sum_time_view|\n",
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "| 754230|           72|             1|0.027575641516660282|1938.5352233676977|      2256455|\n",
      "| 761341|            1|             1|3.875968992248062E-4| 2555.931818181818|       112461|\n",
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, cnt_buys_user: bigint, flag_buys_user: int, avg_cnt_buy_user: double, avg_time_view: double, sum_time_view: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_features.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITEM FEATURES + USER FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "|item_id|    avg_cnt_buy_item|cnt_buys_item|flag_buys_item|  year|        genres_array|\n",
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "|   8389|0.005979073243647235|            8|             1|1981.0|[Мультфильмы, Дет...|\n",
      "|   8638|0.001450326323422...|            2|             1|2012.0|[Ужасы, Комедии, ...|\n",
      "+-------+--------------------+-------------+--------------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "|user_id|cnt_buys_user|flag_buys_user|    avg_cnt_buy_user|     avg_time_view|sum_time_view|\n",
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "| 754230|           72|             1|0.027575641516660282|1938.5352233676977|      2256455|\n",
      "| 761341|            1|             1|3.875968992248062E-4| 2555.931818181818|       112461|\n",
      "+-------+-------------+--------------+--------------------+------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = df_train.join(items_features, [\"item_id\"])\\\n",
    "                        .join(users_features, [\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " user_id          | 754230                       \n",
      " item_id          | 9782                         \n",
      " purchase         | 0                            \n",
      " avg_cnt_buy_item | 0.002191380569758948         \n",
      " cnt_buys_item    | 3                            \n",
      " flag_buys_item   | 1                            \n",
      " year             | 2015.0                       \n",
      " genres_array     | [Ужасы, Детективы, Триллеры] \n",
      " cnt_buys_user    | 72                           \n",
      " flag_buys_user   | 1                            \n",
      " avg_cnt_buy_user | 0.027575641516660282         \n",
      " avg_time_view    | 1938.5352233676977           \n",
      " sum_time_view    | 2256455                      \n",
      "-RECORD 1----------------------------------------\n",
      " user_id          | 754230                       \n",
      " item_id          | 10208                        \n",
      " purchase         | 0                            \n",
      " avg_cnt_buy_item | 0.003668378576669112         \n",
      " cnt_buys_item    | 5                            \n",
      " flag_buys_item   | 1                            \n",
      " year             | 2011.0                       \n",
      " genres_array     | [unknown]                    \n",
      " cnt_buys_user    | 72                           \n",
      " flag_buys_user   | 1                            \n",
      " avg_cnt_buy_user | 0.027575641516660282         \n",
      " avg_time_view    | 1938.5352233676977           \n",
      " sum_time_view    | 2256455                      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_full.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'item_id',\n",
       " 'purchase',\n",
       " 'avg_cnt_buy_item',\n",
       " 'cnt_buys_item',\n",
       " 'flag_buys_item',\n",
       " 'year',\n",
       " 'genres_array',\n",
       " 'cnt_buys_user',\n",
       " 'flag_buys_user',\n",
       " 'avg_cnt_buy_user',\n",
       " 'avg_time_view',\n",
       " 'sum_time_view',\n",
       " 'genres_vector']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вектор жанров, закодированный HashingTF\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hasher = HashingTF(numFeatures=80, binary=False, inputCol=\"genres_array\", outputCol=\"genres_vector\")\n",
    "df_train_full = hasher.transform(df_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: int, avg_cnt_buy_item: double, cnt_buys_item: bigint, flag_buys_item: int, year: double, genres_array: array<string>, cnt_buys_user: bigint, flag_buys_user: int, avg_cnt_buy_user: double, avg_time_view: double, sum_time_view: bigint, genres_vector: vector]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = df_train_full.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------\n",
      " user_id          | 754230                                               \n",
      " item_id          | 8389                                                 \n",
      " purchase         | 0                                                    \n",
      " avg_cnt_buy_item | 0.005979073243647235                                 \n",
      " cnt_buys_item    | 8                                                    \n",
      " flag_buys_item   | 1                                                    \n",
      " year             | 1981.0                                               \n",
      " genres_array     | [Мультфильмы, Детские, Союзмультфильм, Наши]         \n",
      " cnt_buys_user    | 72                                                   \n",
      " flag_buys_user   | 1                                                    \n",
      " avg_cnt_buy_user | 0.027575641516660282                                 \n",
      " avg_time_view    | 1938.5352233676977                                   \n",
      " sum_time_view    | 2256455                                              \n",
      " genres_vector    | (80,[17,27,34,74],[1.0,1.0,1.0,1.0])                 \n",
      "-RECORD 1----------------------------------------------------------------\n",
      " user_id          | 754230                                               \n",
      " item_id          | 8638                                                 \n",
      " purchase         | 1                                                    \n",
      " avg_cnt_buy_item | 0.0014503263234227702                                \n",
      " cnt_buys_item    | 2                                                    \n",
      " flag_buys_item   | 1                                                    \n",
      " year             | 2012.0                                               \n",
      " genres_array     | [Ужасы, Комедии, Триллеры, Для взрослых, Зарубежные] \n",
      " cnt_buys_user    | 72                                                   \n",
      " flag_buys_user   | 1                                                    \n",
      " avg_cnt_buy_user | 0.027575641516660282                                 \n",
      " avg_time_view    | 1938.5352233676977                                   \n",
      " sum_time_view    | 2256455                                              \n",
      " genres_vector    | (80,[0,17,32,35,37],[1.0,1.0,1.0,1.0,1.0])           \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_full.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_full.write.parquet(\"df_train_full_v2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator #for ROC AUC\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = ['purchase', 'genres_array']\n",
    "assembler = VectorAssembler(\n",
    "                            inputCols= [x for x in df_train_full.columns if x not in ignore],\n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=80, binary=False, inputCol=\"genres_array\", outputCol=\"genres_vector\")\n",
    "df_train_full = hasher.transform(df_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = assembler.transform(df_train_full).select(\"purchase\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаки, участвующие в обучении: ['avg_cnt_buy_item', 'cnt_buys_item', 'cnt_buys_user', 'avg_cnt_buy_user']\n"
     ]
    }
   ],
   "source": [
    "print(f'Признаки, участвующие в обучении: {[x for x in df_train_full.columns if x not in ignore]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"purchase\", maxDepth=4, minInstancesPerNode=3, maxBins=50, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 1.53 s, total: 4.76 s\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скоринг df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = df_test.join(items_features, [\"item_id\"])\\\n",
    "                      .join(users_features, [\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=80, binary=False, inputCol=\"genres_array\", outputCol=\"genres_vector\")\n",
    "df_test_full = hasher.transform(df_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_full = df_test_full.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembler.transform(df_test_full).select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstelement=udf(lambda v:float(v[1]),FloatType())\n",
    "targets = predictions.select(firstelement('probability')).withColumnRenamed('<lambda>(probability)', \"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_test_full.select(\"user_id\",\"item_id\").withColumn(\"id\", monotonically_increasing_id())\n",
    "df2 = targets.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df2.join(df1, \"id\", \"outer\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[purchase: float, user_id: int, item_id: int]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение средней вероятности покупки по пользователям:\n",
      "+-------+--------------------+\n",
      "|user_id|       avg(purchase)|\n",
      "+-------+--------------------+\n",
      "| 754230| 0.07240736743218835|\n",
      "| 761341| 0.04390896864739177|\n",
      "| 776188| 0.04409278008230809|\n",
      "| 780033| 0.04399668344595928|\n",
      "| 798454|0.043918690197169784|\n",
      "| 825061|0.044525370108707066|\n",
      "| 833685| 0.04794281916446303|\n",
      "| 846231| 0.04445510645527646|\n",
      "| 851486| 0.04388509640103132|\n",
      "| 867850| 0.04390323007341273|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "print(\"Распределение средней вероятности покупки по пользователям:\")\n",
    "submit.groupby('user_id').agg(avg(col(\"purchase\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Максимальная вероятность: {submit.agg({'purchase': 'max'}).collect()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|purchase|            features|\n",
      "+--------+--------------------+\n",
      "|       0|[0.00597907324364...|\n",
      "|       1|[0.00145032632342...|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#остортируем submit по возрастанию идентификаторов пользователей (user_id), \n",
    "#а затем — по возрастанию идентификаторов передач (item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!подходит только для файлов небольшого размера\n",
    "submit.select(\"user_id\", \"item_id\",\"purchase\")\\\n",
    "      .orderBy(col(\"user_id\"),col(\"item_id\"))\\\n",
    "      .toPandas().to_csv(\"/data/home/konstantin.baryshev/lab03.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 0.1647, 1: 0.3672, 2: 0.3661, 3: 0.102})"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
