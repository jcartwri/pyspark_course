{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаба 1. Расчет рейтингов фильмов – RDD\n",
    "\n",
    "По имеющимся данным о рейтингах фильмов (MovieLens: 100 000 рейтингов) посчитать агрегированную статистику по ним.\n",
    "\n",
    "## Описание данных\n",
    "\n",
    "Имеются следующие входные данные:\n",
    "\n",
    "* Таблица `users x movies` с рейтингами. Архив с датасетом нужно скачать с сайта [GroupLens](http://files.grouplens.org/datasets/movielens/ml-100k.zip). Также, он загружен на HDFS в `/labs/laba01/ml-100k`. Файл u.data содержит все оценки, а файл u.item — список всех фильмов.\n",
    "\n",
    "`!hdfs dfs -ls /labs/laba01/ml-100k`\n",
    "\n",
    "* `id фильма` для расчета индивидуальных характеристик — в Личном кабинете на странице [Лабы 1](https://lk-spark.newprolab.com/lab/slaba01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"lab01\") \n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1613394706435_0552'),\n",
       " ('spark.driver.appUIAddress', 'http://spark-de-master-3.newprolab.com:4042'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.history.fs.cleaner.interval', '7d'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://spark-de-master-1.newprolab.com:8088/proxy/application_1613394706435_0552'),\n",
       " ('spark.shuffle.io.serverThreads', '128'),\n",
       " ('spark.sql.streaming.streamingQueryListeners', ''),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.shuffle.file.buffer', '1m'),\n",
       " ('spark.driver.port', '44401'),\n",
       " ('spark.sql.hive.convertMetastoreOrc', 'true'),\n",
       " ('spark.yarn.dist.files', ''),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '26214400'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar,/data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,/data/home/evgeniy.osipchuk/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,/data/home/evgeniy.osipchuk/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,/data/home/evgeniy.osipchuk/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar,/data/home/evgeniy.osipchuk/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.catalogImplementation', 'in-memory'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.orc.impl', 'native'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.history.fs.cleaner.maxAge', '90d'),\n",
       " ('spark.extraListeners', ''),\n",
       " ('spark.yarn.secondary.jars',\n",
       "  'org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar,org.apache.kafka_kafka-clients-2.0.0.jar,org.spark-project.spark_unused-1.0.0.jar,org.lz4_lz4-java-1.4.0.jar,org.xerial.snappy_snappy-java-1.1.7.3.jar,org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.warehouse.dir', '/apps/spark/warehouse'),\n",
       " ('spark.history.store.path', '/var/lib/spark2/shs_db'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1613394706435_0552'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'spark-de-master-1.newprolab.com'),\n",
       " ('spark.app.name', 'lab01'),\n",
       " ('spark.sql.statistics.fallBackToHdfs', 'true'),\n",
       " ('spark.yarn.archive', 'hdfs:///tmp/spark-archive.zip'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.extraClassPath', ''),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar,file:///data/home/evgeniy.osipchuk/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.sql.hive.metastore.jars',\n",
       "  '/usr/hdp/current/spark2-client/standalone-metastore/*'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip<CPS>{{PWD}}/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.5.jar<CPS>{{PWD}}/org.apache.kafka_kafka-clients-2.0.0.jar<CPS>{{PWD}}/org.spark-project.spark_unused-1.0.0.jar<CPS>{{PWD}}/org.lz4_lz4-java-1.4.0.jar<CPS>{{PWD}}/org.xerial.snappy_snappy-java-1.1.7.3.jar<CPS>{{PWD}}/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.yarn.historyServer.address', 'spark-de-master-1.newprolab.com:18081'),\n",
       " ('spark.driver.maxResultSize', '3900m'),\n",
       " ('spark.deploy-mode', 'client'),\n",
       " ('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.sql.queryExecutionListeners', ''),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.port.maxRetries', '80'),\n",
       " ('spark.io.compression.lz4.blockSize', '128kb'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.sql.orc.filterPushdown', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.shuffle.io.backLog', '8192'),\n",
       " ('spark.driver.host', 'spark-de-master-3.newprolab.com'),\n",
       " ('spark.unsafe.sorter.spill.reader.buffer.size', '1m'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.shuffle.unsafe.file.output.buffer', '5m'),\n",
       " ('spark.executor.cores', '1'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseNUMA'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.hive.metastore.version', '3.0')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на файлы в директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 items\n",
      "-rw-r--r--   3 hdfs hdfs       6750 2020-09-05 20:38 /labs/laba01/ml-100k/README\n",
      "-rw-r--r--   3 hdfs hdfs        716 2020-09-05 20:38 /labs/laba01/ml-100k/allbut.pl\n",
      "-rw-r--r--   3 hdfs hdfs        643 2020-09-05 20:38 /labs/laba01/ml-100k/mku.sh\n",
      "-rw-r--r--   3 hdfs hdfs    1979173 2020-09-05 20:38 /labs/laba01/ml-100k/u.data\n",
      "-rw-r--r--   3 hdfs hdfs        202 2020-09-05 20:38 /labs/laba01/ml-100k/u.genre\n",
      "-rw-r--r--   3 hdfs hdfs         36 2020-09-05 20:38 /labs/laba01/ml-100k/u.info\n",
      "-rw-r--r--   3 hdfs hdfs     236344 2020-09-05 20:38 /labs/laba01/ml-100k/u.item\n",
      "-rw-r--r--   3 hdfs hdfs        193 2020-09-05 20:38 /labs/laba01/ml-100k/u.occupation\n",
      "-rw-r--r--   3 hdfs hdfs      22628 2020-09-05 20:38 /labs/laba01/ml-100k/u.user\n",
      "-rw-r--r--   3 hdfs hdfs    1586544 2020-09-05 20:38 /labs/laba01/ml-100k/u1.base\n",
      "-rw-r--r--   3 hdfs hdfs     392629 2020-09-05 20:38 /labs/laba01/ml-100k/u1.test\n",
      "-rw-r--r--   3 hdfs hdfs    1583948 2020-09-05 20:38 /labs/laba01/ml-100k/u2.base\n",
      "-rw-r--r--   3 hdfs hdfs     395225 2020-09-05 20:38 /labs/laba01/ml-100k/u2.test\n",
      "-rw-r--r--   3 hdfs hdfs    1582546 2020-09-05 20:38 /labs/laba01/ml-100k/u3.base\n",
      "-rw-r--r--   3 hdfs hdfs     396627 2020-09-05 20:38 /labs/laba01/ml-100k/u3.test\n",
      "-rw-r--r--   3 hdfs hdfs    1581878 2020-09-05 20:38 /labs/laba01/ml-100k/u4.base\n",
      "-rw-r--r--   3 hdfs hdfs     397295 2020-09-05 20:38 /labs/laba01/ml-100k/u4.test\n",
      "-rw-r--r--   3 hdfs hdfs    1581776 2020-09-05 20:38 /labs/laba01/ml-100k/u5.base\n",
      "-rw-r--r--   3 hdfs hdfs     397397 2020-09-05 20:38 /labs/laba01/ml-100k/u5.test\n",
      "-rw-r--r--   3 hdfs hdfs    1792501 2020-09-05 20:38 /labs/laba01/ml-100k/ua.base\n",
      "-rw-r--r--   3 hdfs hdfs     186672 2020-09-05 20:38 /labs/laba01/ml-100k/ua.test\n",
      "-rw-r--r--   3 hdfs hdfs    1792476 2020-09-05 20:38 /labs/laba01/ml-100k/ub.base\n",
      "-rw-r--r--   3 hdfs hdfs     186697 2020-09-05 20:38 /labs/laba01/ml-100k/ub.test\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/laba01/ml-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится только таблица с рейтингами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_path = '/labs/laba01/ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sc.textFile(rating_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим первые 10 записей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949',\n",
       " '186\\t302\\t3\\t891717742',\n",
       " '22\\t377\\t1\\t878887116',\n",
       " '244\\t51\\t2\\t880606923',\n",
       " '166\\t346\\t1\\t886397596',\n",
       " '298\\t474\\t4\\t884182806',\n",
       " '115\\t265\\t2\\t881171488',\n",
       " '253\\t465\\t5\\t891628467',\n",
       " '305\\t451\\t3\\t886324817',\n",
       " '6\\t86\\t3\\t883603013']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sc.textFile(\"ratings_3_repart.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['508', '132', '5', '883767279'],\n",
       " ['487', '1440', '4', '884045494'],\n",
       " ['387', '27', '1', '886483252'],\n",
       " ['346', '3', '3', '875265392'],\n",
       " ['308', '134', '5', '887737686'],\n",
       " ['714', '477', '2', '892777408'],\n",
       " ['234', '770', '4', '892335920'],\n",
       " ['578', '1264', '3', '890939815'],\n",
       " ['624', '591', '3', '879792557'],\n",
       " ['655', '124', '3', '887426087']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним id фильма в переменную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = '328'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимую агрегацию по всем фильмам получим с помощью функций map и countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_all = ratings.map(lambda x: (x[2], x[2])).countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу отсортируем список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_all = [hist_all[f'{i}'] for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6110, 11370, 27145, 34174, 21201]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные для нашего фильма получим точно так же, но отсортируем dataframe по id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_film = ratings.filter(lambda x: x[1] == movie_id).map(lambda x: (x[2], x[3])).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_film = [hist_film[f'{i}'] for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 40, 94, 109, 40]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = {'hist_film' : hist_film, 'hist_all' : hist_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"lab01.json\", \"w\") as fp:\n",
    "    json.dump(answer, fp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
