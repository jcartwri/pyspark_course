{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Evgeniy.osipchuk\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Evgeniy.osipchuk\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import HashingTF, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-de-master-1.newprolab.com:6667'\n",
    "KAFKA_BOOTSTRAP_SERVER = 'spark-de-node-1.newprolab.com:6667'\n",
    "# KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667' \n",
    "INPUT_KAFKA_TOPIC = 'input_evgeniy.osipchuk'\n",
    "OUTPUT_KAFKA_TOPIC = 'evgeniy.osipchuk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))\n",
    "        \n",
    "def get_visits_list(user_json):\n",
    "    user_json = eval(user_json)[\"visits\"]\n",
    "    try: \n",
    "        list_visits = []\n",
    "        for x in user_json: \n",
    "            for word in re.findall(r'\\w+',re.sub(r'(http://|https://|www)', '', x[\"url\"])):\n",
    "                list_visits.append(word)\n",
    "        return list_visits\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "get_visits_list_udf = F.udf(get_visits_list, ArrayType(StringType()))\n",
    "\n",
    "def get_uid(data):\n",
    "    uid = eval(data)[\"uid\"]\n",
    "    return uid\n",
    "get_uid_udf = F.udf(get_uid, StringType())\n",
    "\n",
    "def get_data(data):\n",
    "    visits = eval(data)[\"visits\"]\n",
    "    uid = eval(data)[\"uid\"]\n",
    "    list_data = []\n",
    "    for visit in eval(visits): \n",
    "        for i in re.findall(r'\\w+',re.sub(r'(http://|https://|www)', '', i[\"url\"])):\n",
    "            list_data.append(i)  \n",
    "    return list_data\n",
    "get_data_udf = F.udf(get_data, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def process_batch(batch_df, batch_id): \n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": KAFKA_BOOTSTRAP_SERVER,\n",
    "   \"topic\": \"evgeniy.osipchuk\", \n",
    "    \"truncate\": \"false\", \n",
    "    \"numRows\": \"1000\"    \n",
    "    }\n",
    "    \n",
    "    batch_df = batch_df.select('value',\n",
    "                     get_data_udf(batch_df.value).alias('parsed_json'),\n",
    "                     get_uid_udf(batch_df.value).alias('uid'))\n",
    "    result = logreg_model_age.transform(batch_df)\n",
    "    result = (result\n",
    "                  .withColumn(\"age\",\n",
    "                              F.when(F.col(\"prediction_age\")==0, '18-24')\n",
    "                              .when(F.col(\"prediction_age\")==1, '25-34')\n",
    "                              .when(F.col(\"prediction_age\")==2, '35-44')\n",
    "                              .when(F.col(\"prediction_age\")==3, '45-54')\n",
    "                              .otherwise(\">=55\"))\n",
    "                  .select([\"uid\", \"age\", \"parsed_json\"]))\n",
    "    result = pipeline_gender.transform(result)\n",
    "    result = result.withColumn(\"gender\", F.when(F.col(\"prediction_gender\")==1, 'F').otherwise('M'))\n",
    "    result = result.select([\"uid\", \"gender\", \"age\"])\n",
    "    result = result.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "    result\\\n",
    "     .write\\\n",
    "     .format('kafka')\\\n",
    "     .options(**write_kafka_params)\\\n",
    "     .mode('append')\\\n",
    "     .save()\n",
    "\n",
    "def create_sink(df):\n",
    "    return df.writeStream\\\n",
    "            .foreachBatch(process_batch)\\\n",
    "            .option('checkpointLocation', 'streaming/chk/chk_evgeniy_osipchuk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.load(\"/labs/slaba04/gender_age_dataset.txt\",\n",
    "                        format = \"csv\",\n",
    "                        sep=\"\\t\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|         parsed_json|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|[zebra, zoya, ru,...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|[sweetrading, ru,...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|[ru, oriflame, co...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|[translate, tatto...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|[mail, rambler, r...|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('parsed_json', get_visits_list_udf(data.user_json))\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingTF(numFeatures=2500, binary=True, inputCol=\"parsed_json\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"gender_cat\", F.when(F.col(\"gender\")=='F', 1).otherwise(0))\n",
    "\n",
    "data = data.withColumn(\"age_cat\", (F.when(F.col(\"age\")=='18-24', 0)\n",
    "                                   .when(F.col(\"age\")=='25-34', 1)\n",
    "                                   .when(F.col(\"age\")=='35-44', 2)\n",
    "                                   .when(F.col(\"age\")=='45-54', 3)\n",
    "                                   .otherwise(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = (StandardScaler().setInputCol(\"words\").setOutputCol(\"scaled_words\"))\n",
    "\n",
    "logreg = LogisticRegression(featuresCol=\"scaled_words\",\n",
    "                            rawPredictionCol='rawPrediction_gender',\n",
    "                            predictionCol='prediction_gender',\n",
    "                            labelCol=\"gender_cat\",\n",
    "                            maxIter=30)\n",
    "\n",
    "pipeline = Pipeline(stages=[hasher, std_scaler, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gender = data.sampleBy(\"gender_cat\",\n",
    "                             fractions={0: 0.75, 1: 0.75},\n",
    "                             seed=42)\n",
    "test_gender = data.filter(~F.col(\"uid\").isin(train_gender.select(F.collect_list('uid')).first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30916"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10222"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gender.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gender = pipeline.fit(train_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipeline_gender.transform(test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROCAUC = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction_gender\",\n",
    "                                       labelCol=\"gender_cat\",\n",
    "                                       metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6745571763262851"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROCAUC.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_age = data.sampleBy(\"age_cat\",\n",
    "                          fractions={0: 0.75,\n",
    "                                     1: 0.75,\n",
    "                                     2:0.75,\n",
    "                                     3:0.75,\n",
    "                                     4:0.75},\n",
    "                          seed=42)\n",
    "test_age = data.filter(~F.col(\"uid\").isin(train_age.select(F.collect_list('uid')).first()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_age = RandomForestClassifier(featuresCol=\"scaled_words\",\n",
    "                                 labelCol=\"age_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_age = LogisticRegression(featuresCol=\"scaled_words\",\n",
    "                                rawPredictionCol='rawPrediction_age',\n",
    "                                predictionCol='prediction_age',\n",
    "                                labelCol=\"age_cat\",\n",
    "                                maxIter=30, regParam=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[hasher, std_scaler, logreg_age])\n",
    "logreg_model_age = pipeline.fit(train_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_age = logreg_model_age.transform(test_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = MulticlassClassificationEvaluator(labelCol=\"age_cat\",\n",
    "                                       predictionCol=\"prediction_age\",\n",
    "                                       metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26149148156245305"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.evaluate(pred_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': INPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'latest'\n",
    "}\n",
    "kafka_data = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .load()\n",
    ")\n",
    "kafka_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/24 21:15:01 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/evgeniy.osipchuk/streaming/chk' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/evgeniy.osipchuk/.Trash/Current/user/evgeniy.osipchuk/streaming/chk1616609701748\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/evgeniy.osipchuk/streaming/chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = create_sink(kafka_data)\n",
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
