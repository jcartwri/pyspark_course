{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Sabilov lab3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_length = 10000\n",
    "col_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id\tchannel_id\tdatetime_availability_start\tdatetime_availability_stop\tdatetime_show_start\tdatetime_show_stop\tcontent_type\ttitle\tyear\tgenres\tregion_id\r\n",
      "65667\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tна пробах только девушки (all girl auditions)\t2013.0\tЭротика\t\r\n",
      "65669\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tскуби ду: эротическая пародия (scooby doo: a xxx parody)\t2011.0\tЭротика\t\r\n",
      "65668\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tгорячие девочки для горячих девочек (hot babes 4 hot babes)\t2011.0\tЭротика\t\r\n",
      "65671\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tсоблазнительницы женатых мужчин (top heavy homewreckers)\t2011.0\tЭротика\t\r\n",
      "65670\t\t1970-01-01T00:00:00Z\t2018-01-01T00:00:00Z\t\t\t1\tсекретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody)\t2010.0\tЭротика\t\r\n",
      "65809\t\t1970-01-01T00:00:00Z\t2099-12-31"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /labs/slaba03/laba03_items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType, TimestampType\n",
    "train_schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "test_schema = StructType(fields=[\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('purchase', DoubleType())\n",
    "])\n",
    "\n",
    "items_schema = StructType(fields=[\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('channel_id', IntegerType()),\n",
    "    StructField('datetime_availability_start', TimestampType()),\n",
    "    StructField('datetime_availability_stop', TimestampType()),\n",
    "    StructField('datetime_show_start', TimestampType()),\n",
    "    StructField('datetime_show_stop', TimestampType()),\n",
    "    StructField('content_type', IntegerType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('year', DoubleType()),\n",
    "    StructField('genres', StringType()),\n",
    "    StructField('region_id', IntegerType())\n",
    "])\n",
    "\n",
    "views_schema = StructType(fields=[\n",
    "    StructField('user_id', IntegerType()),\n",
    "    StructField('item_id', IntegerType()),\n",
    "    StructField('ts_start', LongType()),\n",
    "    StructField('ts_end', LongType()),\n",
    "    StructField('item_type', StringType())\n",
    "])\n",
    "\n",
    "\n",
    "train = spark.read.csv('/labs/slaba03/laba03_train.csv', header=True, schema=train_schema)\n",
    "test = spark.read.csv('/labs/slaba03/laba03_test.csv', header=True, schema=test_schema)\n",
    "items = spark.read.csv('/labs/slaba03/laba03_items.csv', sep='\\t', header=True, schema=items_schema)\n",
    "views = spark.read.csv('/labs/slaba03/laba03_views_programmes.csv', header=True, schema=views_schema)\n",
    "# train.show(5)\n",
    "# test.show(5)\n",
    "# items.show(3, vertical=True, truncate=False)\n",
    "# views.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sampleBy(\"purchase\", fractions={0: 0.01, 1: 0.01}, seed=42)\n",
    "# test = test.sample(False, 0.01, 43)\n",
    "# print(train.count())\n",
    "# print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering paid content\n",
    "# items = items.filter(items.content_type == 1)\n",
    "col_vector += ['content_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = views.filter(views.item_id >= 326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "user_views = views.groupBy('user_id').agg(f.count('item_id').alias('items_viewed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_views = views.groupBy('item_id').agg(f.count('user_id').alias('users_viewed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = views.join(user_views, 'user_id', how='left').join(item_views, 'item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = views.withColumn('item_view_time', f.col('ts_end') - f.col('ts_start'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sum_view = views.groupBy('user_id', 'items_viewed').agg(f.sum('item_view_time').alias('item_sum_view'))\n",
    "\n",
    "user_mean_time_view = user_sum_view.withColumn('user_mean_time_view', f.col('item_sum_view') / (f.col('items_viewed') + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_type labling\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "views = views.fillna('9999', subset=['item_type'])\n",
    "\n",
    "item_types = views.select('item_type').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "item_types_dict = dict(zip(item_types, range(len(item_types))))\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*item_types_dict.items())])\n",
    "\n",
    "views = views.withColumn('item_type_lbl', mapping_expr[views['item_type']]).drop('item_type')\n",
    "\n",
    "\n",
    "mean_view_type = views.groupBy('user_id').agg(f.mean('item_type_lbl').alias('mean_view_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join all to train and test df\n",
    "train = train.join(items, on='item_id', how='left')\n",
    "test = test.join(items, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(user_mean_time_view, on='user_id', how='left')\n",
    "test = test.join(user_mean_time_view, on='user_id', how='left')\n",
    "train = train.join(mean_view_type, on='user_id', how='left')\n",
    "test = test.join(mean_view_type, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.show(2, False, True)\n",
    "# test.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.count())\n",
    "# train = train.dropDuplicates(subset=['user_id', 'item_id'])\n",
    "# print(train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition dfs\n",
    "train = train.coalesce(3)\n",
    "test = test.coalesce(3)\n",
    "# print(train.rdd.getNumPartitions())\n",
    "# print(test.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df's count\n",
    "# print(train.count())\n",
    "# print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate the duration of viewing video by user\n",
    "# import pyspark.sql.functions as f\n",
    "# train = train.withColumn('viewing_time', f.col('ts_end') - f.col('ts_start')).drop('ts_end', 'ts_start')\n",
    "# test = test.withColumn('viewing_time', f.col('ts_end') - f.col('ts_start')).drop('ts_end', 'ts_start')\n",
    "# # train.show(2, False, True)\n",
    "# # test.show(2, False, True)\n",
    "# col_vector += ['viewing_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # item_type labling\n",
    "# from itertools import chain\n",
    "# from pyspark.sql.functions import create_map, lit\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# train = train.fillna('9999', subset=['item_type'])\n",
    "# test = test.fillna('9999', subset=['item_type'])\n",
    "\n",
    "# item_types = train.select('item_type').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# item_types_dict = dict(zip(item_types, range(len(item_types))))\n",
    "\n",
    "# mapping_expr = create_map([lit(x) for x in chain(*item_types_dict.items())])\n",
    "\n",
    "# train = train.withColumn('item_type_lbl', mapping_expr[train['item_type']]).drop('item_type')\n",
    "# # train.show(2, False, True)\n",
    "\n",
    "# item_types = test.select('item_type').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# item_types_dict = dict(zip(item_types, range(len(item_types))))\n",
    "\n",
    "# mapping_expr = create_map([lit(x) for x in chain(*item_types_dict.items())])\n",
    "\n",
    "# test = test.withColumn('item_type_lbl', mapping_expr[test['item_type']]).drop('item_type')\n",
    "# # test.show(2, False, True)\n",
    "# #-------------------------------------------------------------------------------------------\n",
    "# # train_indexer = StringIndexer(inputCol=\"item_type\", outputCol=\"item_type_lbl\") \n",
    "# # train = train_indexer.fit(train).transform(train)\n",
    "\n",
    "# # test_indexer = StringIndexer(inputCol=\"item_type\", outputCol=\"item_type_lbl\")\n",
    "# # test = test_indexer.fit(test).transform(test)\n",
    "\n",
    "# col_vector += ['item_type_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres labling\n",
    "train = train.fillna('Без жанра', subset=['genres'])\n",
    "train = train.fillna(-9999, subset=['year'])\n",
    "genres = train.select('genres').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "genres_dict = dict(zip(genres, range(len(genres))))\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*genres_dict.items())])\n",
    "\n",
    "train = train.withColumn('genres_lbl', mapping_expr[train['genres']]).drop('genres')\n",
    "# train.show(2, False, True)\n",
    "\n",
    "test = test.fillna('Без жанра', subset=['genres'])\n",
    "test = test.fillna(-9999, subset=['year'])\n",
    "genres = test.select('genres').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "genres_dict = dict(zip(genres, range(len(genres))))\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*genres_dict.items())])\n",
    "\n",
    "test = test.withColumn('genres_lbl', mapping_expr[test['genres']]).drop('genres')\n",
    "# test.show(2, False, True)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# train_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_lbl\") \n",
    "# train = train_indexer.fit(train).transform(train)\n",
    "\n",
    "# test_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_lbl\")\n",
    "# test = test_indexer.fit(test).transform(test)\n",
    "\n",
    "col_vector += ['genres_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(items.select('region_id').distinct().rdd.flatMap(lambda x: x).collect())\n",
    "# print(items.select('channel_id').distinct().rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tfidf on title col\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "train = train.withColumn('splited', f.split('title', '[^a-zA-ZА-Яа-я0-9+]{1,}'))\n",
    "\n",
    "ht = HashingTF(inputCol='splited', outputCol='tf', numFeatures=dict_length, binary=True)\n",
    "train = ht.transform(train)\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf').fit(train)\n",
    "train = idf.transform(train)\n",
    "# train.show(2, False, True)\n",
    "\n",
    "test = test.withColumn('splited', f.split('title', '[^a-zA-ZА-Яа-я0-9+]{1,}'))\n",
    "\n",
    "test = ht.transform(test)\n",
    "test = idf.transform(test)\n",
    "# test.show(2, False, True)\n",
    "col_vector += ['tfidf']\n",
    "\n",
    "\n",
    "# word2vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"splited\", outputCol=\"word2vec\")\n",
    "model = word2Vec.fit(train)\n",
    "train = model.transform(train).drop('tf', 'splited', 'title', 'region_id', 'channel_id')\n",
    "\n",
    "test = model.transform(test).drop('tf', 'splited', 'title', 'region_id', 'channel_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate item availability\n",
    "import datetime\n",
    "\n",
    "available = f.when(\n",
    "    (train.datetime_availability_stop > datetime.datetime(2021, 1, 1, 0, 0)) &\n",
    "    (train.datetime_availability_start < datetime.datetime(2021, 1, 1, 0, 0)),\n",
    "    1\n",
    ").otherwise(0)\n",
    "train = train.withColumn('item_availability', available).drop('datetime_availability_stop', \n",
    "                                                              'datetime_availability_start')\n",
    "# train.show(10, False, True)\n",
    "\n",
    "available = f.when(\n",
    "    (test.datetime_availability_stop > datetime.datetime(2021, 1, 1, 0, 0)) &\n",
    "    (test.datetime_availability_start < datetime.datetime(2021, 1, 1, 0, 0)),\n",
    "    1\n",
    ").otherwise(0)\n",
    "test = test.withColumn('item_availability', available).drop('datetime_availability_stop', \n",
    "                                                            'datetime_availability_start')\n",
    "# test.show(10, False, True)\n",
    "col_vector += ['item_availability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # items features vectorization\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# col_vector += ['year']\n",
    "\n",
    "# train = train.fillna(-9999, subset=['year', 'viewing_time'])\n",
    "# test = test.fillna(-9999, subset=['year', 'viewing_time'])\n",
    "\n",
    "# train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "#                                   outputCol=\"features\")\n",
    "# # drop_cols = ['datetime_availability_start', \n",
    "# #              'datetime_availability_stop',\n",
    "# #              'datetime_show_start',\n",
    "# #              'datetime_show_stop'] + col_vector\n",
    "# train = train_assembler.transform(train).drop(*drop_cols)\n",
    "# train.show(5, truncate=False)\n",
    "\n",
    "# test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "#                                   outputCol=\"features\")\n",
    "# test = test_assembler.transform(test).drop(*drop_cols)\n",
    "# test.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.cache()\n",
    "# test.cache()\n",
    "# train.show(2, False, True)\n",
    "# test.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to now\n",
    "train = train.withColumn('years_to_now', f.lit(2021) - f.col('year'))\n",
    "test = test.withColumn('years_to_now', f.lit(2021) - f.col('year'))\n",
    "# train.show(1, False, True)\n",
    "# test.show(1, False, True)\n",
    "col_vector += ['years_to_now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = train.fillna(-9999, subset=[\n",
    "    'content_type', \n",
    "    'year', \n",
    "    'items_viewed', \n",
    "    'user_mean_time_view', \n",
    "    'mean_view_type',\n",
    "    'genres_lbl',\n",
    "    'item_availability',\n",
    "    'years_to_now'\n",
    "                                   ])\n",
    "\n",
    "test = test.fillna(-9999, subset=[\n",
    "    'content_type', \n",
    "    'year', \n",
    "    'items_viewed', \n",
    "    'user_mean_time_view', \n",
    "    'mean_view_type',\n",
    "    'genres_lbl',\n",
    "    'item_availability',\n",
    "    'years_to_now'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature col without tfidf\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# col_vector = ['content_type', \n",
    "#               'year', \n",
    "#               'items_viewed', \n",
    "#               'user_mean_time_view',\n",
    "#               'mean_view_type',\n",
    "#               'genres_lbl',\n",
    "#               'item_availability',\n",
    "#               'years_to_now'\n",
    "#              ]\n",
    "# train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "#                                   outputCol=\"features\")\n",
    "# train = train_assembler.transform(train)\n",
    "# # train.show(1, vertical=True, truncate=False)\n",
    "\n",
    "# test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "#                                   outputCol=\"features\")\n",
    "# test = test_assembler.transform(test)\n",
    "# # test.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature col with tfidf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "col_vector = ['content_type', \n",
    "              'year', \n",
    "              'items_viewed', \n",
    "              'user_mean_time_view',\n",
    "              'mean_view_type',\n",
    "              'genres_lbl',\n",
    "              'item_availability',\n",
    "              'years_to_now',\n",
    "              'tfidf'\n",
    "             ]\n",
    "train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_tfidf\")\n",
    "train = train_assembler.transform(train)\n",
    "# train.show(1, vertical=True, truncate=False)\n",
    "\n",
    "test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_tfidf\")\n",
    "test = test_assembler.transform(test)\n",
    "# test.show(1, vertical=True, truncate=False)\n",
    "\n",
    "\n",
    "col_vector = ['content_type', \n",
    "              'year', \n",
    "              'items_viewed', \n",
    "              'user_mean_time_view',\n",
    "              'mean_view_type',\n",
    "              'genres_lbl',\n",
    "              'item_availability',\n",
    "              'years_to_now',\n",
    "              'word2vec'\n",
    "             ]\n",
    "train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_w2v\")\n",
    "train = train_assembler.transform(train)\n",
    "# train.show(1, vertical=True, truncate=False)\n",
    "\n",
    "test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_w2v\")\n",
    "test = test_assembler.transform(test)\n",
    "# test.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Normalizer\n",
    "# normalizer = Normalizer(p=2.0, inputCol=\"features\", outputCol=\"features_norm\")\n",
    "# train = normalizer.transform(train)\n",
    "# test = normalizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparseVector' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-26a9cdc22119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m tfidf = f.when(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m ).otherwise(train.tfidf)\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf_ed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mwhen\u001b[0;34m(condition, value)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be a Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1218\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1218\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mcommand_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparseVector' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "tfidf = f.when(\n",
    "    (train.content_type == 0),\n",
    "    Vectors.sparse(dict_length, [0], [0])\n",
    ").otherwise(train.tfidf)\n",
    "train.withColumn('tfidf_ed', tfidf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('year_edited', f.col('year') * f.col('content_type'))\n",
    "train = train.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "train = train.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "train = train.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "train = train.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "train = train.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "train = train.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# train = train.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))\n",
    "\n",
    "test = test.withColumn('year_edited', f.col('year') * f.col('content_type'))\n",
    "test = test.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "test = test.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "test = test.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "test = test.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "test = test.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "test = test.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# test = test.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorAssembler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3be425e37967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m              ]\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m train_assembler = VectorAssembler(inputCols=col_vector, \n\u001b[0m\u001b[1;32m     30\u001b[0m                                   outputCol=\"features_tfidf_edited\")\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_assembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorAssembler' is not defined"
     ]
    }
   ],
   "source": [
    "train = train.withColumn('year_edited', f.col('year') * f.col('content_type'))\n",
    "train = train.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "train = train.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "train = train.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "train = train.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "train = train.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "train = train.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# train = train.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))\n",
    "\n",
    "test = test.withColumn('year_edited', f.col('year') * f.col('content_type'))\n",
    "test = test.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "test = test.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "test = test.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "test = test.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "test = test.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "test = test.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# test = test.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))\n",
    "\n",
    "col_vector = ['year_edited',\n",
    "              'items_viewed_edited',\n",
    "              'user_mean_time_view_edited',\n",
    "              'mean_view_type_edited',\n",
    "              'genres_lbl_edited',\n",
    "              'item_availability_edited',\n",
    "              'years_to_now_edited',\n",
    "              'tfidf'\n",
    "             ]\n",
    "\n",
    "train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_tfidf_edited\")\n",
    "train = train_assembler.transform(train)\n",
    "\n",
    "test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_tfidf_edited\")\n",
    "test = test_assembler.transform(test)\n",
    "\n",
    "\n",
    "col_vector = ['year_edited',\n",
    "              'items_viewed_edited',\n",
    "              'user_mean_time_view_edited',\n",
    "              'mean_view_type_edited',\n",
    "              'genres_lbl_edited',\n",
    "              'item_availability_edited',\n",
    "              'years_to_now_edited',\n",
    "              'word2vec'\n",
    "             ]\n",
    "\n",
    "train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_w2v_edited\")\n",
    "train = train_assembler.transform(train)\n",
    "\n",
    "test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features_w2v_edited\")\n",
    "test = test_assembler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year_edited', 'items_viewed_edited', 'user_mean_time_view_edited', 'mean_view_type_edited', 'genres_lbl_edited', 'item_availability_edited', 'years_to_now_edited', 'word2vec']\n"
     ]
    }
   ],
   "source": [
    "print(col_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features_w2v_edited', labelCol=\"purchase\", maxIter=20)\n",
    "\n",
    "# rf = RandomForestClassifier(featuresCol='features_all', labelCol=\"purchase\", numTrees=50, maxDepth=9, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", \n",
    "                                          labelCol=\"purchase\", \n",
    "                                          metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = lr.fit(train)\n",
    "# preds = model.transform(test)\n",
    "# preds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# gbt = GBTClassifier(maxIter=50, \n",
    "#                     maxDepth=7, \n",
    "#                     featuresCol='features', \n",
    "#                     labelCol=\"purchase\", \n",
    "#                     subsamplingRate=0.7, \n",
    "#                     seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cros-val lr fitting\n",
    "train.cache()\n",
    "test.cache()\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [20])\\\n",
    "                              .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator, numFolds=5, parallelism=3)\n",
    "\n",
    "cv_model = crossval.fit(train)\n",
    "print('average metrics - ', cv_model.avgMetrics)\n",
    "\n",
    "\n",
    "predictions_lr = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Cros-val rf fitting\n",
    "\n",
    "# paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10])\\\n",
    "#                               .build()\n",
    "# crossval = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid,\n",
    "#                               evaluator=evaluator, numFolds=5, parallelism=3)\n",
    "\n",
    "\n",
    "# cv_model = crossval.fit(train)\n",
    "# print('average metrics - ', cv_model.avgMetrics)\n",
    "\n",
    "\n",
    "# predictions_rf = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-864bef42e82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_lr' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = predictions_lr.orderBy(['user_id', 'item_id'], ascending=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n",
    "\n",
    "predictions = predictions.withColumn('proba', to_array(col('probability')))\n",
    "\n",
    "predictions.select(f.col('user_id'), \n",
    "                     f.col('item_id'),\n",
    "                     f.col('proba').getItem(1).alias('purchase')\n",
    "                    ).toPandas().to_csv('/data/home/farhad.sabilov/lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.write.csv('train_1000000.csv')\n",
    "# test.write.csv('test_1000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5032624\n",
      "5032624\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_purchases = train.groupBy('user_id')\\\n",
    "                .sum().select(col(\"sum(purchase)\").alias(\"user_purchases\"), col(\"user_id\")).cache()\n",
    "item_purchases = train.groupBy('item_id')\\\n",
    "                        .sum().select(col(\"sum(purchase)\").alias(\"item_purchases\"), col(\"item_id\")).cache()\n",
    "\n",
    "\n",
    "train = train.join(train_purchases, on='user_id', how='left')\n",
    "test = test.join(train_purchases, on='user_id', how='left')\n",
    "\n",
    "train = train.join(item_purchases, on='item_id', how='left')\n",
    "test = test.join(item_purchases, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_user_attempts = train.groupBy('user_id').count().select(col(\"count\").alias(\"user_attempts\"), col(\"user_id\"))\\\n",
    "                            .cache()\n",
    "\n",
    "train_item_attempts = train.groupBy('item_id').count().select(col(\"count\").alias(\"item_attempts\"), col(\"item_id\"))\\\n",
    "                            .cache()\n",
    "\n",
    "\n",
    "train = train.join(train_user_attempts, on='user_id', how='left')\n",
    "test = test.join(train_user_attempts, on='user_id', how='left')\n",
    "\n",
    "train = train.join(train_item_attempts, on='item_id', how='left')\n",
    "test = test.join(train_item_attempts, on='item_id', how='left')\n",
    "\n",
    "\n",
    "train = train.withColumn('user_addict', col('user_purchases') / col('user_attempts'))\n",
    "test = test.withColumn('user_addict', col('user_purchases') / col('user_attempts'))\n",
    "\n",
    "train = train.withColumn('item_addict', col('item_purchases') / col('item_attempts'))\n",
    "test = test.withColumn('item_addict', col('item_purchases') / col('item_attempts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "train = train.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "train = train.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "train = train.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "train = train.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "train = train.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# train = train.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))\n",
    "\n",
    "test = test.withColumn('items_viewed_edited', f.col('items_viewed') * f.col('content_type'))\n",
    "test = test.withColumn('user_mean_time_view_edited', f.col('user_mean_time_view') * f.col('content_type'))\n",
    "test = test.withColumn('mean_view_type_edited', f.col('mean_view_type') * f.col('content_type'))\n",
    "test = test.withColumn('genres_lbl_edited', f.col('genres_lbl') * f.col('content_type'))\n",
    "test = test.withColumn('item_availability_edited', f.col('item_availability') * f.col('content_type'))\n",
    "test = test.withColumn('years_to_now_edited', f.col('years_to_now') * f.col('content_type'))\n",
    "# test = test.withColumn('tfidf_edited', f.col('tfidf') * f.col('content_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('item_purchases_edited', f.col('item_purchases') * f.col('content_type'))\n",
    "train = train.withColumn('user_purchases_edited', f.col('user_purchases') * f.col('content_type'))\n",
    "train = train.withColumn('user_addict_edited', f.col('user_addict') * f.col('content_type'))\n",
    "train = train.withColumn('item_addict_edited', f.col('item_addict') * f.col('content_type'))\n",
    "\n",
    "test = test.withColumn('item_purchases_edited', f.col('item_purchases') * f.col('content_type'))\n",
    "test = test.withColumn('user_purchases_edited', f.col('user_purchases') * f.col('content_type'))\n",
    "test = test.withColumn('user_addict_edited', f.col('user_addict') * f.col('content_type'))\n",
    "test = test.withColumn('item_addict_edited', f.col('item_addict') * f.col('content_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------\n",
      " item_id                    | 8389                         \n",
      " user_id                    | 746713                       \n",
      " purchase                   | 0                            \n",
      " channel_id                 | null                         \n",
      " datetime_show_start        | null                         \n",
      " datetime_show_stop         | null                         \n",
      " content_type               | 1                            \n",
      " title                      | пес в сапогах (сурдоперевод) \n",
      " year                       | 1981.0                       \n",
      " region_id                  | null                         \n",
      " items_viewed               | 3                            \n",
      " item_sum_view              | 1481                         \n",
      " user_mean_time_view        | 493.6665021111659            \n",
      " mean_view_type             | 0.0                          \n",
      " genres_lbl                 | 550                          \n",
      " item_availability          | 1                            \n",
      " years_to_now               | 40.0                         \n",
      " year_edited                | 1981.0                       \n",
      " items_viewed_edited        | 3                            \n",
      " user_mean_time_view_edited | 493.6665021111659            \n",
      " mean_view_type_edited      | 0.0                          \n",
      " genres_lbl_edited          | 550                          \n",
      " item_availability_edited   | 1                            \n",
      " years_to_now_edited        | 40.0                         \n",
      " user_purchases             | 0                            \n",
      " item_purchases             | 8                            \n",
      " user_attempts              | 2553                         \n",
      " item_attempts              | 1338                         \n",
      " user_addict                | 0.0                          \n",
      " item_addict                | 0.005979073243647235         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "col_vector = ['items_viewed_edited',\n",
    "              'user_mean_time_view_edited',\n",
    "              'mean_view_type_edited',\n",
    "              'genres_lbl_edited',\n",
    "              'item_availability_edited',\n",
    "              'years_to_now_edited',\n",
    "              'item_purchases_edited',\n",
    "              'user_purchases_edited',\n",
    "              'user_addict_edited',\n",
    "              'item_addict_edited'\n",
    "             ]\n",
    "\n",
    "train_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features\")\n",
    "train = train_assembler.transform(train)\n",
    "\n",
    "test_assembler = VectorAssembler(inputCols=col_vector, \n",
    "                                  outputCol=\"features\")\n",
    "test = test_assembler.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"purchase\", maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    lr\n",
    "])\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cache()\n",
    "test.cache()\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [10])\\\n",
    "                              .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                          evaluator=evaluator, numFolds=3, parallelism=3)\n",
    "\n",
    "cv_model = crossval.fit(train)\n",
    "\n",
    "predictions_valid = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_valid.orderBy(['user_id', 'item_id'], ascending=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n",
    "\n",
    "predictions = predictions.withColumn('proba', to_array(col('probability')))\n",
    "\n",
    "predictions.select(f.col('user_id'), \n",
    "                     f.col('item_id'),\n",
    "                     f.col('proba').getItem(1).alias('purchase')\n",
    "                    ).toPandas().to_csv('/data/home/farhad.sabilov/lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.show(2, False, True)\n",
    "# test.show(2, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.count())\n",
    "# train = train.dropDuplicates(subset=['user_id', 'item_id'])\n",
    "# print(train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition dfs\n",
    "train = train.coalesce(3)\n",
    "test = test.coalesce(3)\n",
    "# print(train.rdd.getNumPartitions())\n",
    "# print(test.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df's count\n",
    "# print(train.count())\n",
    "# print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate the duration of viewing video by user\n",
    "# import pyspark.sql.functions as f\n",
    "# train = train.withColumn('viewing_time', f.col('ts_end') - f.col('ts_start')).drop('ts_end', 'ts_start')\n",
    "# test = test.withColumn('viewing_time', f.col('ts_end') - f.col('ts_start')).drop('ts_end', 'ts_start')\n",
    "# # train.show(2, False, True)\n",
    "# # test.show(2, False, True)\n",
    "# col_vector += ['viewing_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # item_type labling\n",
    "# from itertools import chain\n",
    "# from pyspark.sql.functions import create_map, lit\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# train = train.fillna('9999', subset=['item_type'])\n",
    "# test = test.fillna('9999', subset=['item_type'])\n",
    "\n",
    "# item_types = train.select('item_type').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# item_types_dict = dict(zip(item_types, range(len(item_types))))\n",
    "\n",
    "# mapping_expr = create_map([lit(x) for x in chain(*item_types_dict.items())])\n",
    "\n",
    "# train = train.withColumn('item_type_lbl', mapping_expr[train['item_type']]).drop('item_type')\n",
    "# # train.show(2, False, True)\n",
    "\n",
    "# item_types = test.select('item_type').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# item_types_dict = dict(zip(item_types, range(len(item_types))))\n",
    "\n",
    "# mapping_expr = create_map([lit(x) for x in chain(*item_types_dict.items())])\n",
    "\n",
    "# test = test.withColumn('item_type_lbl', mapping_expr[test['item_type']]).drop('item_type')\n",
    "# # test.show(2, False, True)\n",
    "# #-------------------------------------------------------------------------------------------\n",
    "# # train_indexer = StringIndexer(inputCol=\"item_type\", outputCol=\"item_type_lbl\") \n",
    "# # train = train_indexer.fit(train).transform(train)\n",
    "\n",
    "# # test_indexer = StringIndexer(inputCol=\"item_type\", outputCol=\"item_type_lbl\")\n",
    "# # test = test_indexer.fit(test).transform(test)\n",
    "\n",
    "# col_vector += ['item_type_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres labling\n",
    "train = train.fillna('Без жанра', subset=['genres'])\n",
    "train = train.fillna(-9999, subset=['year'])\n",
    "genres = train.select('genres').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "genres_dict = dict(zip(genres, range(len(genres))))\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*genres_dict.items())])\n",
    "\n",
    "train = train.withColumn('genres_lbl', mapping_expr[train['genres']]).drop('genres')\n",
    "# train.show(2, False, True)\n",
    "\n",
    "test = test.fillna('Без жанра', subset=['genres'])\n",
    "test = test.fillna(-9999, subset=['year'])\n",
    "genres = test.select('genres').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "genres_dict = dict(zip(genres, range(len(genres))))\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*genres_dict.items())])\n",
    "\n",
    "test = test.withColumn('genres_lbl', mapping_expr[test['genres']]).drop('genres')\n",
    "# test.show(2, False, True)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# train_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_lbl\") \n",
    "# train = train_indexer.fit(train).transform(train)\n",
    "\n",
    "# test_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_lbl\")\n",
    "# test = test_indexer.fit(test).transform(test)\n",
    "\n",
    "col_vector += ['genres_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
