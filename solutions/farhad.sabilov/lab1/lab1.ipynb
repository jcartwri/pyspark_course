{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark SQL seminar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"rating\", StringType()),\n",
    "    StructField(\"timestamp\", LongType())])\n",
    "df = spark.read.csv('/labs/laba01/ml-100k/u.data', sep='\\t', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|    196|    242|     3|881250949|\n",
      "|    186|    302|     3|891717742|\n",
      "|     22|    377|     1|878887116|\n",
      "|    244|     51|     2|880606923|\n",
      "|    166|    346|     1|886397596|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|rating|hist_film|hist_all|\n",
      "+------+---------+--------+\n",
      "|     1|       12|    6110|\n",
      "|     2|       40|   11370|\n",
      "|     3|       94|   27145|\n",
      "|     4|      109|   34174|\n",
      "|     5|       40|   21201|\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "hfilm = df.where(df.item_id == 328).groupby('rating').count().orderBy('rating').select('rating', f.col('count').alias('hist_film'))\n",
    "hall = df.groupby('rating').count().orderBy('rating').select('rating', f.col('count').alias('hist_all'))\n",
    "joined_df = hfilm.join(hall, on='rating', how='inner')\n",
    "hist_film = joined_df.select(joined_df.hist_film).rdd.flatMap(lambda x: x).collect()\n",
    "hist_all = joined_df.select(joined_df.hist_all).rdd.flatMap(lambda x: x).collect()\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\"hist_film\": hist_film,\n",
    "        \"hist_all\": hist_all\n",
    "}\n",
    "with open('/data/home/farhad.sabilov/lab01.json', 'w') as fout:\n",
    "    json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
