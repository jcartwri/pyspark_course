{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"andrey.frolov\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-3.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>andrey.frolov</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f57b7895a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType(fields=[\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"provider\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"desc\", StringType())\n",
    "])\n",
    "\n",
    "data = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\", schema=data_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------+---+--------------------+\n",
      "|lang|                name|                 cat|      provider| id|                desc|\n",
      "+----+--------------------+--------------------+--------------+---+--------------------+\n",
      "|  en|Accounting Cycle:...|3/business_manage...|Canvas Network|  4|This course intro...|\n",
      "|  en|American Counter ...|              11/law|Canvas Network|  5|This online cours...|\n",
      "|  fr|Arithmétique: en ...|5/computer_scienc...|Canvas Network|  6|This course is ta...|\n",
      "|  en|Becoming a Dynami...|  14/social_sciences|Canvas Network|  7|We live in a digi...|\n",
      "|  en|           Bioethics|2/biology_life_sc...|Canvas Network|  8|This self-paced c...|\n",
      "+----+--------------------+--------------------+--------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(ArrayType(StringType()))\n",
    "def get_words(desc):\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "    return desc.apply(lambda x: regex.findall(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_token = data.withColumn(\"tokens\", get_words(\"desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------+---+--------------------+--------------------+\n",
      "|lang|                name|                 cat|      provider| id|                desc|              tokens|\n",
      "+----+--------------------+--------------------+--------------+---+--------------------+--------------------+\n",
      "|  en|Accounting Cycle:...|3/business_manage...|Canvas Network|  4|This course intro...|[this, course, in...|\n",
      "|  en|American Counter ...|              11/law|Canvas Network|  5|This online cours...|[this, online, co...|\n",
      "|  fr|Arithmétique: en ...|5/computer_scienc...|Canvas Network|  6|This course is ta...|[this, course, is...|\n",
      "|  en|Becoming a Dynami...|  14/social_sciences|Canvas Network|  7|We live in a digi...|[we, live, in, di...|\n",
      "|  en|           Bioethics|2/biology_life_sc...|Canvas Network|  8|This self-paced c...|[this, self, pace...|\n",
      "+----+--------------------+--------------------+--------------+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_token.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtf = HashingTF(numFeatures=10000, inputCol=\"tokens\", outputCol='tf')\n",
    "tf = hashtf.transform(data_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"tf\", outputCol=\"idf\").fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = tfidf.join(\n",
    "    tfidf.select(f.col('lang'), f.col('id').alias('id2'), f.col('idf').alias('idf2')), \n",
    "    on='lang', \n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_joined.select(\n",
    "    f.col('lang'),    \n",
    "    f.col('name'),\n",
    "    f.col('cat'),\n",
    "    f.col('provider'),\n",
    "    f.col('desc'),\n",
    "    f.col('id'),\n",
    "    f.col('id2'),\n",
    "    f.col('idf'),\n",
    "    f.col('idf2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my courses\n",
    "df = df.filter(f.col('id').isin([23126, 21617, 16627, 11556, 16704, 13702]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Normalizer(inputCol=\"idf\", outputCol=\"idf_norm\", p=2) \\\n",
    "    .transform(df)\n",
    "df = Normalizer(inputCol=\"idf2\", outputCol=\"idf2_norm\", p=2) \\\n",
    "    .transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotProductUdf = f.udf(\n",
    "    lambda v1, v2: float(v1.dot(v2)), DoubleType())\n",
    "\n",
    "df = df.withColumn('cosine', dotProductUdf(f.col('idf_norm'), f.col('idf2_norm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_window = Window().partitionBy('id').orderBy(f.col('cosine').desc(), f.col('name'), f.col('id2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('n_row', f.row_number().over(order_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(f.col('id') != f.col('id2')).filter(f.col('n_row') <= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"11556\": df.filter(f.col('id') == 11556).select('id2').rdd.flatMap(lambda x: x).collect(),\n",
    "    \"13702\": df.filter(f.col('id') == 13702).select('id2').rdd.flatMap(lambda x: x).collect(),\n",
    "    \"16627\": df.filter(f.col('id') == 16627).select('id2').rdd.flatMap(lambda x: x).collect(),\n",
    "    \"16704\": df.filter(f.col('id') == 16704).select('id2').rdd.flatMap(lambda x: x).collect(),\n",
    "    \"21617\": df.filter(f.col('id') == 21617).select('id2').rdd.flatMap(lambda x: x).collect(),\n",
    "    \"23126\": df.filter(f.col('id') == 23126).select('id2').rdd.flatMap(lambda x: x).collect()\n",
    "}\n",
    "\n",
    "with open('/data/home/andrey.frolov/lab02.json', 'w') as fl:\n",
    "    json.dump(data, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
