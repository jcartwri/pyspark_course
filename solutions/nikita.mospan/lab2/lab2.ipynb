{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 8 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder \\\n",
    "    .appName(\"nikita.mospan lab2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab2 parameters\n",
    "\n",
    "inputFilePath = '/labs/slaba02/DO_record_per_line.json'\n",
    "idsOfCoursesForRecommendations = [23126,21617,16627,11556,16704,13702]\n",
    "numberOfFeatures = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "ID = \"id\"\n",
    "NAME = \"name\"\n",
    "LANG = \"lang\"\n",
    "DESC = \"desc\"\n",
    "CLEAN_DESC = \"cleanDesc\"\n",
    "WORDS = \"words\"\n",
    "CLEAN_WORDS = \"cleanWords\"\n",
    "RAW_FEATURES = \"rawFeatures\"\n",
    "FEATURES = \"features\"\n",
    "NORM_FEATURES = \"normFeatures\"\n",
    "CURR_NORM_FEATURES = \"currNormFeatures\"\n",
    "COSINE = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema to read courses data\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "inputSchema = StructType([\n",
    "    StructField(LANG, StringType()),\n",
    "    StructField(NAME, StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"provider\", StringType()),\n",
    "    StructField(ID, LongType()),\n",
    "    StructField(DESC, StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf to preprocess course description\n",
    "\n",
    "import string\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def cleanString(in_str):\n",
    "    return ' '.join(in_str.lower()\\\n",
    "            .translate(str.maketrans('', '', string.punctuation))\\\n",
    "            .translate(str.maketrans('', '', string.digits)).split())\n",
    "\n",
    "cleanStringUdf = udf(cleanString, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lang: string, name: string, cat: string, provider: string, id: bigint, desc: string, cleanDesc: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and cache courses data\n",
    "# caching is useful because dataframe will be reused for each course from idsOfCoursesForRecommendations\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "coursesDf = sparkSession.read.schema(inputSchema).format(\"json\").load(inputFilePath)\\\n",
    "            .withColumn(CLEAN_DESC, cleanStringUdf(col(DESC)))\n",
    "coursesDf.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf to calculate dot product between vectors\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "dotProductUdf = udf(lambda v1, v2: float(v1.dot(v2)), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Normalizer, StopWordsRemover, Tokenizer\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "def getRecommendationsForSingleCourse(courseId, coursesDf, numberOfFeatures):\n",
    "    courseLang = coursesDf.where(col(ID) == courseId).select(LANG).collect()[0][0]\n",
    "    tokenizer = Tokenizer(inputCol=CLEAN_DESC, outputCol=WORDS)\n",
    "    tokenizedCoursesDf = tokenizer.transform(coursesDf.where(col(LANG) == courseLang))\n",
    "    \n",
    "    stopWordsRemover = StopWordsRemover(inputCol=WORDS, outputCol=CLEAN_WORDS)\n",
    "    stopWordsRemovedCoursesDf = stopWordsRemover.transform(tokenizedCoursesDf)\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=CLEAN_WORDS, outputCol=RAW_FEATURES, numFeatures = numberOfFeatures)\n",
    "    coursesWithRawFeaturesDf = hashingTF.transform(stopWordsRemovedCoursesDf)\n",
    "    \n",
    "    idf = IDF(inputCol = RAW_FEATURES, outputCol = FEATURES)\n",
    "    coursesWithRescaledFeaturesDf = idf.fit(coursesWithRawFeaturesDf).transform(coursesWithRawFeaturesDf)\n",
    "    \n",
    "    normalizer = Normalizer(inputCol = FEATURES, outputCol = NORM_FEATURES)\n",
    "    coursesWithNormalizedFeaturesDf = normalizer.transform(coursesWithRescaledFeaturesDf)\n",
    "    \n",
    "    return coursesWithNormalizedFeaturesDf.where(col(ID) == courseId)\\\n",
    "            .select(col(NORM_FEATURES).alias(CURR_NORM_FEATURES))\\\n",
    "            .crossJoin(coursesWithNormalizedFeaturesDf)\\\n",
    "            .withColumn(COSINE, dotProductUdf(col(CURR_NORM_FEATURES), col(NORM_FEATURES)))\\\n",
    "            .where(col(ID) != courseId)\\\n",
    "            .orderBy(desc(COSINE), asc(NAME), asc(ID))\\\n",
    "            .select(ID)\\\n",
    "            .limit(10).toPandas().iloc[:, 0].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultAsDict = {}\n",
    "for courseId in idsOfCoursesForRecommendations:\n",
    "    resultAsDict[courseId] = getRecommendationsForSingleCourse(courseId, coursesDf, numberOfFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lab02.json', 'w') as lab02_out:\n",
    "    lab02_out.write(json.dumps(resultAsDict, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
