{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 64 pyspark-shell'\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"nikita.mospan lab3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, ByteType, TimestampType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "itemsSchema = StructType([\n",
    "    StructField(\"item_id\", LongType(), False),\n",
    "    StructField(\"channel_id\", StringType(), True), \n",
    "    StructField(\"datetime_availability_start\", TimestampType(), True),\n",
    "    StructField(\"datetime_availability_stop\", TimestampType(), True),\n",
    "    StructField(\"datetime_show_start\", TimestampType(), True),\n",
    "    StructField(\"datetime_show_stop\", TimestampType(), True),\n",
    "    StructField(\"content_type\", ByteType(), False),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    # year contains nulls so its type has to be string\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), False),\n",
    "    StructField(\"region_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "itemsDf = spark.read.format(\"csv\")\\\n",
    "    .schema(itemsSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"null\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"/labs/slaba03/laba03_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf = itemsDf.selectExpr(\"item_id\", \"genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTrainSchema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"item_id\", LongType(), False), \n",
    "    StructField(\"purchase\", ByteType(), False),\n",
    "])\n",
    "\n",
    "fullTrainDf = spark.read.format(\"csv\")\\\n",
    "    .schema(fullTrainSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"/labs/slaba03/laba03_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5032624\n",
    "# fullTrainDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknownTestSchema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"item_id\", LongType(), False),\n",
    "    StructField(\"purchase\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "unknownTestDf = spark.read.format(\"csv\")\\\n",
    "    .schema(unknownTestSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"/labs/slaba03/laba03_test.csv\") \\\n",
    "    .select(\"user_id\", \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknownTestDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewItemsSchema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"item_id\", LongType(), False),\n",
    "    StructField(\"ts_start\", LongType(), False),\n",
    "    StructField(\"ts_end\", LongType(), False),\n",
    "    StructField(\"item_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "viewItemsDf = spark.read.format(\"csv\")\\\n",
    "    .schema(viewItemsSchema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"failfast\") \\\n",
    "    .load(\"/labs/slaba03/laba03_views_programmes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTrainDf.createOrReplaceTempView(\"full_train\")\n",
    "itemsDf.createOrReplaceTempView(\"items\")\n",
    "unknownTestDf.createOrReplaceTempView(\"unknown_test\")\n",
    "viewItemsDf.createOrReplaceTempView(\"views_of_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsGenresDf = spark.sql(\"\"\"select item_id, genre\n",
    "from (select item_id, split(genres, ',') as genres_arr from items)\n",
    "lateral view outer explode(genres_arr) as genre\"\"\").groupBy(\"item_id\")\\\n",
    "    .pivot(\"genre\").count().na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemsGenresDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsGenresDf.createOrReplaceTempView(\"items_genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boughtGenresCols = ', '.join([\"nvl(`bought_\" + genreName + \"`, 0) as `bought_\" + genreName + \"`\" \n",
    "                              for genreName in itemsGenresDf.columns \\\n",
    "     if genreName not in ['item_id']])\n",
    "\n",
    "boughtGenresAggrCols = ', '.join([\"sum(items_genres.`\" + genreName + \"`) as `bought_\" + genreName + \"`\" \n",
    "                            for genreName in itemsGenresDf.columns \\\n",
    "     if genreName not in ['item_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genresBoughtByUserDf = spark.sql(\"select full_train.user_id, \" + boughtGenresAggrCols + \n",
    "    \"\"\" from full_train join items_genres on full_train.item_id = items_genres.item_id where purchase = 1 \n",
    "    group by user_id\"\"\")\n",
    "genresBoughtByUserDf.createOrReplaceTempView(\"genres_bought_by_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemsBought = spark.sql(\"\"\"select user_id, count(*) as user_bought_cnt from full_train where purchase = 1\n",
    "    group by user_id\"\"\")\n",
    "userItemsBought.createOrReplaceTempView(\"user_items_bought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsBoughtCnt = spark.sql(\"\"\"select item_id, count(*) as items_bought_cnt from full_train where purchase = 1\n",
    "    group by item_id\"\"\")\n",
    "itemsBoughtCnt.createOrReplaceTempView(\"items_bought_cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewGenresCols = ', '.join([\"nvl(`view_\" + genreName + \"`, 0) as `view_\" + genreName + \"`\" for genreName in itemsGenresDf.columns \\\n",
    "     if genreName not in ['item_id']])\n",
    "\n",
    "viewGenresAggrCols = ', '.join([\"sum(items_genres.`\" + genreName + \"`) as `view_\" + genreName + \"`\" for genreName in itemsGenresDf.columns \\\n",
    "     if genreName not in ['item_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genresViewedByUserDf = spark.sql(\"select views_of_items.user_id, \" + viewGenresAggrCols + \n",
    "    \" from views_of_items join items_genres on views_of_items.item_id = items_genres.item_id group by user_id\")\n",
    "genresViewedByUserDf.createOrReplaceTempView(\"genres_viewed_by_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSql(inputTableName, withLabelColumn=True):\n",
    "    labelColumnExpr = \"\"\n",
    "    if withLabelColumn:\n",
    "        labelColumnExpr = \",full_train.purchase as label\"\n",
    "    return \"\"\"select {0}.user_id,  \n",
    "            nvl(items_bought_cnt, 0) as items_bought_cnt,\n",
    "            nvl(user_bought_cnt, 0) as user_bought_cnt,\n",
    "            items_genres.* ,\n",
    "            {2},\n",
    "            {3}\n",
    "            {1}\n",
    "            from {0} \n",
    "                join items_genres on {0}.item_id = items_genres.item_id\n",
    "                left join items_bought_cnt on {0}.item_id = items_bought_cnt.item_id\n",
    "                left join user_items_bought on {0}.item_id = user_items_bought.user_id\n",
    "                left join genres_viewed_by_user on {0}.user_id = genres_viewed_by_user.user_id\n",
    "                left join genres_bought_by_user on {0}.user_id = genres_bought_by_user.user_id\"\"\"\\\n",
    "            .format(inputTableName, labelColumnExpr, viewGenresCols, boughtGenresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTrainWithFeaturesDf = spark.sql(getFeatureSql(\"full_train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5032624\n",
    "# fullTrainWithFeaturesDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fullTrainWithFeaturesDf.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=41)\n",
    "test = fullTrainWithFeaturesDf.join(train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [column for column in fullTrainWithFeaturesDf.columns if column not in [\"label\", \"user_id\", \"item_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "    .setInputCols(features_list)\\\n",
    "    .setOutputCol(\"features\")\n",
    "scaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"norm_features\")\n",
    "layers = [len(features_list), 64, 2]\n",
    "# layers = [len(features_list), 22, 2]\n",
    "# layers = [len(features_list), 20, 5, 2]\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"label\", \\\n",
    "                                     featuresCol=\"norm_features\", \\\n",
    "                                     layers=layers, \\\n",
    "                                         seed=41)\n",
    "pipelineStages = [vectorAssembler, scaler, mlp]\n",
    "pipeline = Pipeline().setStages(pipelineStages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    "    .addGrid(mlp.maxIter, [100])\\\n",
    "    .addGrid(mlp.blockSize, [128])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "    .setMetricName(\"areaUnderROC\")\\\n",
    "    .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "trainValidationSplit = TrainValidationSplit()\\\n",
    "    .setTrainRatio(0.999)\\\n",
    "    .setEstimatorParamMaps(params)\\\n",
    "    .setEstimator(pipeline)\\\n",
    "    .setEvaluator(evaluator)\n",
    "\n",
    "fittedTrainValidationSplit = trainValidationSplit.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661337195768572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(fittedTrainValidationSplit.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknownTestWithFeaturesDf = spark.sql(getFeatureSql(\"unknown_test\", withLabelColumn=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionPredictions = fittedTrainValidationSplit.transform(unknownTestWithFeaturesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.storagelevel import StorageLevel\n",
    "# submissionPredictions.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissionPredictions.select(\"user_id\", \"item_id\", \"rawPrediction\", \"probability\", \"prediction\").show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "vectorSecondElementUdf = F.udf(lambda v:float(v[1]),DoubleType())\n",
    "\n",
    "resultDf = submissionPredictions.select(\"user_id\", \"item_id\", \"probability\")\\\n",
    "    .withColumn(\"purchase\", vectorSecondElementUdf(F.col(\"probability\")))\\\n",
    "    .select((F.row_number().over(Window.orderBy(F.col(\"user_id\"), F.col(\"item_id\"))) -1 ).alias(\"idx\"),\n",
    "            \"user_id\", \"item_id\", \"purchase\") \\\n",
    "    .orderBy(F.asc(\"user_id\"), F.asc(\"item_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDf.repartition(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"lab03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 nikita.mospan nikita.mospan          0 2021-03-17 10:36 lab03/_SUCCESS\r\n",
      "-rw-r--r--   3 nikita.mospan nikita.mospan   89958090 2021-03-17 10:36 lab03/part-00000-997f6cce-350f-4e1d-a02a-a67b26d2ff92-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls lab03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -get lab03/part-00000-997f6cce-350f-4e1d-a02a-a67b26d2ff92-c000.csv lab03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
