{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Denis.Ivashchenko_lab04\")\n",
    "conf.set('spark.executor.instances', '3')\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType, FloatType, BinaryType, TimestampType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"gender\", StringType(),True),\n",
    "    StructField(\"age\", StringType(),True),\n",
    "    StructField(\"uid\", StringType(),True),\n",
    "    StructField(\"user_json\", StringType(),True) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/labs/slaba04/gender_age_dataset.txt\", schema=schema, sep='\\t', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIpeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer,VectorAssembler,VectorIndexer,StringIndexer,StringIndexerModel,HashingTF,PCA\n",
    "from pyspark.ml.linalg import Vectors, DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml import Model,Pipeline\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import Evaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DagTransformer(Transformer):\n",
    "    \"\"\"Column transformer.\n",
    "    \n",
    "    Given 2 Columns  'Age' , \"Gender\" adds an indexed Column 'Dag' which codes Age + Gender\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dag = {}\n",
    "        self.uid = '142345'\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.filter(f.col('age') != '-')\n",
    "        lages = dataset.select('age').filter(f.col('age') != '-').distinct().collect()\n",
    "        self.dag = dict(zip(([(lambda x,y: (x.age,y))(i,k) for i in lages for k in ['F','M']]),range(10)))\n",
    "        dataset = dataset\\\n",
    "            .withColumn('ag',f.udf(lambda x,y: self.dag[(x,y)], IntegerType())(f.col('age'),f.col('gender')))\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.filter(f.col('age') != '-')\n",
    "lages = df.select('age').filter(f.col('age') != '-').distinct().collect()\n",
    "dag = dict(zip(([(lambda x,y: (x.age,y))(i,k) for i in lages for k in ['F','M']]),range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisitTransformer(Transformer):\n",
    "    \"\"\"Transforms column with visit logs \n",
    "    \n",
    "    Parses each row of the column and results with column of array of url's\n",
    "    \n",
    "    Adds column visits.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn('visits',sub_url(f.col('user_json')))\n",
    "        dataset = dataset.withColumn('visits',vis_uni(vis_net(f.col('visits'))))\n",
    "        #dataset = dataset.withColumn('visits',f.explode('visits')).drop('user_json')\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "regexp = re.compile(r'(?<=url\": \")[^\"]+')\n",
    "\n",
    "@f.udf(ArrayType(StringType()))\n",
    "def sub_url(a):\n",
    "    return regexp.findall(a)\n",
    "\n",
    "@f.udf(ArrayType(StringType()))\n",
    "def vis_net(a): \n",
    "    return [(lambda x: urllib.parse.urlparse(x).netloc)(i) for i in a]\n",
    "\n",
    "\n",
    "@f.udf(ArrayType(StringType()))\n",
    "def vis_uni(a):\n",
    "    return list(dict(zip(a,range(len(a)))).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.randomSplit([1.0,4.0],123)\n",
    "df_test = dfs[0]\n",
    "df_train = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DagTransformer().transform(df_train)\n",
    "df = VisitTransformer().transform(df)\n",
    "cv = CountVectorizer(minTF=1, minDF=20, vocabSize=5000, inputCol='visits',outputCol='features')\n",
    "cv_model = cv.fit(df)\n",
    "df = cv_model.transform(df)\n",
    "pca = PCA(k=15, inputCol='features',outputCol='pca_features')\n",
    "pca_model = pca.fit(df)\n",
    "df = pca_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol='pca_features', labelCol='ag')\n",
    "dt_model = dt.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative Model\n",
    "#lr = LogisticRegression(labelCol='ag',featuresCol='pca_features', family='multinomial', maxIter=15, regParam=0.005, elasticNetParam=0.05)\n",
    "#lr_model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', predictionCol='prediction', labelCol='ag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = DagTransformer().transform(df_test)\n",
    "df_test = VisitTransformer().transform(df_test)\n",
    "df_test = cv_model.transform(df_test)\n",
    "df_test = pca_model.transform(df_test)\n",
    "df_test = dt_model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26106934001670845"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_test = StructType([ StructField(\"key\", StringType(), True),\n",
    "                           StructField(\"value\", StringType(), True),\n",
    "                           StructField(\"topic\", StringType(), True),\n",
    "                           StructField(\"partition\", IntegerType(), True),\n",
    "                           StructField(\"offset\", IntegerType(), True),\n",
    "                           StructField(\"timestamp\", TimestampType(), True),\n",
    "                           StructField(\"timetype\", IntegerType(), True)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_denis.ivashchenko\",\n",
    "    \"failOnDataLoss\": 'False',\n",
    "    #\"startingOffsets\": \"\"\"{\"input_denis.ivashchenko\":{\"0\": 50000}}\"\"\"\n",
    "    #\"checkpointLocation\": \"streaming/chk/chk_kafka\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "inputDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "inp = inputDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\",\"topic\",\"partition\",\"offset\",\"timestamp\",\"timestampType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_struct = StructType([\n",
    "    StructField('uid', StringType()),\n",
    "    StructField('visits', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_batch( df , epoch_id):\n",
    "    sdf = df.withColumn('value', f.from_json(f.col('value'), json_struct, { 'multiLine': 'True', 'allowBackslash-EscapingAnyCharacter' : 'True'}))\n",
    "    sdf = sdf.select(f.col('value'))\\\n",
    "        .withColumn('uids',f.udf(lambda x: x[0])(f.col('value')))\\\n",
    "        .withColumn('user_json',f.udf(lambda x: x[1])(f.col('value')))\\\n",
    "        .drop('value')   \n",
    "    sdf = VisitTransformer().transform(sdf)\n",
    "    sdf = cv_model.transform(sdf)\n",
    "    sdf = pca_model.transform(sdf)\n",
    "    sdf = dt_model.transform(sdf)\n",
    "    sdf = sdf.withColumn('gender', f.udf(lambda x: list(dag.keys())[x][1])(f.col('prediction').cast('int')))\\\n",
    "            .withColumn('age', f.udf(lambda x: list(dag.keys())[x][0])(f.col('prediction').cast('int')))\n",
    "    sdf = sdf.withColumnRenamed(existing='uids', new='uid')\\\n",
    "                .select('uid', 'gender' , 'age')\n",
    "    sdf = sdf.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "    sdf.write.format('kafka').options(**write_kafka_params).save()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf = spark.read.csv(path=\"./lab04/part-00000-590f11f5-c6ee-466f-b629-0ddc62b78359-c000.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"denis.ivashchenko\"\n",
    "}\n",
    "output_f = (\n",
    "  inp\n",
    "    .writeStream\n",
    "    .foreachBatch(proc_batch)\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_f.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_f.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_f.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
