{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Denis.Ivashchenko_lab03\")\n",
    "conf.set('spark.executor.instances', '16')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType, FloatType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_it = StructType([\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", DoubleType()),\n",
    "    StructField(\"avail_start\", StringType()),\n",
    "    StructField(\"avail_stop\", StringType()),\n",
    "    StructField(\"show_start\", StringType()),\n",
    "    StructField(\"show_stop\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", StringType()),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", IntegerType())\n",
    "])\n",
    "\n",
    "schema_view = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"ts_start\", IntegerType()),\n",
    "    StructField(\"ts_stop\", IntegerType()),\n",
    "    StructField(\"item_type\", StringType())\n",
    "])\n",
    "\n",
    "schema_set = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка  items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'[\\w ]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import CountVectorizer,ElementwiseProduct\n",
    "from pyspark.ml.feature import OneHotEncoder,StringIndexer, RegexTokenizer, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(VectorUDT())\n",
    "def vec_to_arr(a):\n",
    "    return Vectors.dense(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.csv(\"/labs/slaba03/laba03_items.csv\", schema=schema_it, sep='\\t',header=True)\n",
    "views = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", schema=schema_view, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.drop('year','title','region','avail_stop','avail_start','region_id')\n",
    "items = items.withColumn('stop', f.regexp_replace(f.col('show_stop'), r'[-ZT:]','-')).\\\n",
    "                    withColumn('start', f.regexp_replace(f.col('show_start'), r'[-ZT:]','-')).\\\n",
    "                    withColumn('stop_ux', f.unix_timestamp(f.col('stop'), format='yyyy-MM-dd-HH-mm-ss-')).\\\n",
    "                    withColumn('start_ux', f.unix_timestamp(f.col('start'), format='yyyy-MM-dd-HH-mm-ss-')).\\\n",
    "                    na.fill({'genres': 'General', 'channel_id': '0', 'start_ux': '1485624600', 'stop_ux':'1485627935'}).\\\n",
    "                    withColumn('duration', f.col('stop_ux') - f.col('start_ux')).\\\n",
    "                    withColumn('genres', f.udf(lambda x: regex.findall(x.lower()),ArrayType(StringType()))('genres')).\\\n",
    "                    drop('show_start','channel_id','show_stop','start' , 'stop')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"genres\", outputCol=\"gen_vec\")\n",
    "\n",
    "model = cv.fit(items)\n",
    "\n",
    "items = model.transform(items)\n",
    "items = items.filter(f.col('type') == 1 ).drop('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.withColumn('gen_arr', vec_to_arr(f.col('gen_vec')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обрабтка views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_user = Window.partitionBy('user_id')\n",
    "views = views.withColumn('duration', (f.col('ts_stop') - f.col('ts_start'))/60/60/24).\\\n",
    "                withColumn('days_on_service', (f.max(f.col('ts_stop')).over(win_user) - f.min(f.col('ts_start')).over(win_user))/60/60/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_user_data = views.groupBy('user_id').agg(f.sum('duration').alias('total_time'),f.avg('days_on_service').alias('days_on_service'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", schema=schema_set, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = train.withColumnRenamed(existing='item_id',new='item_id1').\\\n",
    "    groupBy('item_id1').agg(f.count('purchase').alias('views'),f.sum('purchase').alias('buys')).sort(f.col('buys').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = item_stats.join(items, item_stats.item_id1 == items.item_id, 'inner').drop('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat = item_stats.withColumnRenamed(existing='gen_vec',new='gen_vec1').\\\n",
    "            groupBy('gen_vec1').agg(f.sum('buys').alias('buys_c'), f.sum('views').alias('views_c')).sort(f.col('buys_c').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = item_stats.join(item_cat, item_stats.gen_vec == item_cat.gen_vec1, 'inner').drop('stop_ux','start_ux','type','duration','gen_vec1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_stats = item_stats.withColumn('ibr', f.col('buys') / f.col('views')).\\\n",
    "                        withColumn('bic', f.col('buys') / (f.col('buys_c')+1)).\\\n",
    "                        withColumn('vic', f.col('views') / f.col('views_c'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Items:\n",
    "\n",
    "ibr - items buy ratio\n",
    "bic - buys in category ratio\n",
    "vic - views in category ratio\n",
    "\n",
    "\n",
    "Users : \n",
    "ubr - buys / views\n",
    "vpd - views / (day+1)\n",
    "bpd - buys / (day+1)\n",
    "tpv - time / view\n",
    "bpt - buys / total time\n",
    "tpd -  time / (day+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = train.withColumnRenamed(existing='user_id',new='user_id1').\\\n",
    "    groupBy('user_id1').agg(f.count('purchase').alias('views'),f.sum('purchase').alias('buys')).sort(f.col('buys').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = user_stats.withColumnRenamed(existing='user_id', new='user_id1').\\\n",
    "            join(views_user_data, views_user_data.user_id == user_stats.user_id1, 'inner').drop('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = user_stats.withColumn('ubr', f.col('buys') / (f.col('views')))\\\n",
    "            .withColumn('vpd', f.col('views') / (f.col('days_on_Service') + 1))\\\n",
    "            .withColumn('bpd', f.col('buys') / (f.col('days_on_Service') + 1))\\\n",
    "            .withColumn('tpv', f.col('total_time') / f.col('views'))\\\n",
    "            .withColumn('bpt', f.col('buys') / f.col('total_time'))\\\n",
    "            .withColumn('tpd', f.col('total_time') / (f.col('days_on_Service') + 1))\\\n",
    "            .drop('total_time','days_on_service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(item_stats, item_stats.item_id1 == train.item_id, 'inner').\\\n",
    "    select(train.user_id, train.item_id, train.purchase, item_stats.ibr, item_stats.bic,  item_stats.vic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = train.join(user_stats, user_stats.user_id1 == train.user_id, 'inner').\\\n",
    "    select(train.user_id, train.item_id, train.purchase, train.ibr, train.bic, train.vic, user_stats.ubr ,user_stats.vpd, user_stats.bpd, user_stats.tpv ,user_stats.bpt, user_stats.tpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m = train_res.sampleBy(\"purchase\", fractions={0: 0.8, 1: 0.8}, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = train_res.join(train_m, on=[\"user_id\",'item_id'], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=['ibr','ubr'], outputCol=\"features\")\n",
    "#vecAssembler = VectorAssembler(inputCols=['ibr','bic','vic','ubr','vpd','bpd','tpv','bpt','tpd'], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m = vecAssembler.transform(train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.9269945352919823\n",
    "lr = LogisticRegression(featuresCol='features',\\\n",
    "                        labelCol=\"purchase\",\\\n",
    "                        tol=1e-5,\\\n",
    "                        maxIter=15,\\\n",
    "                        regParam=0.025, \\\n",
    "                        weightCol='ibr',\\\n",
    "                        elasticNetParam=0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_89f73a17efb4, numClasses = 2, numFeatures = 2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = vecAssembler.transform(test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9269536789885084"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model.write().overwrite().save(os.path.curdir + '/lr1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делаем предсказание "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(item_stats, item_stats.item_id1 == test.item_id, 'inner').\\\n",
    "    select(test.user_id, test.item_id, item_stats.ibr, item_stats.bic, item_stats.vic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(user_stats, user_stats.user_id1 == test.user_id, 'left').\\\n",
    "    select(test.user_id, test.item_id, test.ibr, test.bic, test.vic, user_stats.ubr ,user_stats.vpd, user_stats.bpd, user_stats.tpv ,user_stats.bpt, user_stats.tpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibr_m , bic_m, vic_m, ubr_m, vpd_m, bpd_m, tpv_m, bpt_m, tpd_m = stats.rdd.collect()[3][3:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.na.fill({'ibr': ibr_m, 'bic': bic_m, 'vic': vic_m, 'ubr': ubr_m, 'vpd': vpd_m, 'bpd' : bpd_m, 'tpv': tpv_m, 'bpt': bpt_m, 'tpd': tpd_m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vecAssembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_res = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf()\n",
    "def vec_to_arr(a):\n",
    "    return (lambda x : float(Vectors.dense(x)[1]))(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pred_res.withColumn('proba_pos', vec_to_arr(f.col('probability'))).\\\n",
    "    select(f.col('user_id').cast('int'),f.col('item_id').cast('int'),f.col('proba_pos').cast('float').alias('purchase')).sort(f.col('user_id').asc(), f.col('item_id').asc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраняем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf = out.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf.to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2156840, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
