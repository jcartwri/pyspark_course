{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "#exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"karavaev_andrei_spark_lab_2\") \n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_COURSES = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "              [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "              [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "              [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "              [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "              [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_data = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words_desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData = tokenizer.transform(course_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words_desc\", outputCol=\"rawFeatures\", numFeatures=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizedData = hashingTF.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm\")\n",
    "data = normalizer.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_id = [str(i[0]) for i in MY_COURSES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для расчета скалярного произведения векторов (т.к. вектора нормализованы - равняется косинусной мере схожетси)\n",
    "dot_udf = f.udf(lambda x,y: float(x.dot(y)), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждого курса из личного списка считаем косинусную меру с помощью dot_udf с каждым курсом\n",
    "\n",
    "for selected_id in courses_id:\n",
    "    selected_vector = data.filter(f.col('id') == selected_id).select(f.col('norm').alias('target_norm'))\n",
    "\n",
    "    joined_df = data.filter(f.col('id') != selected_id) \\\n",
    "                    .select('id', 'norm') \\\n",
    "                    .crossJoin(selected_vector)\n",
    "\n",
    "    dot_df = joined_df.select(dot_udf('norm', 'target_norm').alias(\"dot_norm\"),\n",
    "                 'id')\n",
    "\n",
    "    most_similar_id_list = dot_df.sort(f.desc('dot_norm')).select('id').take(10)\n",
    "    \n",
    "    most_similar_id_list = [i.id for i in most_similar_id_list]\n",
    "    \n",
    "    submit_dict[selected_id] = most_similar_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'23126': [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909],\n",
       " '21617': [21609,\n",
       "  21608,\n",
       "  21616,\n",
       "  21492,\n",
       "  21624,\n",
       "  21623,\n",
       "  21630,\n",
       "  21628,\n",
       "  21508,\n",
       "  21857],\n",
       " '16627': [11431, 12247, 13021, 25010, 11575, 5687, 9598, 5372, 12863, 16769],\n",
       " '11556': [16488, 13461, 468, 10447, 387, 22710, 9289, 5936, 23357, 7833],\n",
       " '16704': [3864, 23407, 1365, 20645, 20105, 1426, 8217, 1236, 1164, 23864],\n",
       " '13702': [864, 1216, 7173, 8313, 1052, 17017, 19613, 21017, 17015, 8082]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/home/andrey.karavaev/lab02.json\", mode='w') as file:\n",
    "    json.dump(submit_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
