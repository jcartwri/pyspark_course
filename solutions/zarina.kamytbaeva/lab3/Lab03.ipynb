{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"ZK ML app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_train = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "train_df = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", schema=schema_train, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " user_id  | 1654  \n",
      " item_id  | 74107 \n",
      " purchase | 0     \n",
      "-RECORD 1---------\n",
      " user_id  | 1654  \n",
      " item_id  | 89249 \n",
      " purchase | 0     \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.repartition(6)\n",
    "train_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_items = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "                                       StructField('channel_id', IntegerType()),\n",
    "                                       StructField('datetime_availability_start', StringType()),\n",
    "                                       StructField('datetime_availability_stop', StringType()),\n",
    "                                       StructField('datetime_show_start', StringType()),\n",
    "                                       StructField('datetime_show_stop', StringType()),\n",
    "                                       StructField('content_type', IntegerType()),\n",
    "                                       StructField('title', StringType(), nullable=True),\n",
    "                                       StructField('year', FloatType(), nullable=True),\n",
    "                                       StructField('genres', StringType()),\n",
    "                                       StructField('region_id', IntegerType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = spark.read \\\n",
    ".format('csv')\\\n",
    ".schema(schema_items)\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"delimiter\", \"\\t\")\\\n",
    ".load('/labs/slaba03/laba03_items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------\n",
      " item_id                     | 65667                                                    \n",
      " channel_id                  | null                                                     \n",
      " datetime_availability_start | 1970-01-01T00:00:00Z                                     \n",
      " datetime_availability_stop  | 2018-01-01T00:00:00Z                                     \n",
      " datetime_show_start         | null                                                     \n",
      " datetime_show_stop          | null                                                     \n",
      " content_type                | 1                                                        \n",
      " title                       | на пробах только девушки (all girl auditions)            \n",
      " year                        | 2013.0                                                   \n",
      " genres                      | Эротика                                                  \n",
      " region_id                   | null                                                     \n",
      "-RECORD 1-------------------------------------------------------------------------------\n",
      " item_id                     | 65669                                                    \n",
      " channel_id                  | null                                                     \n",
      " datetime_availability_start | 1970-01-01T00:00:00Z                                     \n",
      " datetime_availability_stop  | 2018-01-01T00:00:00Z                                     \n",
      " datetime_show_start         | null                                                     \n",
      " datetime_show_stop          | null                                                     \n",
      " content_type                | 1                                                        \n",
      " title                       | скуби ду: эротическая пародия (scooby doo: a xxx parody) \n",
      " year                        | 2011.0                                                   \n",
      " genres                      | Эротика                                                  \n",
      " region_id                   | null                                                     \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_df.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------\n",
      " item_id      | 65667                                                    \n",
      " content_type | 1                                                        \n",
      " title        | на пробах только девушки (all girl auditions)            \n",
      " year         | 2013.0                                                   \n",
      " genres       | Эротика                                                  \n",
      "-RECORD 1----------------------------------------------------------------\n",
      " item_id      | 65669                                                    \n",
      " content_type | 1                                                        \n",
      " title        | скуби ду: эротическая пародия (scooby doo: a xxx parody) \n",
      " year         | 2011.0                                                   \n",
      " genres       | Эротика                                                  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,when,count,sum\n",
    "items_df_filtered = items_df.select(col(\"item_id\"), col(\"content_type\"), col(\"title\"), col(\"year\"), col(\"genres\"))\\\n",
    ".na.fill({'year': -1, 'genres': 'na'})\n",
    "\n",
    "items_df_filtered.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_test = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"purchase\", IntegerType())\n",
    "])\n",
    "\n",
    "test_df = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", schema=schema_test, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " user_id  | 1654  \n",
      " item_id  | 94814 \n",
      " purchase | null  \n",
      "-RECORD 1---------\n",
      " user_id  | 1654  \n",
      " item_id  | 93629 \n",
      " purchase | null  \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "user_mean = train_df.select(['user_id', 'purchase'])\\\n",
    "        .groupBy('user_id')\\\n",
    "        .agg(f.mean('purchase').alias('user_purchase_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " user_id            | 754230               \n",
      " user_purchase_mean | 0.027575641516660282 \n",
      "-RECORD 1----------------------------------\n",
      " user_id            | 833685               \n",
      " user_purchase_mean | 0.007500986971969996 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_mean.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " item_id            | 93486                 \n",
      " item_purchase_mean | 0.0021413276231263384 \n",
      "-RECORD 1-----------------------------------\n",
      " item_id            | 90019                 \n",
      " item_purchase_mean | 0.0022813688212927757 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_mean = train_df.select(['item_id', 'purchase'])\\\n",
    "        .groupBy('item_id')\\\n",
    "        .agg(f.mean('purchase').alias('item_purchase_mean'))\n",
    "item_mean.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " user_id            | 920599                \n",
      " item_id            | 8389                  \n",
      " purchase           | 0                     \n",
      " user_purchase_mean | 0.0015564202334630351 \n",
      " item_purchase_mean | 0.005979073243647235  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_pre = train_df.join(user_mean, train_df.user_id == user_mean.user_id)\\\n",
    ".select(train_df.user_id, train_df.item_id, train_df.purchase, user_mean.user_purchase_mean)\n",
    "train_df_mean = train_df_pre.join(item_mean, train_df_pre.item_id == item_mean.item_id)\\\n",
    ".select(train_df_pre.user_id, train_df_pre.item_id, train_df_pre.purchase, train_df_pre.user_purchase_mean, \\\n",
    "        item_mean.item_purchase_mean)\n",
    "train_df_mean.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " user_id            | 761341               \n",
      " item_id            | 8389                 \n",
      " purchase           | null                 \n",
      " user_purchase_mean | 3.875968992248062E-4 \n",
      " item_purchase_mean | 0.005979073243647235 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_pre = test_df.join(user_mean, test_df.user_id == user_mean.user_id)\\\n",
    ".select(test_df.user_id, test_df.item_id, test_df.purchase, user_mean.user_purchase_mean)\n",
    "test_df_mean = test_df_pre.join(item_mean, test_df_pre.item_id == item_mean.item_id)\\\n",
    ".select(test_df_pre.user_id, test_df_pre.item_id, test_df_pre.purchase, test_df_pre.user_purchase_mean, item_mean.item_purchase_mean)\n",
    "test_df_mean.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------\n",
      " user_id            | 754230                                  \n",
      " item_id            | 8389                                    \n",
      " purchase           | 0                                       \n",
      " user_purchase_mean | 0.027575641516660282                    \n",
      " item_purchase_mean | 0.005979073243647235                    \n",
      " content_type       | 1                                       \n",
      " title              | пес в сапогах (сурдоперевод)            \n",
      " year               | 1981.0                                  \n",
      " genres             | Мультфильмы,Детские,Союзмультфильм,Наши \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_items = train_df_mean.join(items_df_filtered, train_df_mean.item_id == items_df_filtered.item_id, how=\"left\")\\\n",
    ".select(train_df_mean.user_id, train_df_mean.item_id, train_df_mean.purchase, train_df_mean.user_purchase_mean, \\\n",
    "        train_df_mean.item_purchase_mean \\\n",
    "       , items_df_filtered.content_type, items_df_filtered.title, items_df_filtered.year, items_df_filtered.genres)\n",
    "train_df_items.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------\n",
      " user_id            | 814235                                  \n",
      " item_id            | 8389                                    \n",
      " purchase           | null                                    \n",
      " user_purchase_mean | 7.78816199376947E-4                     \n",
      " item_purchase_mean | 0.005979073243647235                    \n",
      " content_type       | 1                                       \n",
      " title              | пес в сапогах (сурдоперевод)            \n",
      " year               | 1981.0                                  \n",
      " genres             | Мультфильмы,Детские,Союзмультфильм,Наши \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_items = test_df_mean.join(items_df_filtered, test_df_mean.item_id == items_df_filtered.item_id, how=\"left\")\\\n",
    ".select(test_df_mean.user_id, test_df_mean.item_id, test_df_mean.purchase, test_df_mean.user_purchase_mean \\\n",
    "        , test_df_mean.item_purchase_mean \\\n",
    "       , items_df_filtered.content_type, items_df_filtered.title, items_df_filtered.year, items_df_filtered.genres)\n",
    "test_df_items.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"genres_array\", outputCol=\"genres_vector\")\n",
    "\n",
    "train_model = cv.fit(train_df_items_v)\n",
    "train_result = train_model.transform(train_df_items_v)\n",
    "train_result.show(1,False,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, VectorAssembler, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"genres\", outputCol=\"genres_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Vectorizer = CountVectorizer(inputCol=\"genres_words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"features\", \"user_purchase_mean\", \"item_purchase_mean\", \"content_type\"], \\\n",
    "                            outputCol=\"features_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=assembler.getOutputCol(), labelCol=\"purchase\", maxIter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline2 = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    Count_Vectorizer,\n",
    "    assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline2.fit(train_df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_fd695dcb68a8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = pipeline_model.transform(test_df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------\n",
      " user_id            | 822709                                                                         \n",
      " item_id            | 8389                                                                           \n",
      " purchase           | null                                                                           \n",
      " user_purchase_mean | 3.7893141341417203E-4                                                          \n",
      " item_purchase_mean | 0.005979073243647235                                                           \n",
      " content_type       | 1                                                                              \n",
      " title              | пес в сапогах (сурдоперевод)                                                   \n",
      " year               | 1981.0                                                                         \n",
      " genres             | Мультфильмы,Детские,Союзмультфильм,Наши                                        \n",
      " genres_words       | [мультфильмы,детские,союзмультфильм,наши]                                      \n",
      " features           | (1035,[6],[1.0])                                                               \n",
      " features_fin       | (1038,[6,1035,1036,1037],[1.0,3.7893141341417203E-4,0.005979073243647235,1.0]) \n",
      " rawPrediction      | [6.429880141647428,-6.429880141647428]                                         \n",
      " probability        | [0.9983899523221372,0.001610047677862651]                                      \n",
      " prediction         | 0.0                                                                            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------\n",
      " user_id     | 846231                                    \n",
      " item_id     | 8389                                      \n",
      " prediction  | 0.0                                       \n",
      " probability | [0.9983060094178553,0.001693990582144607] \n",
      " c1          | 0.9983060094178553                        \n",
      " c2          | 0.001693990582144607                      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "split1_udf = udf(lambda value: value[0].item(), DoubleType())\n",
    "split2_udf = udf(lambda value: value[1].item(), DoubleType())\n",
    "output2 = predictions2.select(\"user_id\",\"item_id\", \"prediction\", \"probability\", split1_udf('probability').alias('c1'), split2_udf('probability').alias('c2'))\n",
    "output2.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------------------+\n",
      "|user_id|item_id|purchase             |\n",
      "+-------+-------+---------------------+\n",
      "|1654   |336    |2.6411577989123003E-4|\n",
      "|1654   |678    |5.756595342452835E-4 |\n",
      "|1654   |691    |0.0010737500270940703|\n",
      "|1654   |696    |0.0011576214649155882|\n",
      "|1654   |763    |0.0014251116907521974|\n",
      "|1654   |795    |0.0054233830496116945|\n",
      "|1654   |861    |7.372708443897341E-4 |\n",
      "|1654   |1137   |0.0018439466283109627|\n",
      "|1654   |1159   |0.0015648764454790005|\n",
      "|1654   |1428   |0.0010289588725163864|\n",
      "+-------+-------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab_df = output2.select(\"user_id\",\"item_id\",col(\"c2\").alias('purchase'))\\\n",
    ".orderBy('user_id','item_id')\n",
    "lab_df.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_df.toPandas().to_csv('lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
