{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Zakharov YA\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Zakharov YA\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.load(\"/labs/slaba04/gender_age_dataset.txt\",\n",
    "                        format = \"csv\",\n",
    "                        sep=\"\\t\",\n",
    "                        header=True,\n",
    "                        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|\n",
      "+------+-----+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|\n",
      "+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "@f.udf(returnType=ArrayType(StringType()))\n",
    "def find_url(dict_data):\n",
    "    data_dict = eval(dict_data)[\"visits\"]\n",
    "    try: \n",
    "        list_data = []\n",
    "        for i in data_dict: \n",
    "            list_data.extend(re.findall(r'\\w+',re.sub(r'(http://|https://|www)', '', i[\"url\"])))\n",
    "        return list_data\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSitesTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CleanSitesTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(self.getOutputCol(), find_url(f.col(self.getInputCol())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer1 = CleanSitesTransformer(inputCol=\"user_json\", outputCol=\"user_json_visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transformer1.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|gender|  age|                 uid|           user_json|    user_json_visits|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|[zebra, zoya, ru,...|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|[sweetrading, ru,...|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|[ru, oriflame, co...|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|[translate, tatto...|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|[mail, rambler, r...|\n",
      "|     F|25-34|d5090ddf-5648-487...|{\"visits\": [{\"url...|[cfire, mail, ru,...|\n",
      "|     F|25-34|d50bcef8-16ff-4e8...|{\"visits\": [{\"url...|[msn, com, ru, ru...|\n",
      "|     F|18-24|d50e23dc-0cbd-488...|{\"visits\": [{\"url...|[gazprom, ru, pre...|\n",
      "|     F|45-54|d50fdabb-4208-441...|{\"visits\": [{\"url...|[lifenews, ru, li...|\n",
      "|     F|18-24|d511b480-23a6-482...|{\"visits\": [{\"url...|[google, ru, film...|\n",
      "+------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"gender_map\", f.when(f.col(\"gender\")=='F', 1).otherwise(0))\n",
    "df = df.withColumn(\"age_map\", f.when(f.col(\"age\")=='18-24', 0).when(f.col(\"age\")=='25-34', 1).when(f.col(\"age\")=='35-44', 2).when(f.col(\"age\")=='45-54', 3).otherwise(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler,StandardScaler\n",
    "\n",
    "# en_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "# remover = StopWordsRemover(inputCol=\"user_json_visits\",\n",
    "#                            outputCol=\"filtered\",\n",
    "#                            stopWords=en_stopwords)\n",
    "\n",
    "hasher = HashingTF(numFeatures=2500, binary=True, inputCol=\"user_json_visits\", outputCol=\"word_vector\")\n",
    "\n",
    "# scaler = StandardScaler()\\\n",
    "#          .setInputCol(\"word_vector\")\\\n",
    "#          .setOutputCol(\"sc_word_vector\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"word_vector\", rawPredictionCol='rawPrediction_gender', predictionCol='prediction_gender', labelCol=\"gender_map\", maxIter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    hasher, lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sampleBy(\"gender_map\", fractions={0: 0.75, 1: 0.75}, seed=42)\n",
    "test = df.join(train, on=\"uid\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gender = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.sampleBy(\"age_map\", fractions={0: 0.75, 1: 0.75, 2:0.75, 3:0.75, 4:0.75}, seed=42)\n",
    "test = df.join(train, on=\"uid\", how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_age = LogisticRegression(featuresCol=\"word_vector\", rawPredictionCol='rawPrediction_age', predictionCol='prediction_age', labelCol=\"age_map\", maxIter=30, regParam=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    hasher, lg_age\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_age = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAFKA_BOOTSTRAP_SERVER = 'spark-node-1.newprolab.com:6667'\n",
    "KAFKA_BOOTSTRAP_SERVER = 'spark-de-node-1.newprolab.com:6667'\n",
    "INPUT_KAFKA_TOPIC = 'input_yakov.zakharov'\n",
    "OUTPUT_KAFKA_TOPIC = 'yakov.zakharov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_kafka_params = {\n",
    "    'kafka.bootstrap.servers': KAFKA_BOOTSTRAP_SERVER,\n",
    "    'subscribe': INPUT_KAFKA_TOPIC,\n",
    "    'startingOffsets': 'latest'\n",
    "}\n",
    "kafka_sdf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('kafka')\n",
    "    .options(**read_kafka_params)\n",
    "    .load()\n",
    ")\n",
    "kafka_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sdf = kafka_sdf.select(F.col(\"value\").cast(\"string\"), F.col(\"topic\"), F.col(\"partition\"), F.col(\"offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def extract_dictionary_from_kafca(dict_data):\n",
    "    data_dict = eval(dict_data)[\"visits\"]\n",
    "    uid_dict = eval(dict_data)[\"uid\"]\n",
    "    list_data = []\n",
    "    for i in eval(data_dict): \n",
    "        list_data.extend(re.findall(r'\\w+',re.sub(r'(http://|https://|www)', '', i[\"url\"])))    \n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "@udf(returnType=StringType())\n",
    "def extract_uid_from_kafca(dict_data):\n",
    "    uid_dict = eval(dict_data)[\"uid\"]\n",
    "    return uid_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ReadTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.select(self.getInputCol(), \n",
    "                                  extract_dictionary_from_kafca(F.col(self.getInputCol())).alias(self.getOutputCol().split(\",\")[1]),\n",
    "                                  extract_uid_from_kafca(F.col(self.getInputCol())).alias(self.getOutputCol().split(\",\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    #parsed_sdf = df.select(F.col(\"value\").cast(\"string\"), F.col(\"topic\"), F.col(\"partition\"), F.col(\"offset\"))\n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": KAFKA_BOOTSTRAP_SERVER,\n",
    "   \"topic\": \"yakov.zakharov\", \n",
    "    \"truncate\": \"false\", \n",
    "    \"numRows\": \"1000\"    \n",
    "    }\n",
    "    read_transformer = ReadTransformer(inputCol=\"value\", outputCol=\"uid,user_json_visits\")\n",
    "    predictions_ = read_transformer.transform(parsed_sdf)\n",
    "    pred_topic = pipeline_age.transform(predictions_)\n",
    "    pred_topic = pred_topic.withColumn(\"age\", f.when(f.col(\"prediction_age\")==0, '18-24').when(f.col(\"prediction_age\")==1, '25-34').when(f.col(\"prediction_age\")==2, '35-44').when(f.col(\"prediction_age\")==3, '45-54').otherwise(\">=55\")).select([\"uid\", \"age\", \"user_json_visits\"])\n",
    "    pred_topic = pipeline_gender.transform(pred_topic)\n",
    "    pred_topic = pred_topic.withColumn(\"gender\", f.when(f.col(\"prediction_gender\")==1, 'F').otherwise('M'))\n",
    "    pred_topic = pred_topic.select([\"uid\", \"gender\", \"age\"])\n",
    "    pred_topic = pred_topic.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "    return pred_topic \\\n",
    "        .writeStream.format(\"kafka\").options(**write_kafka_params) \\\n",
    "        .option(\"checkpointLocation\", \"streaming/chk/chk_yakov.zakharov\")\\\n",
    "        .outputMode(\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/25 20:10:57 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/yakov.zakharov/streaming/chk' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/yakov.zakharov/.Trash/Current/user/yakov.zakharov/streaming/chk1616692257762\n",
      "rm: `/user/yakov.zakharov/streaming/datasets': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/yakov.zakharov/streaming/chk\n",
    "!hdfs dfs -rm -r /user/yakov.zakharov/streaming/datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/yakov.zakharov/streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = create_console_sink(parsed_sdf)\n",
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '496f5afe-117a-421a-9a5e-cfb88815170a',\n",
       " 'runId': 'ecb5257d-3f3d-4a3b-9a2e-210d0e199e1e',\n",
       " 'name': None,\n",
       " 'timestamp': '2021-03-25T17:12:43.391Z',\n",
       " 'batchId': 2,\n",
       " 'numInputRows': 294,\n",
       " 'inputRowsPerSecond': 94.47300771208226,\n",
       " 'processedRowsPerSecond': 38.082901554404145,\n",
       " 'durationMs': {'addBatch': 7468,\n",
       "  'getBatch': 0,\n",
       "  'getEndOffset': 0,\n",
       "  'queryPlanning': 78,\n",
       "  'setOffsetRange': 2,\n",
       "  'triggerExecution': 7720,\n",
       "  'walCommit': 74},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[input_yakov.zakharov]]',\n",
       "   'startOffset': {'input_yakov.zakharov': {'0': 45001}},\n",
       "   'endOffset': {'input_yakov.zakharov': {'0': 45295}},\n",
       "   'numInputRows': 294,\n",
       "   'inputRowsPerSecond': 94.47300771208226,\n",
       "   'processedRowsPerSecond': 38.082901554404145}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider@41a05500'}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    for s in streams:\n",
    "        desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "        s.stop()\n",
    "        print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[input_yakov.zakharov]]\n"
     ]
    }
   ],
   "source": [
    "kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
