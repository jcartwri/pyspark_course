{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rkable medical revolution going on today. Students will complete this course in 12 weeks. Each week features a series of lecture sequences that are supported by interactive video tutorials and interspersed exercises or problems.&nbsp;Students will work on a homework assignment or exam each week. The course will conclude with a comprehensive final exam. 7.00x \\u2013 The Secret of Life will let you explore the mysteries of biochemistry, genetics, molecular biology, recombinant DNA technology and genomics, and rational medicine. We are excited to take this journey with you! Before your course starts, try the new edX Demo where you can explore the fun, interactive learning environment and virtual labs.&nbsp;Learn more. FAQ Can I still choose an honor code certificate at no cost? Yes, if you want an Honor Code certificate after successfully completing the course and do not need or want the verified certificate, you can select that option by clicking \\u201cWhy do I have to pay?\\u201d under the verified amounts. \"}\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /labs/slaba02/DO_record_per_line.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"laba2 app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"laba2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>laba2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdd6406ff28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# курс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = [23126, 21617, 16627, 11556, 16704, 13702]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не плохое дополнение!\n",
    "\n",
    "# stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "# swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)\n",
    "\n",
    "# preprocessing = Pipeline(stages=[\n",
    "#     tokenizer,\n",
    "#     swr,\n",
    "#     count_vectorizer\n",
    "# ])\n",
    "\n",
    "# preprocessing_model = preprocessing.fit(new_df)\n",
    "# preprocessed_dataset = preprocessing_model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "23126\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "|13782| 0.4720272349196929|\n",
      "|13665|0.44990966856587233|\n",
      "|24419|0.42697047829852686|\n",
      "|20638| 0.4106274562765083|\n",
      "| 2724| 0.3797487681859415|\n",
      "|25782| 0.2837238538640951|\n",
      "| 2633|0.26715960070589767|\n",
      "| 2723| 0.2657748793766494|\n",
      "|13348| 0.2504132200626535|\n",
      "|15909|0.24576952252068635|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "#####################\n",
      "21617\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "|21608| 0.4684746966615021|\n",
      "|21616| 0.4645572985597666|\n",
      "|21492|0.37345437787709834|\n",
      "|21624| 0.3191321071809383|\n",
      "|21623|0.31407486478991886|\n",
      "|21630|0.31390543251576386|\n",
      "|21628|0.31380989052004515|\n",
      "|21508| 0.3058525152715214|\n",
      "|21857| 0.3046877707105122|\n",
      "|21506|0.30304327691152194|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "#####################\n",
      "16627\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "|11431|0.42094501636697473|\n",
      "|12247|0.26345336029922906|\n",
      "| 5687|0.24793186044437293|\n",
      "|17964|0.22395896187856823|\n",
      "|12660| 0.2208602339545875|\n",
      "|16694|0.20475244430773296|\n",
      "|17961|0.19862079924429346|\n",
      "| 9563|0.19058691470715672|\n",
      "| 5558|0.18190004111112296|\n",
      "| 9598| 0.1721752637261746|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "#####################\n",
      "11556\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "|16488|0.28275359544184764|\n",
      "|10384|0.23181934454299152|\n",
      "|22710|0.21539887261083765|\n",
      "|  468|0.20328103884989426|\n",
      "|10447|0.18930909406456728|\n",
      "|13461| 0.1888044900066971|\n",
      "|19330|0.15979245047150342|\n",
      "|23357| 0.1468494314239205|\n",
      "|12679| 0.1394525192644919|\n",
      "|21707|0.13432627630732785|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "#####################\n",
      "16704\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "| 1228|0.13259561056914945|\n",
      "| 1219|0.13203145826676593|\n",
      "|20362|0.11165396465657111|\n",
      "| 1327|0.11165396465657111|\n",
      "| 1365|0.09625618509063799|\n",
      "|   55| 0.0872374754189695|\n",
      "|26980|0.08546174616238521|\n",
      "|18331|0.08433126218873695|\n",
      "| 1236|0.08378398770773123|\n",
      "| 8186|0.07793107497630021|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "#####################\n",
      "13702\n",
      "+-----+-------------------+\n",
      "|   id|                cos|\n",
      "+-----+-------------------+\n",
      "|21079| 0.1373228794340864|\n",
      "|  792| 0.0978387799389964|\n",
      "| 8123|0.08745487121357656|\n",
      "| 8083|0.08405291794951518|\n",
      "| 1033|0.08206338272126433|\n",
      "| 1041| 0.0787618768464898|\n",
      "|13057| 0.0763826991174937|\n",
      "|19925|0.07457052743201635|\n",
      "| 1217|0.07365366303958584|\n",
      "| 1396|0.07140252268991829|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dict_mas = {}\n",
    "for i in [23126, 21617, 16627, 11556, 16704, 13702]:\n",
    "    lang = df.filter('id == ' + str(i)).select('lang').rdd.flatMap(lambda x: x).collect()[0]\n",
    "    new_df = df.filter(df.lang.like(lang))\n",
    "\n",
    "    # TFIDF\n",
    "    tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "    wordsData = tokenizer.transform(new_df)\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "    featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    # Нормализация\n",
    "    normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "    l2NormData = normalizer.transform(rescaledData)\n",
    "\n",
    "    # добавление колонки с id заданого вектора\n",
    "    v = l2NormData.filter('id == ' + str(i)).select(col('normFeatures').alias(\"id_vector\")).limit(1)\n",
    "    new_df = l2NormData.crossJoin(v)\n",
    "\n",
    "    # cos расстояние\n",
    "    dotProductUDF = udf(lambda v1, v2: float(v1.dot(v2)), DoubleType())\n",
    "    new_df = new_df.withColumn(\"cos\", dotProductUDF(\"id_vector\", \"normFeatures\"))[['id', 'cos']]\n",
    "\n",
    "    new_df = new_df[new_df['id'] != i]\n",
    "    new_df = new_df[new_df['cos'] < 0.99]\n",
    "    new_df = new_df.orderBy(new_df['cos'].desc())\n",
    "    print('#####################')\n",
    "    print(i)\n",
    "    print(new_df.show(10))\n",
    "\n",
    "    mas = new_df.select('id').limit(10).collect()\n",
    "    mas = [j['id'] for j in mas]\n",
    "    dict_mas[str(i)] = mas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'23126': [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909],\n",
       " '21617': [21608,\n",
       "  21616,\n",
       "  21492,\n",
       "  21624,\n",
       "  21623,\n",
       "  21630,\n",
       "  21628,\n",
       "  21508,\n",
       "  21857,\n",
       "  21506],\n",
       " '16627': [11431, 12247, 5687, 17964, 12660, 16694, 17961, 9563, 5558, 9598],\n",
       " '11556': [16488, 10384, 22710, 468, 10447, 13461, 19330, 23357, 12679, 21707],\n",
       " '16704': [1228, 1219, 20362, 1327, 1365, 55, 26980, 18331, 1236, 8186],\n",
       " '13702': [21079, 792, 8123, 8083, 1033, 1041, 13057, 19925, 1217, 1396]}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variant1 = dict_mas.copy()\n",
    "variant1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/home/yakov.zakharov/lab02.json\", \"w\") as write_file:\n",
    "    json.dump(variant1, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
