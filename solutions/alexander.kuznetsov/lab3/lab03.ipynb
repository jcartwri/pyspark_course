{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2021-02-27 22:12 /labs/slaba03/laba03_items.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2021-02-27 22:12 /labs/slaba03/laba03_test.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2021-02-27 22:12 /labs/slaba03/laba03_train.csv\r\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2021-02-27 22:12 /labs/slaba03/laba03_views_programmes.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark context init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"KuznetsovA VideoRecsys app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KuznetsovA VideoRecsys app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f202d3eb550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, ArrayType, FloatType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType(fields=[\n",
    "    StructField(\"item_id\", IntegerType()),\n",
    "    StructField(\"channel_id\", IntegerType()),\n",
    "    StructField(\"datetime_availability_start\", TimestampType()),\n",
    "    StructField(\"datetime_availability_stop\", TimestampType()),\n",
    "    StructField(\"datetime_show_start\", TimestampType()),\n",
    "    StructField(\"datetime_show_stop\", TimestampType()),\n",
    "    StructField(\"content_type\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"year\", IntegerType()),\n",
    "    StructField(\"genres\", StringType()),\n",
    "    StructField(\"region_id\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = (spark\n",
    "         .read\n",
    "#          .schema(items_schema)\n",
    "         .format('csv')\n",
    "         .option(\"sep\", \"\\t\")\n",
    "         .option(\"header\", True)\n",
    "         .option('inferSchema', True)\n",
    "         .load('/labs/slaba03/laba03_items.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- channel_id: double (nullable = true)\n",
      " |-- datetime_availability_start: timestamp (nullable = true)\n",
      " |-- datetime_availability_stop: timestamp (nullable = true)\n",
      " |-- datetime_show_start: timestamp (nullable = true)\n",
      " |-- datetime_show_stop: timestamp (nullable = true)\n",
      " |-- content_type: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- region_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------\n",
      " item_id                     | 65667                                                                                  \n",
      " channel_id                  | null                                                                                   \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                                                    \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                                                    \n",
      " datetime_show_start         | null                                                                                   \n",
      " datetime_show_stop          | null                                                                                   \n",
      " content_type                | 1                                                                                      \n",
      " title                       | на пробах только девушки (all girl auditions)                                          \n",
      " year                        | 2013.0                                                                                 \n",
      " genres                      | Эротика                                                                                \n",
      " region_id                   | null                                                                                   \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------\n",
      " item_id                     | 65669                                                                                  \n",
      " channel_id                  | null                                                                                   \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                                                    \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                                                    \n",
      " datetime_show_start         | null                                                                                   \n",
      " datetime_show_stop          | null                                                                                   \n",
      " content_type                | 1                                                                                      \n",
      " title                       | скуби ду: эротическая пародия (scooby doo: a xxx parody)                               \n",
      " year                        | 2011.0                                                                                 \n",
      " genres                      | Эротика                                                                                \n",
      " region_id                   | null                                                                                   \n",
      "-RECORD 2-------------------------------------------------------------------------------------------------------------\n",
      " item_id                     | 65668                                                                                  \n",
      " channel_id                  | null                                                                                   \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                                                    \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                                                    \n",
      " datetime_show_start         | null                                                                                   \n",
      " datetime_show_stop          | null                                                                                   \n",
      " content_type                | 1                                                                                      \n",
      " title                       | горячие девочки для горячих девочек (hot babes 4 hot babes)                            \n",
      " year                        | 2011.0                                                                                 \n",
      " genres                      | Эротика                                                                                \n",
      " region_id                   | null                                                                                   \n",
      "-RECORD 3-------------------------------------------------------------------------------------------------------------\n",
      " item_id                     | 65671                                                                                  \n",
      " channel_id                  | null                                                                                   \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                                                    \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                                                    \n",
      " datetime_show_start         | null                                                                                   \n",
      " datetime_show_stop          | null                                                                                   \n",
      " content_type                | 1                                                                                      \n",
      " title                       | соблазнительницы женатых мужчин (top heavy homewreckers)                               \n",
      " year                        | 2011.0                                                                                 \n",
      " genres                      | Эротика                                                                                \n",
      " region_id                   | null                                                                                   \n",
      "-RECORD 4-------------------------------------------------------------------------------------------------------------\n",
      " item_id                     | 65670                                                                                  \n",
      " channel_id                  | null                                                                                   \n",
      " datetime_availability_start | 1970-01-01 03:00:00                                                                    \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                                                                    \n",
      " datetime_show_start         | null                                                                                   \n",
      " datetime_show_stop          | null                                                                                   \n",
      " content_type                | 1                                                                                      \n",
      " title                       | секретные секс-материалы ii: темная секс пародия (the sex files ii: a dark xxx parody) \n",
      " year                        | 2010.0                                                                                 \n",
      " genres                      | Эротика                                                                                \n",
      " region_id                   | null                                                                                   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  year| count|\n",
      "+------+------+\n",
      "|1988.0|    15|\n",
      "|1976.0|    20|\n",
      "|1951.0|     1|\n",
      "|1940.0|     2|\n",
      "|1979.0|    22|\n",
      "|1953.0|     5|\n",
      "|1987.0|     8|\n",
      "|1959.0|     3|\n",
      "|1978.0|    18|\n",
      "|1968.0|    18|\n",
      "|2010.0|   259|\n",
      "|  null|631868|\n",
      "|1967.0|    19|\n",
      "|1964.0|    16|\n",
      "|1916.0|     1|\n",
      "|1993.0|     6|\n",
      "|2001.0|    20|\n",
      "|1965.0|    16|\n",
      "|1954.0|     8|\n",
      "|1984.0|    25|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.groupby(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------\n",
      " item_id      | 65667                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Эротика]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 1--------------------------------------------------------\n",
      " item_id      | 65669                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Эротика]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 2--------------------------------------------------------\n",
      " item_id      | 65668                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Эротика]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 3--------------------------------------------------------\n",
      " item_id      | 65671                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Эротика]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 4--------------------------------------------------------\n",
      " item_id      | 65670                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Эротика]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 5--------------------------------------------------------\n",
      " item_id      | 65809                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Комедии]                                        \n",
      " n_genres     | 1                                                \n",
      "-RECORD 6--------------------------------------------------------\n",
      " item_id      | 65810                                            \n",
      " content_type | 1                                                \n",
      " genres_array | [Комедии, Мелодрамы]                             \n",
      " n_genres     | 2                                                \n",
      "-RECORD 7--------------------------------------------------------\n",
      " item_id      | 326                                              \n",
      " content_type | 1                                                \n",
      " genres_array | [Ужасы, Триллеры, Драмы, Фантастика, Зарубежные] \n",
      " n_genres     | 5                                                \n",
      "-RECORD 8--------------------------------------------------------\n",
      " item_id      | 336                                              \n",
      " content_type | 1                                                \n",
      " genres_array | [Ужасы, Комедии, Фантастика, Зарубежные]         \n",
      " n_genres     | 4                                                \n",
      "-RECORD 9--------------------------------------------------------\n",
      " item_id      | 357                                              \n",
      " content_type | 1                                                \n",
      " genres_array | [Комедии, Мелодрамы, Наши]                       \n",
      " n_genres     | 3                                                \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_genres = (items\n",
    " .filter(f.col(\"genres\").isNotNull())\n",
    " .withColumn(\"genres_array\", f.split(f.col(\"genres\"), \",\"))\n",
    " .withColumn(\"n_genres\", f.size(f.col(\"genres_array\")))\n",
    " .drop(\"channel_id\", \"year\", \"title\", \"region_id\", \"genres\",\n",
    "       \"datetime_show_start\", \"datetime_show_stop\", \n",
    "       \"datetime_availability_start\", \"datetime_availability_stop\")\n",
    ")\n",
    "items_genres.show(10, vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|n_genres| count|\n",
      "+--------+------+\n",
      "|       1|632108|\n",
      "|       6|   106|\n",
      "|       3|  1361|\n",
      "|       5|   309|\n",
      "|       4|   775|\n",
      "|       8|    11|\n",
      "|       7|    46|\n",
      "|       2|   819|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_genres.groupby(\"n_genres\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = items_genres.filter(f.col(\"n_genres\") == 1).select(\"genres_array\", ).distinct().orderBy(\"genres_array\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[General]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Боевики]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Военные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Детективы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Детские]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Для детей]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Для самых маленьких]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Документальные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Документальный]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Драма]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Драмы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Зарубежные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Игры]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Исторический]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Комедии]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Комедия]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[Криминал]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Кулинария]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Мелодрамы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[Музыкальные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[Мультсериалы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Мультфильм]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Мюзиклы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[Наши]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[О здоровье]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[Познавательные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Приключения]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[Прочие]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Развлекательные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[Реалити-шоу]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[Семейные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[Спортивные]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[Триллеры]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[Ужасы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[Фантастика]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[Фэнтези]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[Эротика]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[Юмористические]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             genres_array\n",
       "0               [General]\n",
       "1               [Боевики]\n",
       "2               [Военные]\n",
       "3             [Детективы]\n",
       "4               [Детские]\n",
       "5             [Для детей]\n",
       "6   [Для самых маленьких]\n",
       "7        [Документальные]\n",
       "8        [Документальный]\n",
       "9                 [Драма]\n",
       "10                [Драмы]\n",
       "11           [Зарубежные]\n",
       "12                 [Игры]\n",
       "13         [Исторический]\n",
       "14              [Комедии]\n",
       "15              [Комедия]\n",
       "16             [Криминал]\n",
       "17            [Кулинария]\n",
       "18            [Мелодрамы]\n",
       "19          [Музыкальные]\n",
       "20         [Мультсериалы]\n",
       "21           [Мультфильм]\n",
       "22              [Мюзиклы]\n",
       "23                 [Наши]\n",
       "24           [О здоровье]\n",
       "25       [Познавательные]\n",
       "26          [Приключения]\n",
       "27               [Прочие]\n",
       "28      [Развлекательные]\n",
       "29          [Реалити-шоу]\n",
       "30             [Семейные]\n",
       "31           [Спортивные]\n",
       "32             [Триллеры]\n",
       "33                [Ужасы]\n",
       "34           [Фантастика]\n",
       "35              [Фэнтези]\n",
       "36              [Эротика]\n",
       "37       [Юмористические]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|content_type| count|\n",
      "+------------+------+\n",
      "|           1|  3671|\n",
      "|           0|631864|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_genres.groupby(\"content_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_array(v):\n",
    "    return Vectors.dense(v).tolist()\n",
    "\n",
    "sparse_to_array_udf = f.udf(sparse_to_array, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65667                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[21],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65669                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[21],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65668                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[21],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65671                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[21],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65670                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[21],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65809                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[3],[1.0])                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 65810                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[3,8],[1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 326                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[1,2,4,11,13],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " genres_dense_features | [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 336                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[1,3,11,13],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      " genres_dense_features | [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " item_id               | 357                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " content_type          | 1                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      " genres_features       | (84,[3,7,8],[1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      " genres_dense_features | [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"genres_array\", outputCol=\"genres_features\")\n",
    "model_cv_genres = cv.fit(items_genres)\n",
    "items_cv_genres = model_cv_genres.transform(items_genres)\n",
    "items_cv_dense_genres = (\n",
    "    items_cv_genres\n",
    "    .withColumn(\"genres_dense_features\", sparse_to_array_udf(f.col(\"genres_features\")))\n",
    "    .drop(\"genres_array\", \"n_genres\")\n",
    ")\n",
    "items_cv_dense_genres.show(10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, item_features: array<float>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_features = items_cv_dense_genres.select(\"item_id\", f.col(\"genres_dense_features\").alias(\"item_features\"))\n",
    "items_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|item_id|       item_features|\n",
      "+-------+--------------------+\n",
      "|  65667|[0.0, 0.0, 0.0, 0...|\n",
      "|  65669|[0.0, 0.0, 0.0, 0...|\n",
      "|  65668|[0.0, 0.0, 0.0, 0...|\n",
      "|  65671|[0.0, 0.0, 0.0, 0...|\n",
      "|  65670|[0.0, 0.0, 0.0, 0...|\n",
      "|  65809|[0.0, 0.0, 0.0, 1...|\n",
      "|  65810|[0.0, 0.0, 0.0, 1...|\n",
      "|    326|[0.0, 1.0, 1.0, 0...|\n",
      "|    336|[0.0, 1.0, 0.0, 1...|\n",
      "|    357|[0.0, 0.0, 0.0, 1...|\n",
      "|    396|[0.0, 1.0, 1.0, 0...|\n",
      "|    400|[0.0, 1.0, 0.0, 0...|\n",
      "|    423|[0.0, 1.0, 1.0, 0...|\n",
      "|    430|[0.0, 1.0, 1.0, 0...|\n",
      "|    449|[0.0, 1.0, 0.0, 0...|\n",
      "|    453|[0.0, 1.0, 1.0, 1...|\n",
      "|    478|[0.0, 0.0, 0.0, 0...|\n",
      "|    495|[0.0, 1.0, 0.0, 0...|\n",
      "|    505|[0.0, 1.0, 1.0, 0...|\n",
      "|    540|[0.0, 1.0, 1.0, 1...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635535"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (spark\n",
    "        .read\n",
    "#          .schema(test_schema)\n",
    "        .format('csv')\n",
    "        .option(\"sep\", \",\")\n",
    "        .option(\"header\", True)\n",
    "        .option('inferSchema', True)\n",
    "        .load('/labs/slaba03/laba03_test.csv')\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "|   1654|  93629|    null|\n",
      "|   1654|   9980|    null|\n",
      "|   1654|  95099|    null|\n",
      "|   1654|  11265|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 3704)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.select(\"user_id\").distinct().count(), test.select(\"item_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (spark\n",
    "         .read\n",
    "#          .schema(train_schema)\n",
    "         .format('csv')\n",
    "         .option(\"sep\", \",\")\n",
    "         .option(\"header\", True)\n",
    "         .option('inferSchema', True)\n",
    "         .load('/labs/slaba03/laba03_train.csv')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|purchase|  count|\n",
      "+--------+-------+\n",
      "|       1|  10904|\n",
      "|       0|5021720|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('purchase').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 3704)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(\"user_id\").distinct().count(), train.select(\"item_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natalya's approach -- start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train.sampleBy(\"purchase\", fractions={0.0: 0.8, 1.0: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = train.join(train_train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|user_purchases|user_id|\n",
      "+--------------+-------+\n",
      "|            72| 754230|\n",
      "|             1| 761341|\n",
      "+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_purchases = (\n",
    "    train\n",
    "    .groupBy('user_id')\n",
    "    .sum()\n",
    "    .select(f.col(\"sum(purchase)\").alias(\"user_purchases\"), f.col(\"user_id\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "train_purchases.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|item_purchases|item_id|\n",
      "+--------------+-------+\n",
      "|             2|   8638|\n",
      "|             1|  95940|\n",
      "+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_purchases = (\n",
    "    train\n",
    "    .groupBy('item_id')\n",
    "    .sum()\n",
    "    .select(f.col(\"sum(purchase)\").alias(\"item_purchases\"), f.col(\"item_id\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "item_purchases.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train_train.join(train_purchases, on='user_id', how='left')\n",
    "train_valid = train_valid.join(train_purchases, on='user_id', how='left')\n",
    "test = test.join(train_purchases, on='user_id', how='left')\n",
    "\n",
    "train_train = train_train.join(item_purchases, on='item_id', how='left')\n",
    "train_valid = train_valid.join(item_purchases, on='item_id', how='left')\n",
    "test = test.join(item_purchases, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|user_attempts|user_id|\n",
      "+-------------+-------+\n",
      "|         2611| 754230|\n",
      "|         2580| 761341|\n",
      "+-------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_user_attempts = (\n",
    "    train\n",
    "    .groupBy('user_id')\n",
    "    .count()\n",
    "    .select(f.col(\"count\").alias(\"user_attempts\"), f.col(\"user_id\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "train_user_attempts.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|item_attempts|item_id|\n",
      "+-------------+-------+\n",
      "|         1379|   8638|\n",
      "|         1409|  95940|\n",
      "+-------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_item_attempts = (\n",
    "    train\n",
    "    .groupBy('item_id')\n",
    "    .count()\n",
    "    .select(f.col(\"count\").alias(\"item_attempts\"), f.col(\"item_id\"))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "train_item_attempts.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train_train.join(train_user_attempts, on='user_id', how='left')\n",
    "train_valid = train_valid.join(train_user_attempts, on='user_id', how='left')\n",
    "test = test.join(train_user_attempts, on='user_id', how='left')\n",
    "\n",
    "train_train = train_train.join(train_item_attempts, on='item_id', how='left')\n",
    "train_valid = train_valid.join(train_item_attempts, on='item_id', how='left')\n",
    "test = test.join(train_item_attempts, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchases: long (nullable = true)\n",
      " |-- item_purchases: long (nullable = true)\n",
      " |-- user_attempts: long (nullable = true)\n",
      " |-- item_attempts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      " |-- user_purchases: long (nullable = true)\n",
      " |-- item_purchases: long (nullable = true)\n",
      " |-- user_attempts: long (nullable = true)\n",
      " |-- item_attempts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      " |-- user_purchases: long (nullable = true)\n",
      " |-- item_purchases: long (nullable = true)\n",
      " |-- user_attempts: long (nullable = true)\n",
      " |-- item_attempts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train_train.withColumn('user_addict', f.col('user_purchases') / f.col('user_attempts'))\n",
    "train_valid = train_valid.withColumn('user_addict', f.col('user_purchases') / f.col('user_attempts'))\n",
    "test = test.withColumn('user_addict', f.col('user_purchases') / f.col('user_attempts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train_train.withColumn('item_addict', f.col('item_purchases') / f.col('item_attempts'))\n",
    "train_valid = train_valid.withColumn('item_addict', f.col('item_purchases') / f.col('item_attempts'))\n",
    "test = test.withColumn('item_addict', f.col('item_purchases') / f.col('item_attempts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.na.fill(0)\n",
    "train_train = train_train.na.fill(0)\n",
    "train_valid = train_valid.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_attempts: bigint, item_id: int]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_purchases.unpersist()\n",
    "item_purchases.unpersist()\n",
    "train_user_attempts.unpersist()\n",
    "train_item_attempts.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "cols = ['item_purchases', 'user_purchases', 'user_addict', 'item_addict']\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "train_data = assembler.transform(train_train).cache()\n",
    "valid_data = assembler.transform(train_valid)\n",
    "test_data = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"purchase\")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.001, 0.01, 0.1]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator, numFolds=3, parallelism=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9112065837379674,\n",
       " 0.9168167794576112,\n",
       " 0.9181673912830515,\n",
       " 0.9252267030406724]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_553603cb3e9e', name='regParam', doc='regularization parameter (>= 0).'): 0.1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.getEstimatorParamMaps()[np.argmax(cv_model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_valid = cv_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192379017100841"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, user_id: int, purchase: int, user_purchases: bigint, item_purchases: bigint, user_attempts: bigint, item_attempts: bigint, user_addict: double, item_addict: double, features: vector]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"purchase\", regParam=0.1)\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions_valid = lr_model.transform(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192379017100839"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", metricName='areaUnderROC')\n",
    "score = evaluator.evaluate(predictions_valid)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w_features = train_data.union(valid_data)\n",
    "train_w_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"purchase\", regParam=0.1)\n",
    "\n",
    "lr_final_model = lr.fit(train_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_final_model.summary.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = lr_final_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      " |-- user_purchases: long (nullable = true)\n",
      " |-- item_purchases: long (nullable = true)\n",
      " |-- user_attempts: long (nullable = true)\n",
      " |-- item_attempts: long (nullable = true)\n",
      " |-- user_addict: double (nullable = false)\n",
      " |-- item_addict: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " item_id        | 8389                                                 \n",
      " user_id        | 566758                                               \n",
      " purchase       | null                                                 \n",
      " user_purchases | 1                                                    \n",
      " item_purchases | 8                                                    \n",
      " user_attempts  | 2655                                                 \n",
      " item_attempts  | 1338                                                 \n",
      " user_addict    | 3.766478342749529E-4                                 \n",
      " item_addict    | 0.005979073243647235                                 \n",
      " features       | [8.0,1.0,3.766478342749529E-4,0.005979073243647235]  \n",
      " rawPrediction  | [6.107867514628841,-6.107867514628841]               \n",
      " probability    | [0.9977796497682532,0.002220350231746737]            \n",
      " prediction     | 0.0                                                  \n",
      "-RECORD 1--------------------------------------------------------------\n",
      " item_id        | 8389                                                 \n",
      " user_id        | 816426                                               \n",
      " purchase       | null                                                 \n",
      " user_purchases | 0                                                    \n",
      " item_purchases | 8                                                    \n",
      " user_attempts  | 2603                                                 \n",
      " item_attempts  | 1338                                                 \n",
      " user_addict    | 0.0                                                  \n",
      " item_addict    | 0.005979073243647235                                 \n",
      " features       | [8.0,0.0,0.0,0.005979073243647235]                   \n",
      " rawPrediction  | [6.11432706609418,-6.11432706609418]                 \n",
      " probability    | [0.9977939144727415,0.002206085527258545]            \n",
      " prediction     | 0.0                                                  \n",
      "-RECORD 2--------------------------------------------------------------\n",
      " item_id        | 8389                                                 \n",
      " user_id        | 892290                                               \n",
      " purchase       | null                                                 \n",
      " user_purchases | 2                                                    \n",
      " item_purchases | 8                                                    \n",
      " user_attempts  | 2591                                                 \n",
      " item_attempts  | 1338                                                 \n",
      " user_addict    | 7.719027402547279E-4                                 \n",
      " item_addict    | 0.005979073243647235                                 \n",
      " features       | [8.0,2.0,7.719027402547279E-4,0.005979073243647235]  \n",
      " rawPrediction  | [6.101249545215118,-6.101249545215118]               \n",
      " probability    | [0.9977649397793359,0.0022350602206641064]           \n",
      " prediction     | 0.0                                                  \n",
      "-RECORD 3--------------------------------------------------------------\n",
      " item_id        | 8389                                                 \n",
      " user_id        | 740836                                               \n",
      " purchase       | null                                                 \n",
      " user_purchases | 7                                                    \n",
      " item_purchases | 8                                                    \n",
      " user_attempts  | 2519                                                 \n",
      " item_attempts  | 1338                                                 \n",
      " user_addict    | 0.00277888050813815                                  \n",
      " item_addict    | 0.005979073243647235                                 \n",
      " features       | [8.0,7.0,0.00277888050813815,0.005979073243647235]   \n",
      " rawPrediction  | [6.067898295167708,-6.067898295167708]               \n",
      " probability    | [0.9976893159044092,0.0023106840955908173]           \n",
      " prediction     | 0.0                                                  \n",
      "-RECORD 4--------------------------------------------------------------\n",
      " item_id        | 8389                                                 \n",
      " user_id        | 866581                                               \n",
      " purchase       | null                                                 \n",
      " user_purchases | 4                                                    \n",
      " item_purchases | 8                                                    \n",
      " user_attempts  | 2513                                                 \n",
      " item_attempts  | 1338                                                 \n",
      " user_addict    | 0.0015917230401910067                                \n",
      " item_addict    | 0.005979073243647235                                 \n",
      " features       | [8.0,4.0,0.0015917230401910067,0.005979073243647235] \n",
      " rawPrediction  | [6.087764061083333,-6.087764061083333]               \n",
      " probability    | [0.9977346635031175,0.0022653364968825617]           \n",
      " prediction     | 0.0                                                  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted.show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------\n",
      " item_id        | 336                                        \n",
      " user_id        | 1654                                       \n",
      " purchase       | null                                       \n",
      " user_purchases | 5                                          \n",
      " item_purchases | 0                                          \n",
      " user_attempts  | 2568                                       \n",
      " item_attempts  | 1377                                       \n",
      " user_addict    | 0.0019470404984423676                      \n",
      " item_addict    | 0.0                                        \n",
      " features       | [0.0,5.0,0.0019470404984423676,0.0]        \n",
      " rawPrediction  | [6.193518934200147,-6.193518934200147]     \n",
      " probability    | [0.9979615375924372,0.0020384624075628103] \n",
      " prediction     | 0.0                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted.filter(\"user_id == 1654 and item_id == 336\").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_array(v):\n",
    "    return Vectors.dense(v).tolist()\n",
    "\n",
    "sparse_to_array_udf = f.udf(sparse_to_array, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    test_predicted\n",
    "    .select(\n",
    "        f.col(\"user_id\"), \n",
    "        f.col(\"item_id\"), \n",
    "        sparse_to_array_udf(f.col(\"probability\")).getItem(1).alias(\"purchase\"))\n",
    "    .orderBy(\"user_id\", \"item_id\")\n",
    "    .select(f.monotonically_increasing_id().alias(\"\"), \n",
    "            f.col(\"user_id\"), \n",
    "            f.col(\"item_id\"), \n",
    "            f.col(\"purchase\")\n",
    "           )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+------------+\n",
      "|   |user_id|item_id|    purchase|\n",
      "+---+-------+-------+------------+\n",
      "|  0|   1654|    336|0.0020384623|\n",
      "|  1|   1654|    678|0.0020384623|\n",
      "|  2|   1654|    691|0.0020384623|\n",
      "|  3|   1654|    696|0.0020964418|\n",
      "|  4|   1654|    763| 0.002066894|\n",
      "|  5|   1654|    795|0.0023081175|\n",
      "|  6|   1654|    861| 0.002066852|\n",
      "|  7|   1654|   1137|0.0021564376|\n",
      "|  8|   1654|   1159|0.0020957638|\n",
      "|  9|   1654|   1428| 0.002067064|\n",
      "| 10|   1654|   1685|0.0020953426|\n",
      "| 11|   1654|   1686| 0.002066915|\n",
      "| 12|   1654|   1704| 0.002124982|\n",
      "| 13|   1654|   2093|0.0020384623|\n",
      "| 14|   1654|   2343| 0.002066894|\n",
      "| 15|   1654|   2451|0.0020384623|\n",
      "| 16|   1654|   2469|0.0024767197|\n",
      "| 17|   1654|   2603| 0.002067118|\n",
      "| 18|   1654|   2609|0.0020384623|\n",
      "| 19|   1654|   2621| 0.002095384|\n",
      "+---+-------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/alexander.kuznetsov\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.toPandas().to_csv(\"lab03.csv\", header=True, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_regex_transformer_skeleton.ipynb  lab01.json   lab02.json  lab03.ipynb\r\n",
      "lab01.ipynb\t\t\t\t lab02.ipynb  lab03.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natalya's approach -- end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"ts_start\", TimestampType(), True),\n",
    "    StructField(\"ts_stop\", TimestampType(), True),\n",
    "    StructField(\"item_type\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = (spark\n",
    "         .read\n",
    "#          .schema(views_schema)\n",
    "         .format('csv')\n",
    "         .option(\"sep\", \",\")\n",
    "         .option(\"header\", True)\n",
    "         .option('inferSchema', True)\n",
    "         .load('/labs/slaba03/laba03_views_programmes.csv')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- ts_start: integer (nullable = true)\n",
      " |-- ts_end: integer (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|  ts_start|    ts_end|item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|      0|7101053|1491409931|1491411600|     live|\n",
      "|      0|7101054|1491412481|1491451571|     live|\n",
      "|      0|7101054|1491411640|1491412481|     live|\n",
      "|      0|6184414|1486191290|1486191640|     live|\n",
      "|    257|4436877|1490628499|1490630256|     live|\n",
      "|   1654|7489015|1493434801|1493435401|     live|\n",
      "|   1654|7489023|1493444101|1493445601|     live|\n",
      "|   1654|6617053|1489186156|1489200834|     live|\n",
      "|   1654|6438693|1487840070|1487840433|     live|\n",
      "|   1654|6526859|1488705452|1488706154|     live|\n",
      "|   1654|6526754|1488532396|1488532895|      pvr|\n",
      "|   1654|6239098|1486732011|1486732410|     live|\n",
      "|   1654|6438763|1488305761|1488307286|      pvr|\n",
      "|   1654|7489013|1493433301|1493434201|     live|\n",
      "|   1654|6317094|1486829784|1486830389|     live|\n",
      "|   1654|6799393|1490172025|1490173391|      pvr|\n",
      "|   1654|6616978|1488962050|1488962874|      pvr|\n",
      "|   1654|6384471|1487332434|1487333704|     live|\n",
      "|   1654|6688552|1489602627|1489603198|      pvr|\n",
      "|   1654|6678411|1489307460|1489307856|     live|\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20845607"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79385, 633840)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views.select(\"user_id\").distinct().count(), views.select(\"item_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|   min(ts_start_dt)|   max(ts_start_dt)|\n",
      "+-------------------+-------------------+\n",
      "|2017-02-01 03:00:19|2017-05-04 03:54:32|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(views\n",
    " .withColumn(\"ts_start_dt\", f.col(\"ts_start\").cast(TimestampType()))\n",
    " .withColumn(\"ts_end_dt\", f.col(\"ts_end\").cast(TimestampType()))\n",
    " .agg(f.min(\"ts_start_dt\"), f.max(\"ts_start_dt\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_riched = (views\n",
    " .select(\"user_id\", \"item_id\", \"ts_start\", \"ts_end\", \"item_type\")\n",
    " .withColumn(\"ts_start_dt\", f.col(\"ts_start\").cast(TimestampType()))\n",
    " .withColumn(\"ts_end_dt\", f.col(\"ts_end\").cast(TimestampType()))\n",
    " .withColumn(\"duration_min\", (f.col(\"ts_end\") - f.col(\"ts_start\")) / 60)\n",
    " .withColumn(\"hour_start\", f.hour(f.col(\"ts_start_dt\")))\n",
    " .withColumn(\"dayofweek_start_dt\", f.dayofweek(f.col(\"ts_start_dt\")))\n",
    " .withColumn(\"is_weekend_start_dt\", f.dayofweek(f.col(\"ts_start_dt\")).isin([1, 7]).cast(\"int\"))\n",
    " .drop(\"ts_start\", \"ts_end\", \"ts_start_dt\", \"ts_end_dt\")\n",
    "#  .join(items.select(\"item_id\", \"content_type\"), on=\"item_id\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# views_riched.groupby(\"content_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_type: string, duration_min: double, hour_start: int, is_weekend_start_dt: int, item_features_notna: array<double>, duration_adj_item_features: array<double>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views_genres = (\n",
    "    views_riched\n",
    "    .join(f.broadcast(items_features.select(\"item_id\", \"item_features\")), on=\"item_id\", how=\"left\")\n",
    "    .drop(\"item_id\", \"dayofweek_start_dt\")\n",
    "#     .drop(\"content_type\")\n",
    "    .withColumn(\"item_features_notna\", \n",
    "                f.when(f.col(\"item_features\").isNull(), f.array([f.lit(0.0)] * 84))\n",
    "                .otherwise(f.col(\"item_features\")))\n",
    "    .drop(\"item_features\")\n",
    "    .withColumn(\"duration_adj_item_features\", \n",
    "                f.array(*[f.col(\"duration_min\") * f.col(\"item_features_notna\")[i] for i in range(85)]))\n",
    ")\n",
    "views_genres.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2279.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:259)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:172)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:83)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:59)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:276)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:105)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:104)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:310)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\t... 104 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-11d50d9cbf8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mviews_genres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2279.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:259)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:67)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:172)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:37)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:83)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:59)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:276)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:105)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:104)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:310)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\t... 104 more\n"
     ]
    }
   ],
   "source": [
    "views_genres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+----------+-------------------+-------------------+--------------------------+\n",
      "|user_id|item_type|duration_min|hour_start|is_weekend_start_dt|item_features_notna|duration_adj_item_features|\n",
      "+-------+---------+------------+----------+-------------------+-------------------+--------------------------+\n",
      "+-------+---------+------------+----------+-------------------+-------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "views_genres.filter(f.col(\"duration_adj_item_features\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20845607"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views_genres.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, sum_duration_pvr: double, mean_duration_pvr: double, std_duration_pvr: double, mean_is_weekend_start_dt_pvr: double, std_is_weekend_start_dt_pvr: double, mean_hour_start_pvr: double, std_hour_start_pvr: double, weighted_genres_dense_features_pvr: array<double>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_type = \"pvr\"\n",
    "user_pvr = (\n",
    "    views_genres\n",
    "    .filter(f.col(\"item_type\") == item_type)\n",
    "    .select(\"user_id\", \"duration_min\", \"hour_start\", \"is_weekend_start_dt\", \n",
    "            \"genres_dense_features_notna\", \"duration_adj_genres_dense_features\")\n",
    "    .groupby(\"user_id\")\n",
    "    .agg(f.sum(\"duration_min\").alias(f\"sum_duration_{item_type}\"),\n",
    "         f.mean(\"duration_min\").alias(f\"mean_duration_{item_type}\"),\n",
    "         f.stddev(\"duration_min\").alias(f\"std_duration_{item_type}\"),\n",
    "         f.mean(\"is_weekend_start_dt\").alias(f\"mean_is_weekend_start_dt_{item_type}\"),\n",
    "         f.stddev(\"is_weekend_start_dt\").alias(f\"std_is_weekend_start_dt_{item_type}\"),\n",
    "         f.mean(\"hour_start\").alias(f\"mean_hour_start_{item_type}\"),\n",
    "         f.stddev(\"hour_start\").alias(f\"std_hour_start_{item_type}\"),\n",
    "         f.array(*[f.sum(f.col(\"duration_adj_genres_dense_features\")[i]) for i in range(84)])\n",
    "          .alias(f\"sum_duration_adj_genres_dense_features_{item_type}\")\n",
    "        )\n",
    "    .withColumn(f\"weighted_genres_dense_features_{item_type}\", \n",
    "                f.array(*[(f.col(f\"sum_duration_adj_genres_dense_features_{item_type}\")[i] \n",
    "                           / f.col(f\"sum_duration_{item_type}\")) for i in range(84)]))\n",
    "    .drop(f\"sum_duration_adj_genres_dense_features_{item_type}\")\n",
    ")\n",
    "user_pvr.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, sum_duration_live: double, mean_duration_live: double, std_duration_live: double, mean_is_weekend_start_dt_live: double, std_is_weekend_start_dt_live: double, mean_hour_start_live: double, std_hour_start_live: double, weighted_genres_dense_features_live: array<double>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_type = \"live\"\n",
    "user_live = (\n",
    "    views_genres\n",
    "    .filter(f.col(\"item_type\") == item_type)\n",
    "    .select(\"user_id\", \"duration_min\", \"hour_start\", \"is_weekend_start_dt\", \n",
    "            \"genres_dense_features_notna\", \"duration_adj_genres_dense_features\")\n",
    "    .groupby(\"user_id\")\n",
    "    .agg(f.sum(\"duration_min\").alias(f\"sum_duration_{item_type}\"),\n",
    "         f.mean(\"duration_min\").alias(f\"mean_duration_{item_type}\"),\n",
    "         f.stddev(\"duration_min\").alias(f\"std_duration_{item_type}\"),\n",
    "         f.mean(\"is_weekend_start_dt\").alias(f\"mean_is_weekend_start_dt_{item_type}\"),\n",
    "         f.stddev(\"is_weekend_start_dt\").alias(f\"std_is_weekend_start_dt_{item_type}\"),\n",
    "         f.mean(\"hour_start\").alias(f\"mean_hour_start_{item_type}\"),\n",
    "         f.stddev(\"hour_start\").alias(f\"std_hour_start_{item_type}\"),\n",
    "         f.array(*[f.sum(f.col(\"duration_adj_genres_dense_features\")[i]) for i in range(84)])\n",
    "          .alias(f\"sum_duration_adj_genres_dense_features_{item_type}\")\n",
    "        )\n",
    "    .withColumn(f\"weighted_genres_dense_features_{item_type}\", \n",
    "                f.array(*[(f.col(f\"sum_duration_adj_genres_dense_features_{item_type}\")[i] \n",
    "                           / f.col(f\"sum_duration_{item_type}\")) for i in range(84)]))\n",
    "    .drop(f\"sum_duration_adj_genres_dense_features_{item_type}\")\n",
    ")\n",
    "user_live.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type = \"all\"\n",
    "user_all = (\n",
    "    views_genres\n",
    "    .select(\"user_id\", \"duration_min\", \"hour_start\", \"is_weekend_start_dt\", \n",
    "            \"item_features_notna\", \"duration_adj_item_features\")\n",
    "    .groupby(\"user_id\")\n",
    "    .agg(f.sum(\"duration_min\").alias(f\"sum_duration_{item_type}\"),\n",
    "         f.mean(\"duration_min\").alias(f\"mean_duration_{item_type}\"),\n",
    "         f.stddev(\"duration_min\").alias(f\"std_duration_{item_type}\"),\n",
    "         f.mean(\"is_weekend_start_dt\").alias(f\"mean_is_weekend_start_dt_{item_type}\"),\n",
    "         f.stddev(\"is_weekend_start_dt\").alias(f\"std_is_weekend_start_dt_{item_type}\"),\n",
    "         f.mean(\"hour_start\").alias(f\"mean_hour_start_{item_type}\"),\n",
    "         f.stddev(\"hour_start\").alias(f\"std_hour_start_{item_type}\"),\n",
    "         f.array(*[f.sum(f.col(\"duration_adj_item_features\")[i]) for i in range(84)])\n",
    "          .alias(f\"sum_duration_adj_item_features_{item_type}\")\n",
    "        )\n",
    "    .withColumn(f\"weighted_item_features_{item_type}\", \n",
    "                f.array(*[(f.col(f\"sum_duration_adj_item_features_{item_type}\")[i] \n",
    "                           / f.col(f\"sum_duration_{item_type}\")) for i in range(84)]))\n",
    "    .drop(f\"sum_duration_adj_item_features_{item_type}\")\n",
    ")\n",
    "# user_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pvr.show(10, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_live.show(10, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-1f429587eae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_all.show(10, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_features = (\n",
    "    user_all\n",
    "    .join(user_pvr, on=\"user_id\", how=\"left\")\n",
    "    .join(user_live, on=\"user_id\", how=\"left\")\n",
    "    .fillna({\"sum_duration_pvr\": 0.0, \n",
    "             \"mean_duration_pvr\": 0.0, \n",
    "             \"std_duration_pvr\": 0.0, \n",
    "             \"mean_is_weekend_start_dt_pvr\": 0.0, \n",
    "             \"std_is_weekend_start_dt_pvr\": 0.0, \n",
    "             \"mean_hour_start_pvr\": 0.0, \n",
    "             \"std_hour_start_pvr\": 0.0, \n",
    "             \"sum_duration_live\": 0.0, \n",
    "             \"mean_duration_live\": 0.0, \n",
    "             \"std_duration_live\": 0.0, \n",
    "             \"mean_is_weekend_start_dt_live\": 0.0, \n",
    "             \"std_is_weekend_start_dt_live\": 0.0, \n",
    "             \"mean_hour_start_live\": 0.0, \n",
    "             \"std_hour_start_live\": 0.0, \n",
    "             \"sum_duration_all\": 0.0, \n",
    "             \"mean_duration_all\": 0.0, \n",
    "             \"std_duration_all\": 0.0, \n",
    "             \"mean_is_weekend_start_dt_all\": 0.0, \n",
    "             \"std_is_weekend_start_dt_all\": 0.0, \n",
    "             \"mean_hour_start_all\": 0.0, \n",
    "             \"std_hour_start_all\": 0.0, \n",
    "            })\n",
    "    .withColumn(\"weighted_genres_dense_features_pvr_notna\", \n",
    "                f.when(f.col(\"weighted_genres_dense_features_pvr\").isNull(), f.array([f.lit(0.0)] * 85))\n",
    "                .otherwise(f.col(\"weighted_genres_dense_features_pvr\")))\n",
    "    .drop(\"weighted_genres_dense_features_pvr\")\n",
    "    .withColumn(\"weighted_genres_dense_features_live_notna\", \n",
    "                f.when(f.col(\"weighted_genres_dense_features_live\").isNull(), f.array([f.lit(0.0)] * 85))\n",
    "                .otherwise(f.col(\"weighted_genres_dense_features_live\")))\n",
    "    .drop(\"weighted_genres_dense_features_live\")\n",
    "    .withColumn(\"weighted_genres_dense_features_all_notna\", \n",
    "                f.when(f.col(\"weighted_genres_dense_features_all\").isNull(), f.array([f.lit(0.0)] * 85))\n",
    "                .otherwise(f.col(\"weighted_genres_dense_features_all\")))\n",
    "    .drop(\"weighted_genres_dense_features_all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_features = (\n",
    "    user_all\n",
    "    .fillna({\"sum_duration_all\": 0.0, \n",
    "             \"mean_duration_all\": 0.0, \n",
    "             \"std_duration_all\": 0.0, \n",
    "             \"mean_is_weekend_start_dt_all\": 0.0, \n",
    "             \"std_is_weekend_start_dt_all\": 0.0, \n",
    "             \"mean_hour_start_all\": 0.0, \n",
    "             \"std_hour_start_all\": 0.0, \n",
    "            })\n",
    "    .withColumn(\"weighted_item_features_all_notna\", \n",
    "                f.when(f.col(\"weighted_item_features_all\").isNull(), f.array([f.lit(0.0)] * 84))\n",
    "                .otherwise(f.col(\"weighted_item_features_all\")))\n",
    "    .drop(\"weighted_item_features_all\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- sum_duration_all: double (nullable = false)\n",
      " |-- mean_duration_all: double (nullable = false)\n",
      " |-- std_duration_all: double (nullable = false)\n",
      " |-- mean_is_weekend_start_dt_all: double (nullable = false)\n",
      " |-- std_is_weekend_start_dt_all: double (nullable = false)\n",
      " |-- mean_hour_start_all: double (nullable = false)\n",
      " |-- std_hour_start_all: double (nullable = false)\n",
      " |-- weighted_item_features_all_notna: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, user_feature: array<double>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_features_concat = (\n",
    "    users_features.select(\"user_id\",\n",
    "    f.concat(\n",
    "    f.array(f.col(\"sum_duration_all\")), # 1\n",
    "    f.array(f.col(\"mean_duration_all\")), # 2\n",
    "    f.array(f.col(\"std_duration_all\")), # 3\n",
    "    f.array(f.col(\"mean_is_weekend_start_dt_all\")), # 4\n",
    "    f.array(f.col(\"std_is_weekend_start_dt_all\")), # 5\n",
    "    f.array(f.col(\"mean_hour_start_all\")), # 6\n",
    "    f.array(f.col(\"std_hour_start_all\")), # 7\n",
    "    f.array(f.col(\"sum_duration_pvr\")), # 8\n",
    "    f.array(f.col(\"mean_duration_pvr\")), # 9\n",
    "    f.array(f.col(\"std_duration_pvr\")), # 10\n",
    "    f.array(f.col(\"mean_is_weekend_start_dt_pvr\")), # 11\n",
    "    f.array(f.col(\"std_is_weekend_start_dt_pvr\")), # 12\n",
    "    f.array(f.col(\"mean_hour_start_pvr\")), # 13\n",
    "    f.array(f.col(\"std_hour_start_pvr\")), # 14\n",
    "    f.array(f.col(\"sum_duration_live\")), # 15\n",
    "    f.array(f.col(\"mean_duration_live\")), # 16\n",
    "    f.array(f.col(\"std_duration_live\")), # 17\n",
    "    f.array(f.col(\"mean_is_weekend_start_dt_live\")), # 18\n",
    "    f.array(f.col(\"std_is_weekend_start_dt_live\")), # 19\n",
    "    f.array(f.col(\"mean_hour_start_live\")), # 20\n",
    "    f.array(f.col(\"std_hour_start_live\")), # 21\n",
    "    f.col(\"weighted_genres_dense_features_all_notna\"), # 22 : 22 + 85\n",
    "    f.col(\"weighted_genres_dense_features_pvr_notna\"), # 108 : 108 + 85\n",
    "    f.col(\"weighted_genres_dense_features_live_notna\"), # 194 : 194 + 85\n",
    "           ).alias(\"user_feature\")) # (0 : 280)\n",
    ")\n",
    "users_features_concat.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, user_feature: array<double>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_features_concat = (\n",
    "    users_features.select(\"user_id\",\n",
    "    f.concat(\n",
    "    f.array(f.col(\"sum_duration_all\")), # 1\n",
    "    f.array(f.col(\"mean_duration_all\")), # 2\n",
    "    f.array(f.col(\"std_duration_all\")), # 3\n",
    "    f.array(f.col(\"mean_is_weekend_start_dt_all\")), # 4\n",
    "    f.array(f.col(\"std_is_weekend_start_dt_all\")), # 5\n",
    "    f.array(f.col(\"mean_hour_start_all\")), # 6\n",
    "    f.array(f.col(\"std_hour_start_all\")), # 7\n",
    "    f.col(\"weighted_item_features_all_notna\"), # \n",
    "           ).alias(\"user_feature\")) # \n",
    ")\n",
    "users_features_concat.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "HashAggregate(keys=[user_id#258], functions=[sum(duration_min#314), avg(duration_min#314), stddev_samp(duration_min#314), avg(cast(is_weekend_start_dt#344 as bigint)), stddev_samp(cast(is_weekend_start_dt#344 as double)), avg(cast(hour_start#323 as bigint)), stddev_samp(cast(hour_start#323 as double)), sum(duration_adj_item_features#393[0]), sum(duration_adj_item_features#393[1]), sum(duration_adj_item_features#393[2]), sum(duration_adj_item_features#393[3]), sum(duration_adj_item_features#393[4]), sum(duration_adj_item_features#393[5]), sum(duration_adj_item_features#393[6]), sum(duration_adj_item_features#393[7]), sum(duration_adj_item_features#393[8]), sum(duration_adj_item_features#393[9]), sum(duration_adj_item_features#393[10]), sum(duration_adj_item_features#393[11]), sum(duration_adj_item_features#393[12]), sum(duration_adj_item_features#393[13]), sum(duration_adj_item_features#393[14]), sum(duration_adj_item_features#393[15]), sum(duration_adj_item_features#393[16]), ... 67 more fields])\n",
      "+- Exchange hashpartitioning(user_id#258, 200)\n",
      "   +- HashAggregate(keys=[user_id#258], functions=[partial_sum(duration_min#314), partial_avg(duration_min#314), partial_stddev_samp(duration_min#314), partial_avg(cast(is_weekend_start_dt#344 as bigint)), partial_stddev_samp(cast(is_weekend_start_dt#344 as double)), partial_avg(cast(hour_start#323 as bigint)), partial_stddev_samp(cast(hour_start#323 as double)), partial_sum(duration_adj_item_features#393[0]), partial_sum(duration_adj_item_features#393[1]), partial_sum(duration_adj_item_features#393[2]), partial_sum(duration_adj_item_features#393[3]), partial_sum(duration_adj_item_features#393[4]), partial_sum(duration_adj_item_features#393[5]), partial_sum(duration_adj_item_features#393[6]), partial_sum(duration_adj_item_features#393[7]), partial_sum(duration_adj_item_features#393[8]), partial_sum(duration_adj_item_features#393[9]), partial_sum(duration_adj_item_features#393[10]), partial_sum(duration_adj_item_features#393[11]), partial_sum(duration_adj_item_features#393[12]), partial_sum(duration_adj_item_features#393[13]), partial_sum(duration_adj_item_features#393[14]), partial_sum(duration_adj_item_features#393[15]), partial_sum(duration_adj_item_features#393[16]), ... 67 more fields])\n",
      "      +- InMemoryTableScan [user_id#258, duration_min#314, hour_start#323, is_weekend_start_dt#344, duration_adj_item_features#393]\n",
      "            +- InMemoryRelation [user_id#258, item_type#262, duration_min#314, hour_start#323, is_weekend_start_dt#344, item_features_notna#379, duration_adj_item_features#393], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- Coalesce 12\n",
      "                     +- *(5) Project [user_id#258, item_type#262, duration_min#314, hour_start#323, is_weekend_start_dt#344, CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END AS item_features_notna#379, array((duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[0]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[1]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[2]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[3]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[4]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[5]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[6]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[7]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[8]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[9]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[10]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[11]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[12]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[13]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[14]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[15]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[16]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[17]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[18]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[19]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[20]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[21]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[22]), (duration_min#314 * CASE WHEN isnull(item_features#215) THEN [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ELSE cast(item_features#215 as array<double>) END[23]), ... 61 more fields) AS duration_adj_item_features#393]\n",
      "                        +- SortMergeJoin [item_id#259], [item_id#10], LeftOuter\n",
      "                           :- *(2) Sort [item_id#259 ASC NULLS FIRST], false, 0\n",
      "                           :  +- Exchange hashpartitioning(item_id#259, 200)\n",
      "                           :     +- *(1) Project [user_id#258, item_id#259, item_type#262, (cast((ts_end#261 - ts_start#260) as double) / 60.0) AS duration_min#314, hour(cast(ts_start#260 as timestamp), Some(Europe/Moscow)) AS hour_start#323, cast(dayofweek(cast(cast(ts_start#260 as timestamp) as date)) IN (1,7) as int) AS is_weekend_start_dt#344]\n",
      "                           :        +- *(1) FileScan csv [user_id#258,item_id#259,ts_start#260,ts_end#261,item_type#262] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/labs/slaba03/laba03_views_programme..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:int,item_id:int,ts_start:int,ts_end:int,item_type:string>\n",
      "                           +- *(4) Sort [item_id#10 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(item_id#10, 200)\n",
      "                                 +- *(3) Filter isnotnull(item_id#10)\n",
      "                                    +- InMemoryTableScan [item_id#10, item_features#215], [isnotnull(item_id#10)]\n",
      "                                          +- InMemoryRelation [item_id#10, item_features#215], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                +- *(2) Project [item_id#10, pythonUDF0#218 AS item_features#215]\n",
      "                                                   +- BatchEvalPython [sparse_to_array(UDF(split(genres#19, ,)))], [genres#19, item_id#10, pythonUDF0#218]\n",
      "                                                      +- *(1) Project [genres#19, item_id#10]\n",
      "                                                         +- *(1) Filter isnotnull(genres#19)\n",
      "                                                            +- *(1) FileScan csv [item_id#10,genres#19] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/labs/slaba03/laba03_items.csv], PartitionFilters: [], PushedFilters: [IsNotNull(genres)], ReadSchema: struct<item_id:int,genres:string>\n"
     ]
    }
   ],
   "source": [
    "users_features.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a3b0f57ad56a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers_features_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "users_features_concat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|user_id|user_feature|\n",
      "+-------+------------+\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_features_concat.filter(f.col(\"user_feature\").isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|item_id|item_feature|\n",
      "+-------+------------+\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_features.filter(f.col(\"item_feature\").isNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: float, item_user_feature_notna: array<double>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = (\n",
    "    train.select(\"user_id\", \"item_id\", f.col(\"purchase\").cast(FloatType()).alias(\"purchase\"))\n",
    "    .join(items_features, on=\"item_id\", how=\"left\")\n",
    "    .join(users_features_concat, on=\"user_id\", how=\"left\")\n",
    "    .select(\"user_id\", \"item_id\", \"purchase\", \n",
    "        f.concat(f.col(\"item_feature\"), f.col(\"user_feature\")).alias(\"item_user_feature\")\n",
    "           )\n",
    "    .withColumn(\"item_user_feature_notna\", \n",
    "                f.when(f.col(\"item_user_feature\").isNull(), f.array([f.lit(0.0)] * 361))\n",
    "                .otherwise(f.col(\"item_user_feature\")))\n",
    "    .drop(\"item_user_feature\")\n",
    ")\n",
    "train_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: float (nullable = true)\n",
      " |-- item_user_feature_notna: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------------+\n",
      "|user_id|item_id|purchase|item_user_feature_notna|\n",
      "+-------+-------+--------+-----------------------+\n",
      "| 754230|    948|     0.0|   [0.0, 1.0, 1.0, 0...|\n",
      "| 754230|   7038|     0.0|   [0.0, 0.0, 0.0, 0...|\n",
      "| 754230|  66724|     0.0|   [0.0, 0.0, 0.0, 0...|\n",
      "| 754230|  73019|     0.0|   [0.0, 0.0, 0.0, 0...|\n",
      "| 754230|  73215|     0.0|   [0.0, 0.0, 0.0, 1...|\n",
      "| 754230|  75384|     0.0|   [0.0, 1.0, 1.0, 0...|\n",
      "| 754230|  87533|     0.0|   [0.0, 1.0, 0.0, 0...|\n",
      "| 754230|  88770|     0.0|   [0.0, 0.0, 0.0, 1...|\n",
      "| 754230|  90017|     0.0|   [0.0, 1.0, 1.0, 0...|\n",
      "| 754230| 100029|     0.0|   [0.0, 0.0, 0.0, 0...|\n",
      "+-------+-------+--------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------------+\n",
      "|user_id|item_id|purchase|item_user_feature_notna|\n",
      "+-------+-------+--------+-----------------------+\n",
      "+-------+-------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_features.filter(f.col(\"item_user_feature_notna\").isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3675.showString.\n: org.apache.spark.SparkException: Job 119 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-2e6c0e226fe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"item_user_feature_notna\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature_size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3675.showString.\n: org.apache.spark.SparkException: Job 119 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "train_features.groupby(f.size(f.col(\"item_user_feature_notna\")).alias(\"feature_size\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, item_id: int, purchase: string, item_user_feature_notna: array<double>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = (\n",
    "    test\n",
    "    .join(items_features, on=\"item_id\", how=\"left\")\n",
    "    .join(users_features_concat, on=\"user_id\", how=\"left\")\n",
    "    .select(\"user_id\", \"item_id\", \"purchase\", \n",
    "        f.concat(f.col(\"item_feature\"), f.col(\"user_feature\")).alias(\"item_user_feature\")\n",
    "           )\n",
    "    .withColumn(\"item_user_feature_notna\", \n",
    "                f.when(f.col(\"item_user_feature\").isNull(), f.array([f.lit(0.0)] * 361))\n",
    "                .otherwise(f.col(\"item_user_feature\")))\n",
    "    .drop(\"item_user_feature\")\n",
    ")\n",
    "test_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      " |-- item_user_feature_notna: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+\n",
      "|user_id|item_id|purchase|   item_user_feature|\n",
      "+-------+-------+--------+--------------------+\n",
      "| 754230|  73041|    null|[0.0, 1.0, 1.0, 0...|\n",
      "| 754230|  74440|    null|[0.0, 1.0, 0.0, 0...|\n",
      "| 754230|  74452|    null|[0.0, 1.0, 0.0, 0...|\n",
      "| 754230|  93131|    null|[0.0, 0.0, 0.0, 0...|\n",
      "| 754230|  93633|    null|[0.0, 0.0, 0.0, 1...|\n",
      "| 754230|  95151|    null|[0.0, 0.0, 0.0, 0...|\n",
      "| 754230|   9071|    null|[0.0, 0.0, 0.0, 1...|\n",
      "| 754230|  72387|    null|[0.0, 0.0, 1.0, 0...|\n",
      "| 754230|  78269|    null|[0.0, 1.0, 0.0, 1...|\n",
      "| 754230|  93477|    null|[0.0, 0.0, 1.0, 0...|\n",
      "+-------+-------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------------+\n",
      "|user_id|item_id|purchase|item_user_feature_notna|\n",
      "+-------+-------+--------+-----------------------+\n",
      "+-------+-------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_features.filter(f.col(\"item_user_feature_notna\").isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 1.0, 1: 2.0, 2: 0.0, 3: 0.0})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.sparse(4, range(4), [1.0, 2.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.dense([1.0, 2.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_vector(v):\n",
    "#     return Vectors.sparse(len(v), range(len(v)), v)\n",
    "    return Vectors.dense(v)\n",
    "\n",
    "array_to_vector_udf = f.udf(array_to_vector, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------------------------+\n",
      "|user_id|item_id|purchase|item_user_feature_sparse|\n",
      "+-------+-------+--------+------------------------+\n",
      "| 754230|    948|     0.0|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230|   7038|     0.0|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  66724|     0.0|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  73019|     0.0|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  73215|     0.0|    [0.0,0.0,0.0,1.0,...|\n",
      "| 754230|  75384|     0.0|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230|  87533|     0.0|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|  88770|     0.0|    [0.0,0.0,0.0,1.0,...|\n",
      "| 754230|  90017|     0.0|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230| 100029|     0.0|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230| 100218|     0.0|    [0.0,1.0,0.0,1.0,...|\n",
      "| 754230| 100462|     0.0|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|   9294|     0.0|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|   9938|     0.0|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|   9967|     0.0|    [0.0,0.0,1.0,0.0,...|\n",
      "| 754230|  10930|     0.0|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|  71570|     0.0|    [0.0,0.0,0.0,1.0,...|\n",
      "| 754230|  72067|     0.0|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230|  77784|     0.0|    [0.0,0.0,1.0,0.0,...|\n",
      "| 754230|  77867|     0.0|    [0.0,0.0,0.0,1.0,...|\n",
      "+-------+-------+--------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_vector_features = (\n",
    "    train_features\n",
    "    .withColumn(\"item_user_feature_sparse\", array_to_vector_udf(f.col(\"item_user_feature_notna\")))\n",
    "    .drop(\"item_user_feature_notna\")\n",
    ")\n",
    "train_vector_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------------------------+\n",
      "|user_id|item_id|purchase|item_user_feature_sparse|\n",
      "+-------+-------+--------+------------------------+\n",
      "| 754230|  11025|    null|    [0.0,1.0,0.0,1.0,...|\n",
      "| 754230|  72912|    null|    [0.0,0.0,0.0,1.0,...|\n",
      "| 754230|  86406|    null|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  88999|    null|    [0.0,1.0,0.0,1.0,...|\n",
      "| 754230|  93487|    null|    [0.0,0.0,1.0,0.0,...|\n",
      "| 754230|   9071|    null|    [0.0,0.0,0.0,1.0,...|\n",
      "| 754230|  72387|    null|    [0.0,0.0,1.0,0.0,...|\n",
      "| 754230|  78269|    null|    [0.0,1.0,0.0,1.0,...|\n",
      "| 754230|  93477|    null|    [0.0,0.0,1.0,0.0,...|\n",
      "| 754230|  94726|    null|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230|  10128|    null|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|  73421|    null|    [0.0,1.0,1.0,0.0,...|\n",
      "| 754230|  74556|    null|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|  79417|    null|    [0.0,1.0,0.0,1.0,...|\n",
      "| 754230|  92740|    null|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  93020|    null|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  94243|    null|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|  94910|    null|    [0.0,1.0,0.0,0.0,...|\n",
      "| 754230|  83698|    null|    [0.0,0.0,0.0,0.0,...|\n",
      "| 754230|   1525|    null|    [0.0,1.0,1.0,0.0,...|\n",
      "+-------+-------+--------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_vector_features = (\n",
    "    test_features\n",
    "    .withColumn(\"item_user_feature_sparse\", array_to_vector_udf(f.col(\"item_user_feature_notna\")))\n",
    "    .drop(\"item_user_feature_notna\")\n",
    ")\n",
    "test_vector_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train = train_vector_features.sampleBy(\"purchase\", fractions={0.0: 0.8, 1.0: 0.8}, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: float (nullable = true)\n",
      " |-- item_user_feature_sparse: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = train_vector_features.join(train_train, on=[\"user_id\", \"item_id\"], how=\"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- purchase: float (nullable = true)\n",
      " |-- item_user_feature_sparse: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_valid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"item_user_feature_sparse\", labelCol=\"purchase\", maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3581.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 269 (treeAggregate at LogisticRegression.scala:520) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Error in reading FileSegmentManagedBuffer{file=/hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data, offset=16164, length=1947} \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:449) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:811) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:770) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:934) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:970) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125) \tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221) \tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92) \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:123) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Error in reading FileSegmentManagedBuffer{file=/hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data, offset=16164, length=1947} \tat org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:111) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:442) \t... 86 more Caused by: java.io.FileNotFoundException: /hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data (No such file or directory) \tat java.io.FileInputStream.open0(Native Method) \tat java.io.FileInputStream.open(FileInputStream.java:195) \tat java.io.FileInputStream.<init>(FileInputStream.java:138) \tat org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:100) \t... 87 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2143)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-381dd1c5e761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3581.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 269 (treeAggregate at LogisticRegression.scala:520) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Error in reading FileSegmentManagedBuffer{file=/hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data, offset=16164, length=1947} \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:449) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.sort_addToSorter_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:811) \tat org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:770) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:934) \tat org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:970) \tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:125) \tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221) \tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92) \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:123) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Error in reading FileSegmentManagedBuffer{file=/hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data, offset=16164, length=1947} \tat org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:111) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:442) \t... 86 more Caused by: java.io.FileNotFoundException: /hadoop/yarn/local/usercache/alexander.kuznetsov/appcache/application_1615561586883_0711/blockmgr-0275250d-14e2-4b09-9f51-057e33e7f089/0a/shuffle_35_103_0.data (No such file or directory) \tat java.io.FileInputStream.open0(Native Method) \tat java.io.FileInputStream.open(FileInputStream.java:195) \tat java.io.FileInputStream.<init>(FileInputStream.java:138) \tat org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:100) \t... 87 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2143)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1137)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:520)\n\tat org.apache.spark.ml.classification.LogisticRegression$$anonfun$train$1.apply(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:494)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:489)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = lr_model.transform(train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"item_user_feature_sparse\", labelCol=\"purchase\", maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3233.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 241 (count at DecisionTreeMetadata.scala:118) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 25 \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) \tat scala.collection.Iterator$class.foreach(Iterator.scala:891) \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92) \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:123) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2143)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:297)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-70791d5bbd96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3233.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 241 (count at DecisionTreeMetadata.scala:118) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 25 \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882) \tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878) \tat scala.collection.Iterator$class.foreach(Iterator.scala:891) \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334) \tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878) \tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52) \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92) \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359) \tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165) \tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091) \tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156) \tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882) \tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:308) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) \tat org.apache.spark.scheduler.Task.run(Task.scala:123) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2143)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:129)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor$$anonfun$train$2.apply(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:124)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:297)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:55)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:206)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "gbt_model = gbt.fit(train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gbt_model.transform(train_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"purchase\", metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = gbt_model.transform(test_vector_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
