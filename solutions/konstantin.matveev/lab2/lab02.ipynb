{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 hdfs hdfs   69519728 2021-02-27 21:58 /labs/slaba02/DO_record_per_line.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba02/DO_record_per_line.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, DoubleType, StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"provider\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "|9/humanities|15/m...|This game-based c...|  9|  en|College Foundatio...|Canvas Network|\n",
      "|  14/social_sciences|What’s in your di...| 10|  en|Digital Literacies I|Canvas Network|\n",
      "|  14/social_sciences|The goal of the D...| 11|  en|Digital Literacie...|Canvas Network|\n",
      "|  14/social_sciences|Ready to explore ...| 12|  en|Digital Tools for...|Canvas Network|\n",
      "|  14/social_sciences|This self-paced c...| 13|  en|Discover Your Val...|Canvas Network|\n",
      "|  12/medicine_health|What is “interpro...| 14|  en|Enhancing Patient...|Canvas Network|\n",
      "|        16/languages|This course prese...| 15|  en|Ethics and Values...|Canvas Network|\n",
      "|         4/chemistry|Chemistry is an i...| 16|  en| Exploring Chemistry|Canvas Network|\n",
      "|8/engineering_tec...|Are you consideri...| 17|  en|Exploring Enginee...|Canvas Network|\n",
      "|   1/arts_music_film|Princess stories ...| 18|  en|Fairy Tales: Orig...|Canvas Network|\n",
      "|        9/humanities|This first instal...| 19|  en|First Peoples to ...|Canvas Network|\n",
      "|  14/social_sciences|This course exami...| 20|  en| Forums for a Future|Canvas Network|\n",
      "|        9/humanities|This course will ...| 21|  en|From the Gilded A...|Canvas Network|\n",
      "|8/engineering_tec...|The field of tech...| 22|  en|Fundamentals of S...|Canvas Network|\n",
      "|  14/social_sciences|Are you a Higher ...| 23|  en|Hybrid Courses: B...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.json('/labs/slaba02/DO_record_per_line.json', schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accounting'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "import re\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "string = df.take(1)[0].name\n",
    "regex.findall(string.lower())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accounting',\n",
       " 'cycle',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'of',\n",
       " 'business',\n",
       " 'measurement',\n",
       " 'and',\n",
       " 'reporting']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.findall(df.take(1)[0].name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'course', 'introduces', 'the', 'basic', 'financial', 'statements', 'used', 'by', 'most', 'businesses', 'as', 'well', 'as', 'the', 'essential', 'tools', 'used', 'to', 'prepare', 'them', 'This', 'course', 'will', 'serve', 'as', 'resource', 'to', 'help', 'business', 'students', 'succeed', 'in', 'their', 'upcoming', 'university', 'level', 'accounting', 'classes', 'and', 'as', 'refresher', 'for', 'upper', 'division', 'accounting', 'students', 'who', 'are', 'struggling', 'to', 'recall', 'elementary', 'concepts', 'essential', 'to', 'more', 'advanced', 'accounting', 'topics', 'Business', 'owners', 'will', 'also', 'benefit', 'from', 'this', 'class', 'by', 'gaining', 'essential', 'skills', 'necessary', 'to', 'organize', 'and', 'manage', 'information', 'pertinent', 'to', 'operating', 'their', 'business', 'At', 'the', 'conclusion', 'of', 'the', 'class', 'students', 'will', 'understand', 'the', 'balance', 'sheet', 'income', 'statement', 'and', 'cash', 'flow', 'statement', 'They', 'will', 'be', 'able', 'to', 'differentiate', 'between', 'cash', 'basis', 'and', 'accrual', 'basis', 'techniques', 'and', 'know', 'when', 'each', 'is', 'appropriate', 'They', 'll', 'also', 'understand', 'the', 'accounting', 'equation', 'how', 'to', 'journalize', 'and', 'post', 'transactions', 'how', 'to', 'adjust', 'and', 'close', 'accounts', 'and', 'how', 'to', 'prepare', 'key', 'financial', 'reports', 'All', 'material', 'for', 'this', 'class', 'is', 'written', 'and', 'delivered', 'by', 'the', 'professor', 'and', 'can', 'be', 'previewed', 'here', 'Students', 'must', 'have', 'access', 'to', 'spreadsheet', 'program', 'to', 'participate']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType, ArrayType, StringType\n",
    "from pyspark.sql.functions import lower, col\n",
    "import re\n",
    "\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "string=df.take(1)[0].name\n",
    "regex.findall(string.lower())\n",
    "\n",
    "def ppp2(string):\n",
    "     return regex.findall(string)\n",
    "    \n",
    "\n",
    "print(ppp2(df.take(1)[0].desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_udfi = udf(ppp2, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23126\n",
      "21617\n",
      "16627\n",
      "11556\n",
      "16704\n",
      "13702\n"
     ]
    }
   ],
   "source": [
    "my_courses = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "for i in my_courses:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|                 cat|                desc|   id|lang|                name|provider|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|                    | La transformació...|11556|  es|Aprendizaje Colab...|   Udemy|\n",
      "|6/economics_finan...|Математическая эк...|13702|  ru|Математическая эк...|  Intuit|\n",
      "|                    | Hazte más emplea...|16627|  es|Aprende Excel: Ni...|   Udemy|\n",
      "|5/computer_scienc...|В курсе рассматри...|16704|  ru|Программирование ...|  Intuit|\n",
      "|  5/computer_science|An introduction t...|21617|  en|Preparing for the...|     edX|\n",
      "|                    | Improve your SAS...|23126|  en|Compass - powerfu...|   Udemy|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cours = df.where('id in (23126, 21617, 16627, 11556, 16704, 13702)')\n",
    "my_cours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cours.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df.where(\"lang='ru'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------------+--------------------+\n",
      "|                 cat|                desc| id|lang|                name|            provider|               words|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------------+--------------------+\n",
      "|  5/computer_science|Часть 1. Продвину...| 46|  ru|Дополнительные гл...|Computer Science ...|[часть, 1., продв...|\n",
      "|  5/computer_science|Splay-дерево и де...| 47|  ru|Алгоритмы и струк...|Computer Science ...|[splay-дерево, и,...|\n",
      "|  5/computer_science|Курс посвящён тео...| 48|  ru|Технологии хранен...|Computer Science ...|[курс, посвящён, ...|\n",
      "|2/biology_life_sc...|Биоинформатика — ...| 49|  ru|Алгоритмы в биоин...|Computer Science ...|[биоинформатика, ...|\n",
      "|5/computer_scienc...|Курс знакомит со ...| 50|  ru|Сложность вычисле...|Computer Science ...|[курс, знакомит, ...|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsData=wordsData.withColumn(\"count\", f.size(\"words\")).where(\"count<>0\")\n",
    "wordsData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"hTF\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"hTF\", outputCol=\"TFIDF\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### моя база"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2=df.select(\"id\",\"lang\",\"desc\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Base = spark.createDataFrame([[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']],schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "_join=_Base.select(\"id\").join(rescaledData.select(\"id\",\"lang\",\"TFIDF\"),\"id\", how=\"inner\").where(\"id=16704\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|   id|lang|               TFIDF|\n",
      "+-----+----+--------------------+\n",
      "|16704|  ru|(10000,[381,1144,...|\n",
      "+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=_join.join(rescaledData.select(col(\"id\").alias(\"_id\"),\"lang\",col(\"TFIDF\").alias(\"_TFIDF\")),\"lang\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf('double')\n",
    "def cos(x,y):\n",
    "    return float(x.dot(y) / (x.norm(2) * y.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=join.withColumn('cos',cos(\"TFIDF\",\"_TFIDF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+-----+--------------------+---+\n",
      "|lang|   id|               TFIDF|  _id|              _TFIDF|cos|\n",
      "+----+-----+--------------------+-----+--------------------+---+\n",
      "|  ru|16704|(10000,[381,1144,...|16704|(10000,[381,1144,...|1.0|\n",
      "+----+-----+--------------------+-----+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.where(\"id=_id\").orderBy(\"cos\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=res.select(\"lang\",\"id\",\"_id\",\"TFIDF\",\"_TFIDF\",\"cos\").where(\"id<>_id\").orderBy(\"id\",\"cos\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_out=res.select(\"id\",\"_id\",\"cos\", f.row_number().over(Window.partitionBy(\"id\").orderBy(col(\"cos\").desc())).alias(\"N\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_out=res_out.where(\"N<=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+---+\n",
      "|   id|  _id|                cos|  N|\n",
      "+-----+-----+-------------------+---+\n",
      "|16704| 1228|0.13259561056914948|  1|\n",
      "|16704| 1219|0.13203145826676593|  2|\n",
      "|16704| 1327|0.11165396465657113|  3|\n",
      "|16704|20362|0.11165396465657113|  4|\n",
      "|16704| 1365|  0.096256185090638|  5|\n",
      "|16704|   55|0.08723747541896949|  6|\n",
      "|16704|26980|0.08546174616238522|  7|\n",
      "|16704|18331|0.08433126218873696|  8|\n",
      "|16704| 1236|0.08378398770773124|  9|\n",
      "|16704| 8186| 0.0779310749763002| 10|\n",
      "+-----+-----+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_out.where(\"id=16704\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  _id|\n",
      "+-----+\n",
      "| 1228|\n",
      "| 1219|\n",
      "| 1327|\n",
      "|20362|\n",
      "| 1365|\n",
      "|   55|\n",
      "|26980|\n",
      "|18331|\n",
      "| 1236|\n",
      "| 8186|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_out.where(\"id=16704\").select(\"_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "results['11556'] = [16488, 11554, 16929, 9417, 13461, 5750, 23357, 7833, 9289, 17860]\n",
    "\n",
    "results['13702'] = [864, 21079, 792, 8123, 8083, 1033, 1041, 13057, 19925, 1217]\n",
    "\n",
    "results['16627'] = [11431, 5372, 13021, 12247, 25010, 7296, 9470, 5687, 11575, 9598]\n",
    "\n",
    "results['16704'] = [1228, 1219, 1327, 20362, 1365, 55, 26980, 18331, 1236, 8186]\n",
    "\n",
    "results['21617'] = [21609, 58, 21758, 21608, 21616, 21526, 7614, 21718, 7641, 21780]\n",
    "\n",
    "results['23126'] = [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, DoubleType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
