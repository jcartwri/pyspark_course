{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF, StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField('gender', StringType()),\n",
    "                    StructField('age', StringType()),\n",
    "                    StructField('uid', StringType()),\n",
    "                    StructField('user_json', StringType())])\n",
    "data = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', schema=schema, header=True, sep='\\t')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- parsed_json: struct (nullable = true)\n",
      " |    |-- visits: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_schema = StructType().add('visits', ArrayType(StructType([StructField('url', StringType()),\n",
    "                                                              StructField('timestamp', StringType())])))\n",
    "data = data.withColumn('parsed_json', f.from_json('user_json', json_schema)).repartition(9)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = data.select('uid', 'gender', 'age', f.explode(f.col('parsed_json').visits).alias('visits')) \\\n",
    "                .select('uid', 'gender', 'age', f.col('visits').url.alias('url'), \n",
    "                        f.col('visits').timestamp.alias('timestamp').cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = tmp_df.groupby(\"uid\").agg(f.max(tmp_df.gender).alias('gender'), f.max(tmp_df.age).alias('age'),\n",
    "                                     f.concat_ws(\"|\", f.collect_list(tmp_df.url)).alias('urls'),\n",
    "                                    f.collect_list(tmp_df.timestamp).alias('timestamps')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- urls: string (nullable = false)\n",
      " |-- timestamps: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.where((f.col('age')!='-')&(f.col('gender')!='-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.withColumn('age_gender', f.concat(f.col('age'), f.lit('|'), f.col('gender')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|age_gender|count|\n",
      "+----------+-----+\n",
      "|    >=55|M|  784|\n",
      "|   45-54|F| 2597|\n",
      "|   18-24|F| 2886|\n",
      "|   25-34|F| 6791|\n",
      "|   45-54|M| 2147|\n",
      "|   35-44|M| 5089|\n",
      "|   18-24|M| 2012|\n",
      "|   25-34|M| 8666|\n",
      "|   35-44|F| 4271|\n",
      "|    >=55|F|  895|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df.groupby('age_gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(tokens):\n",
    "    return [t for t in tokens if t.isalpha() and len(t)>2 and t not in [\"http\", \"www\", \"https\", \"html\", \"ru\", \"com\"]]\n",
    "\n",
    "class UrlTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(UrlTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        tokenize_udf = f.udf(clean_url, returnType=ArrayType(StringType()))\n",
    "        return dataset.withColumn(self.getOutputCol(), tokenize_udf(f.col(self.getInputCol())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IDF, RegexTokenizer, StopWordsRemover, HashingTF, IndexToString\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_age = StringIndexer(inputCol=\"age\", outputCol=\"age_target\")\n",
    "indexer_gender = StringIndexer(inputCol=\"gender\", outputCol=\"gender_target\")\n",
    "tokenizer = RegexTokenizer(inputCol='urls', outputCol='tokens', pattern=\"[\\\\p{Punct}\\\\s]+\")\n",
    "cleaner = UrlTransformer(inputCol=tokenizer.getOutputCol(), outputCol='clean_tokens')\n",
    "vect = HashingTF(inputCol=cleaner.getOutputCol(), outputCol='features', numFeatures=15000)\n",
    "idf = IDF(inputCol=vect.getOutputCol(), outputCol='idf')\n",
    "pipeline = Pipeline(stages=[indexer_age, indexer_gender, tokenizer, cleaner, vect, idf]).fit(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pipeline.transform(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_gender = GBTClassifier(labelCol=\"gender_target\", featuresCol=\"idf\", predictionCol='gender_pred', maxIter=10)\n",
    "\n",
    "model_gender = gbt_gender.fit(features.select('idf', 'gender_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_age = LogisticRegression(labelCol=\"age_target\", featuresCol=\"idf\", predictionCol='age_pred', \n",
    "                            probabilityCol=\"age_probability\", rawPredictionCol=\"age_rawPrediction\", \n",
    "                            elasticNetParam=0.2, regParam=0.01)\n",
    "\n",
    "model_age = lr_age.fit(features.select('idf', 'age_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/23 17:02:51 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/gender_gbt' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/.Trash/Current/user/maria.godgildieva/gender_gbt1616508171347\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -R gender_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/23 17:02:54 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/age_lr' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/.Trash/Current/user/maria.godgildieva/age_lr1616508174786\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -R age_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gender.save('gender_gbt')\n",
    "model_age.save('age_lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gender = GBTClassifier.load('gender_gbt')\n",
    "# model_age = LogisticRegression.load('age_gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/03/23 17:03:00 INFO fs.TrashPolicyDefault: Moved: 'hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/streaming/chk/chk_kafka' to trash at: hdfs://spark-de-master-1.newprolab.com:8020/user/maria.godgildieva/.Trash/Current/user/maria.godgildieva/streaming/chk/chk_kafka1616508180121\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R streaming/chk/chk_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(batch_df, epoch_id):\n",
    "    json_schema = StructType([StructField('uid', StringType()),\n",
    "                          StructField('visits', StringType())])\n",
    "    json_schema2 = ArrayType(StructType([StructField('url', StringType()),StructField('timestamp', StringType())]))\n",
    "\n",
    "    data = batch_df.withColumn('tmp', f.from_json('value', json_schema)) \\\n",
    "            .select(f.col('tmp.uid').alias('uid'), f.from_json('tmp.visits', json_schema2).alias('visits'))\n",
    "    tmp_df = data.select('uid', f.explode(f.col('visits')).alias('visits')) \\\n",
    "                .select('uid', f.col('visits').url.alias('url'), \n",
    "                        f.col('visits').timestamp.alias('timestamp').cast(LongType()))\n",
    "\n",
    "    clean_data = tmp_df.groupby(\"uid\").agg(f.concat_ws(\"|\", f.collect_list(tmp_df.url)).alias('urls'),\n",
    "                                    f.collect_list(tmp_df.timestamp).alias('timestamps'))\n",
    "    clean_data = pipeline.transform(clean_data)\n",
    "\n",
    "    pred = model_age.transform(model_gender.transform(clean_data))\n",
    "\n",
    "    age_labels = pipeline.stages[0].labels\n",
    "    gender_labels = pipeline.stages[1].labels\n",
    "    age_to_label = IndexToString(inputCol=\"age_pred\", outputCol=\"age\", labels=age_labels)\n",
    "    gender_to_label = IndexToString(inputCol=\"gender_pred\", outputCol=\"gender\", labels=gender_labels)\n",
    "    res = gender_to_label.transform(age_to_label.transform(pred)).select('uid', 'age', 'gender') \\\n",
    "                    .select(f.to_json(f.struct(f.col('*'))).alias('value'))\n",
    "    write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"maria.godgildieva\"\n",
    "    }\n",
    "    res.write\\\n",
    "     .format('kafka')\\\n",
    "     .options(**write_kafka_params)\\\n",
    "     .mode('append')\\\n",
    "     .save()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    return df.writeStream\\\n",
    "            .foreachBatch(foreach_batch_function)\\\n",
    "            .option('checkpointLocation', 'streaming/chk/chk_kafka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_maria.godgildieva\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "kafka_df = spark.readStream.format(\"kafka\").options(**read_kafka_params).option(\"failOnDataLoss\", 'False').load()\n",
    "kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = create_console_sink(kafka_df)\n",
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
