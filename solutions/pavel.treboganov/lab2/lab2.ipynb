{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 hdfs hdfs   69519728 2021-02-27 21:58 /labs/slaba02/DO_record_per_line.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba02/DO_record_per_line.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, DoubleType, StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"provider\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "|9/humanities|15/m...|This game-based c...|  9|  en|College Foundatio...|Canvas Network|\n",
      "|  14/social_sciences|What’s in your di...| 10|  en|Digital Literacies I|Canvas Network|\n",
      "|  14/social_sciences|The goal of the D...| 11|  en|Digital Literacie...|Canvas Network|\n",
      "|  14/social_sciences|Ready to explore ...| 12|  en|Digital Tools for...|Canvas Network|\n",
      "|  14/social_sciences|This self-paced c...| 13|  en|Discover Your Val...|Canvas Network|\n",
      "|  12/medicine_health|What is “interpro...| 14|  en|Enhancing Patient...|Canvas Network|\n",
      "|        16/languages|This course prese...| 15|  en|Ethics and Values...|Canvas Network|\n",
      "|         4/chemistry|Chemistry is an i...| 16|  en| Exploring Chemistry|Canvas Network|\n",
      "|8/engineering_tec...|Are you consideri...| 17|  en|Exploring Enginee...|Canvas Network|\n",
      "|   1/arts_music_film|Princess stories ...| 18|  en|Fairy Tales: Orig...|Canvas Network|\n",
      "|        9/humanities|This first instal...| 19|  en|First Peoples to ...|Canvas Network|\n",
      "|  14/social_sciences|This course exami...| 20|  en| Forums for a Future|Canvas Network|\n",
      "|        9/humanities|This course will ...| 21|  en|From the Gilded A...|Canvas Network|\n",
      "|8/engineering_tec...|The field of tech...| 22|  en|Fundamentals of S...|Canvas Network|\n",
      "|  14/social_sciences|Are you a Higher ...| 23|  en|Hybrid Courses: B...|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.json('/labs/slaba02/DO_record_per_line.json', schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- provider: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accounting'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "import re\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "string = df.take(1)[0].name\n",
    "regex.findall(string.lower())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accounting',\n",
       " 'cycle',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'of',\n",
       " 'business',\n",
       " 'measurement',\n",
       " 'and',\n",
       " 'reporting']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.findall(df.take(1)[0].name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'course', 'introduces', 'the', 'basic', 'financial', 'statements', 'used', 'by', 'most', 'businesses', 'as', 'well', 'as', 'the', 'essential', 'tools', 'used', 'to', 'prepare', 'them', 'this', 'course', 'will', 'serve', 'as', 'resource', 'to', 'help', 'business', 'students', 'succeed', 'in', 'their', 'upcoming', 'university', 'level', 'accounting', 'classes', 'and', 'as', 'refresher', 'for', 'upper', 'division', 'accounting', 'students', 'who', 'are', 'struggling', 'to', 'recall', 'elementary', 'concepts', 'essential', 'to', 'more', 'advanced', 'accounting', 'topics', 'business', 'owners', 'will', 'also', 'benefit', 'from', 'this', 'class', 'by', 'gaining', 'essential', 'skills', 'necessary', 'to', 'organize', 'and', 'manage', 'information', 'pertinent', 'to', 'operating', 'their', 'business', 'at', 'the', 'conclusion', 'of', 'the', 'class', 'students', 'will', 'understand', 'the', 'balance', 'sheet', 'income', 'statement', 'and', 'cash', 'flow', 'statement', 'they', 'will', 'be', 'able', 'to', 'differentiate', 'between', 'cash', 'basis', 'and', 'accrual', 'basis', 'techniques', 'and', 'know', 'when', 'each', 'is', 'appropriate', 'they', 'll', 'also', 'understand', 'the', 'accounting', 'equation', 'how', 'to', 'journalize', 'and', 'post', 'transactions', 'how', 'to', 'adjust', 'and', 'close', 'accounts', 'and', 'how', 'to', 'prepare', 'key', 'financial', 'reports', 'all', 'material', 'for', 'this', 'class', 'is', 'written', 'and', 'delivered', 'by', 'the', 'professor', 'and', 'can', 'be', 'previewed', 'here', 'students', 'must', 'have', 'access', 'to', 'spreadsheet', 'program', 'to', 'participate']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType, ArrayType, StringType\n",
    "from pyspark.sql.functions import lower, col\n",
    "import re\n",
    "\n",
    "regex = re.compile(u'[\\w\\d]{2,}', re.U)\n",
    "string=df.take(1)[0].name\n",
    "regex.findall(string.lower())\n",
    "\n",
    "def ppp2(string):\n",
    "     return regex.findall(string.lower())\n",
    "    \n",
    "\n",
    "print(ppp2(df.take(1)[0].desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_udfi = udf(ppp2, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23126\n",
      "21617\n",
      "16627\n",
      "11556\n",
      "16704\n",
      "13702\n"
     ]
    }
   ],
   "source": [
    "my_courses = [[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']]\n",
    "for i in my_courses:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|                 cat|                desc|   id|lang|                name|provider|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "|                    | La transformació...|11556|  es|Aprendizaje Colab...|   Udemy|\n",
      "|6/economics_finan...|Математическая эк...|13702|  ru|Математическая эк...|  Intuit|\n",
      "|                    | Hazte más emplea...|16627|  es|Aprende Excel: Ni...|   Udemy|\n",
      "|5/computer_scienc...|В курсе рассматри...|16704|  ru|Программирование ...|  Intuit|\n",
      "|  5/computer_science|An introduction t...|21617|  en|Preparing for the...|     edX|\n",
      "|                    | Improve your SAS...|23126|  en|Compass - powerfu...|   Udemy|\n",
      "+--------------------+--------------------+-----+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cours = df.where('id in (23126, 21617, 16627, 11556, 16704, 13702)')\n",
    "my_cours.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cours.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"desc\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|               words|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|[this, course, in...|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|[this, online, co...|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|[this, course, is...|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|[we, live, in, a,...|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|[this, self-paced...|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28153"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsData=wordsData.withColumn(\"count\", f.size(\"words\")).where(\"count<>0\")\n",
    "wordsData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"hTF\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"hTF\", outputCol=\"TFIDF\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2=df.select(\"id\",\"lang\",\"desc\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Base = spark.createDataFrame([[23126, u'en', u'Compass - powerful SASS library that makes your life easier'], [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']],schema=schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_join=_Base.select(\"id\").join(rescaledData.select(\"id\",\"lang\",\"TFIDF\"),\"id\", how=\"inner\").filter(col('id').isin([23126, 21617, 16627, 11556, 16704, 13702]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------------+\n",
      "|   id|lang|               TFIDF|\n",
      "+-----+----+--------------------+\n",
      "|13702|  ru|(10000,[310,763,9...|\n",
      "|16627|  es|(10000,[30,145,19...|\n",
      "|16704|  ru|(10000,[381,1144,...|\n",
      "|23126|  en|(10000,[87,91,96,...|\n",
      "|21617|  en|(10000,[213,360,4...|\n",
      "|11556|  es|(10000,[249,522,5...|\n",
      "+-----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=_join.join(rescaledData.select(col(\"id\").alias(\"_id\"),\"lang\",col(\"TFIDF\").alias(\"_TFIDF\")),\"lang\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf('double')\n",
    "def cos(x,y):\n",
    "    return float(x.dot(y) / (x.norm(2) * y.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=join.withColumn('cos',cos(\"TFIDF\",\"_TFIDF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+-----+--------------------+------------------+\n",
      "|lang|   id|               TFIDF|  _id|              _TFIDF|               cos|\n",
      "+----+-----+--------------------+-----+--------------------+------------------+\n",
      "|  en|23126|(10000,[87,91,96,...|23126|(10000,[87,91,96,...|1.0000000000000002|\n",
      "|  es|16627|(10000,[30,145,19...|16627|(10000,[30,145,19...|1.0000000000000002|\n",
      "|  ru|13702|(10000,[310,763,9...|13702|(10000,[310,763,9...|1.0000000000000002|\n",
      "|  ru|16704|(10000,[381,1144,...|16704|(10000,[381,1144,...|               1.0|\n",
      "|  es|11556|(10000,[249,522,5...|11556|(10000,[249,522,5...|               1.0|\n",
      "|  en|21617|(10000,[213,360,4...|21617|(10000,[213,360,4...|0.9999999999999999|\n",
      "+----+-----+--------------------+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.where(\"id=_id\").orderBy(\"cos\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=res.select(\"lang\",\"id\",\"_id\",\"TFIDF\",\"_TFIDF\",\"cos\").where(\"id<>_id\").orderBy(\"id\",\"cos\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54256"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_out=res.select(\"id\",\"_id\",\"cos\", f.row_number().over(Window.partitionBy(\"id\").orderBy(col(\"cos\").desc())).alias(\"N\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_out=res_out.where(\"N<=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+---+\n",
      "|   id|  _id|                cos|  N|\n",
      "+-----+-----+-------------------+---+\n",
      "|16704| 1365|0.30737172340483715|  1|\n",
      "|16704|20645|0.29816668697589227|  2|\n",
      "|16704|20105|  0.296335695787284|  3|\n",
      "|16704| 1426|  0.296335695787284|  4|\n",
      "|16704| 8217|0.28745405355016035|  5|\n",
      "|16704| 1236| 0.2865114161415227|  6|\n",
      "|16704| 1164| 0.2823455375759388|  7|\n",
      "|16704| 1219|0.27430304672152866|  8|\n",
      "|16704| 8123|0.27147713280944274|  9|\n",
      "|16704|  875|0.27119516453913817| 10|\n",
      "+-----+-----+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_out.where(\"id=16704\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  _id|\n",
      "+-----+\n",
      "| 1365|\n",
      "|20645|\n",
      "|20105|\n",
      "| 1426|\n",
      "| 8217|\n",
      "| 1236|\n",
      "| 1164|\n",
      "| 1219|\n",
      "| 8123|\n",
      "|  875|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_out.where(\"id=16704\").select(\"_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'flatMap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-59cd12cde060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'flatMap'"
     ]
    }
   ],
   "source": [
    "res_out.groupBy('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, _id: int, cos: double, N: int]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_out.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_res = res_out.groupby(col('id')).agg(F.collect_list(col('_id')).alias('merged')).take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[864, 1216, 7173, 8313, 1052, 17017, 19613, 21017, 17015, 8082]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_json = {}\n",
    "for elem in list_res:\n",
    "    final_json[elem[0]] = elem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13702: [864, 1216, 7173, 8313, 1052, 17017, 19613, 21017, 17015, 8082],\n",
       " 16627: [11431, 12247, 13021, 25010, 11575, 5687, 9598, 5372, 12863, 16769],\n",
       " 16704: [1365, 20645, 1426, 20105, 8217, 1236, 1164, 1219, 8123, 875],\n",
       " 23126: [13782, 13665, 24419, 20638, 2724, 25782, 2633, 2723, 13348, 15909],\n",
       " 21617: [21609, 21608, 21616, 21492, 21624, 21623, 21630, 21628, 21508, 21857],\n",
       " 11556: [16488, 13461, 468, 10447, 387, 22710, 9289, 5936, 23357, 7833]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab02.json', 'w') as f:\n",
    "    json.dump(final_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
