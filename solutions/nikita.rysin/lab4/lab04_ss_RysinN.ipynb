{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лаба 4. Прогнозирование пола и возрастной категории — Spark Streaming\n",
    "\n",
    "#### Дедлайн\n",
    "\n",
    "Понедельник, 22 марта, 23:59.\n",
    "\n",
    "#### Дедлайн Github\n",
    "\n",
    "Четверг, 25 марта, 23:59\n",
    "\n",
    "#### Задача\n",
    "\n",
    "У вас имеются данные: логи посещения пользователей по разным сайтам рунета. По этим пользователям вам также известны: пол и возрастная категория. Вам нужно будет в real-time спрогнозировать эти характеристики по пользователям, о которых у вас нет этой информации, но будут все те же самые логи посещения.\n",
    "\n",
    "##### При решении задачи запрещено использовать библиотеки pandas, sklearn (кроме sklearn.metrics), xgboost и прочие.\n",
    "\n",
    "#### Описание данных\n",
    "\n",
    "Обучающая выборка, с которой вы будете работать, выглядит следующим образом:\n",
    "\n",
    "```json\n",
    "gender\tage\tuid\tuser_json\n",
    "F\t18-24\td50192e5-c44e-4ae8-ae7a-7cfe67c8b777\t{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}\n",
    "```\n",
    "\n",
    "Она расположена на hdfs: `/labs/slaba04/`.\n",
    "\n",
    "Поле `gender` принимает значения `F` (женщина) и `M` (мужчина).\n",
    "\n",
    "Поле `age` принимает значения диапазона возраста: `18-24`, `25-34`, `35-44`, `45-54`, `>=55`\n",
    "\n",
    "Поле `uid` принимает значения уникального ID пользователя (cookies): `d50192e5-c44e-4ae8-ae7a-7cfe67c8b777`.\n",
    "\n",
    "Поле `user_json` имеет внутри json со следующей схемой данных: `{\"visits\": [{\"url\": \"url1\", \"timestamp\": \"timestamp1\"}, {\"url\": \"url2\", \"timestamp\": \"timestamp2\"}]}`. В нем содержатся непосредственно логи посещения пользователем страниц вместе с временной меткой посещения.\n",
    "\n",
    "Данные, по которым вам надо будет построить прогноз (тестовая выборка), хранятся в Kafka в таком виде:\n",
    "\n",
    "```json\n",
    "{\"uid\": \"bdd48781-8243-493d-8ae6-794050a4417f\",\n",
    " \"visits\": \"[{\"url\": \"http://vk.com/feed\", \"timestamp\": 1425077901001}, {\"url\": \"http://big-cards.a5ltd.com/app/html/vk.html?api_url=http://api.vk.com/api.php&api_id=1804162&api_settings=663823&viewer_id=40849488&viewer_type=2&sid=1b77e9ca975573fed6e31746ca58ea2509f3588d918e7dab2a291f7f7579ae7dab6c21058c837b74b1fbb&secret=37339dfcbf&access_token=06c53bd8f9f9a0afad23b4e166e448495396259fc5a1c5e5243474df82591bd5d0afe877d993ea4a82a81&user_id=40849488&group_id=0&is_app_user=1&auth_key=d7c91ad4bb27e20cb3ed81810f5649db&language=0&parent_language=0&ad_info=elsdcqvcsvftaqxtawjsxht b0q8htjxuvbbjrvbnwojfji2ha8h&is_secure=0&ads_app_id=1804162_903aa103fa48b1e378&referrer=menu&lc_name=2c073eed&hash=\", \"timestamp\": 1425077901000}]\"}\n",
    "```\n",
    "\n",
    "То есть все то же самое, только без поля `gender` и `age` и немного с другой схемой данных. Название топика, в который мы будем присылать данные: `input_ivan.ivanov`, где вместо `ivan.ivanov` ваш логин от личного кабинета. Данные вам будут поступать по нажатию кнопки \"Проверить\" в личном кабинете. Вам топик создавать не нужно. Он будет автоматом создаваться при записи. Чистить его тоже не надо, потому что Spark Streaming хранит оффсеты, то есть знает, что он уже считывал, что еще не считывал. \n",
    "\n",
    "Порт брокера Kafka: `6667`. Hostname: `spark-master-1.newprolab.com` или `spark-node-1.newprolab.com`.\n",
    "\n",
    "#### Результат\n",
    "\n",
    "Вам в свою очередь нужно будет, считывая данные из Kafka, делать прогноз по ним в Spark Streaming и отправлять в real-time эти прогнозные значения в свой другой топик в Kafka в формате:\n",
    "\n",
    "```json\n",
    "{\"uid\": \"fe1dba8f-3131-439f-9031-851c0da0f126\", \"gender\": \"M\", \"age\": \"25-34\"}\n",
    "{\"uid\": \"d50192e5-c44e-4ae8-ae7a-7cfe67c8b777\", \"gender\": \"F\", \"age\": \"18-24\"}\n",
    "```\n",
    "\n",
    "Название топика — ваш логин в личном кабинете: `ivan.ivanov`. После отправки данных чекер будет ждать 90 секунд, чтобы вы обработали входные данные из топика `input_ivan.ivanov` и записали результаты прогноза уже в этот топик. \n",
    "\n",
    "#### Подсказки\n",
    "\n",
    "Чтение из Kafka:\n",
    "\n",
    "```python\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_ivan.ivanov\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "```\n",
    "\n",
    "Запись в Kafka:\n",
    "\n",
    "```python\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"ivan.ivanov\"\n",
    "}\n",
    "batch_df.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()\n",
    "```\n",
    "  \n",
    "Бывают проблемы при чтении старых данных, которых уже нет в кафке:\n",
    "  \n",
    "```\n",
    ".option(\"failOnDataLoss\", 'False')\n",
    "```\n",
    "  \n",
    "#### Проверка\n",
    "Проверка лабы устроена следующим образом:\n",
    "1. Вы готовите свою модель\n",
    "2. Настраиваете чтение данных из одного топика и запись в другой топик.\n",
    "4. Запускаете свой скрипт в стриминговым режиме\n",
    "3. Идет подключение к топику на чтение данных.\n",
    "4. На странице лабы нажимаете кнопку \"Проверить\".\n",
    "5. Вам в топик текут данные.\n",
    "6. Модель обрабатывает входные данные.\n",
    "7. Ваш скрипт сохраняет данные в другой топик.\n",
    "8. Чекер на странице лабы выдает результат проверки.\n",
    "\n",
    "\n",
    "Проверка осуществляется автоматическим скриптом на странице лабы в личном кабинете. Наш чекер в конечном итоге подключится к вашему финальному топику в Kafka и сравнит ваш прогноз с эталонным ответом. На выходе вы получите свой score. В качестве него метрика `accuracy`: доля пользователей от общего числа, по которым вы верно угадали и пол, и возрастную категорию. Если `accuracy` будет больше **0.25**, то лаба будет засчитана.\n",
    "\n",
    "##### Особенности проверки\n",
    "1. Проверка (генерация событий, обработка вашим скриптом, чтение и проверка результатов) занимают до 90 секунд, просьба ожидать (можно нажимать кнопку \"обновить\")\n",
    "2. За один раз (одно нажание \"Проверить\") чекер присылает батч в 5000 событий. Если для отладки / повторной проверки требуется еще, надо нажимать снова\n",
    "\n",
    "####  Рекомендации по прохождению\n",
    "1. Разработанную модель (и вообще весь пайплайн) можно сохранить в HDFS и потом загрузить уже в стриминговом скрипте (методы save/load - см.документацию)\n",
    "2. Настоятельно рекомендуется сначала отладить весь процесс чтения данных, предсказания и сохранения результатов в **батчевом** режиме (сгенерить порцию событий чекером и потом считать ее в обычный DataFrame через  `spark.read.format(\"kafka\")`. И уже только после проверки полной  работоспособсности всего скрипта и корректности формата выходных данных в Кафке, переписать на стриминговый режим (благодаря единому API Spark, это не потребует больших изменений)\n",
    "\n",
    "Обязательное условие зачета лабораторной работы – это выкладка после дедлайна лабы своего решения в репозиторий через pull-request. Как это сделать, можно прочитать [здесь](/git.md). Если будут вопросы – спрашивайте в Slack.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'spark-node-1.newprolab.com:6667' \n",
    "'spark-master-1.newprolab.com:6667'  \n",
    "'spark-de-node-1.newprolab.com:6667' \n",
    "'spark-de-master-1.newprolab.com:6667'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"lab04_RysinN\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-4.newprolab.com:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9721378278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, HashingTF, RegexTokenizer, StopWordsRemover, IDF, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml import Transformer\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_data = '/labs/slaba04/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2021-02-27 22:13 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655GB\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -du -s /labs/slaba04/  | awk '{s+=$1} END {printf \"%.3fGB\\n\", s/1000000000}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('gender', StringType()), \n",
    "                     StructField('age', StringType()),\n",
    "                     StructField('uid', StringType()),\n",
    "                     StructField('user_json', StringType())\n",
    "                    ]\n",
    "                   )\n",
    "\n",
    "train_data = spark.read.format(\"csv\")\\\n",
    "                       .option(\"inferSchema\", \"true\")\\\n",
    "                       .schema(schema)\\\n",
    "                       .option(\"header\", \"true\")\\\n",
    "                       .option(\"delimiter\", \"\\\\t\")\\\n",
    "                       .load(path_to_train_data)\n",
    "\n",
    "train_data = train_data.filter('age != \"-\" and gender != \"-\"')\n",
    "\n",
    "visits_schema = StructType([\n",
    "    StructField(\"visits\", ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"url\", StringType()),\n",
    "          StructField(\"timestamp\", LongType())\n",
    "      ])\n",
    "   ))\n",
    "]) \n",
    "\n",
    "train_data = train_data.withColumn('visits', \n",
    "                                   F.from_json(F.col('user_json'),\n",
    "                                               schema=visits_schema\n",
    "                                              )\n",
    "                                  )\n",
    "\n",
    "train_data = train_data.withColumn('visited_pages', F.col('visits.visits.url'))\n",
    "\n",
    "train_data = train_data.withColumn('is_M', F.when(F.col(\"gender\") == 'M', 1).otherwise(0))\n",
    "\n",
    "train_data = train_data.withColumn(\"age_category\", F.when(F.col(\"age\") == '18-24', 0)\\\n",
    "                                                    .when(F.col(\"age\") == '25-34', 1)\\\n",
    "                                                    .when(F.col(\"age\") == '35-44', 2)\\\n",
    "                                                    .when(F.col(\"age\") == '45-54', 3)\\\n",
    "                                                    .otherwise(4)\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_data.groupby('age_gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- visits: struct (nullable = true)\n",
      " |    |-- visits: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- timestamp: long (nullable = true)\n",
      " |-- visited_pages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- is_M: integer (nullable = false)\n",
      " |-- age_category: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+--------------------+----+------------+\n",
      "|gender|  age|                 uid|           user_json|              visits|       visited_pages|is_M|age_category|\n",
      "+------+-----+--------------------+--------------------+--------------------+--------------------+----+------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|[[[http://zebra-z...|[http://zebra-zoy...|   0|           0|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|[[[http://sweetra...|[http://sweetradi...|   1|           1|\n",
      "|     F|25-34|d50237ea-747e-48a...|{\"visits\": [{\"url...|[[[http://ru.orif...|[http://ru.orifla...|   0|           1|\n",
      "|     F|25-34|d502f29f-d57a-46b...|{\"visits\": [{\"url...|[[[http://transla...|[http://translate...|   0|           1|\n",
      "|     M| >=55|d503c3b2-a0c2-4f4...|{\"visits\": [{\"url...|[[[https://mail.r...|[https://mail.ram...|   1|           4|\n",
      "+------+-----+--------------------+--------------------+--------------------+--------------------+----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(StringType()))\n",
    "def extract_key_words(user_sites):\n",
    "    sites_cleaned = [re.sub(r'(http://|https://|www)', '', site) for site in user_sites]    \n",
    "    sites_cleaned = sum([re.findall(r'\\w+', site) for site in sites_cleaned], [])\n",
    "    sites_cleaned = [site for site in sites_cleaned if site.isalpha()]\n",
    "    return sites_cleaned\n",
    "\n",
    "class CleanSitesTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CleanSitesTransformer, self).__init__()\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "            \n",
    "    def _transform(self, dataset):\n",
    "        return dataset.withColumn(self.getOutputCol(), extract_key_words(F.col(self.getInputCol())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_cleaner = CleanSitesTransformer(inputCol=\"visited_pages\", outputCol=\"sites_words\")\n",
    "\n",
    "en_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "remover = StopWordsRemover(inputCol=\"sites_words\",\n",
    "                           outputCol=\"sites_words_filtered\",\n",
    "                           stopWords=en_stopwords)\n",
    "\n",
    "tf = HashingTF(inputCol=\"sites_words_filtered\", outputCol=\"tf\", numFeatures=15000)\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "\n",
    "scaler = StandardScaler()\\\n",
    "         .setInputCol(\"tf_idf\")\\\n",
    "         .setOutputCol(\"tf_idf_norm\")\n",
    "\n",
    "\n",
    "gender_model = LogisticRegression(featuresCol='tf_idf_norm', \n",
    "                                  rawPredictionCol='rawPrediction_gender', \n",
    "                                  predictionCol='prediction_gender', \n",
    "                                  labelCol='is_M', \n",
    "                                  maxIter=25)\n",
    "\n",
    "\n",
    "pipeline1 = Pipeline(stages=[transformer_cleaner, remover, tf, idf, scaler, gender_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model = LogisticRegression(featuresCol='tf_idf_norm', \n",
    "                               rawPredictionCol='rawPrediction_age', \n",
    "                               predictionCol='prediction_age', \n",
    "                               labelCol='age_category', \n",
    "                               maxIter=25, \n",
    "                               regParam=0.3)\n",
    "\n",
    "pipeline2 = Pipeline(stages=[age_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline1 = pipeline1.fit(train_data)\n",
    "scored_train_data = pipeline1.transform(train_data)\n",
    "scored_train_data = scored_train_data.select([col for col in scored_train_data.columns if col != \"probability\"])\n",
    "pipeline2 = pipeline2.fit(scored_train_data)\n",
    "scored_train_data = pipeline2.transform(scored_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- user_json: string (nullable = true)\n",
      " |-- visits: struct (nullable = true)\n",
      " |    |-- visits: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- timestamp: long (nullable = true)\n",
      " |-- visited_pages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- is_M: integer (nullable = false)\n",
      " |-- age_category: integer (nullable = false)\n",
      " |-- sites_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sites_words_filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- tf_idf_norm: vector (nullable = true)\n",
      " |-- rawPrediction_gender: vector (nullable = true)\n",
      " |-- prediction_gender: double (nullable = false)\n",
      " |-- rawPrediction_age: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction_age: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " gender               | F                    \n",
      " age                  | 18-24                \n",
      " uid                  | d50192e5-c44e-4ae... \n",
      " user_json            | {\"visits\": [{\"url... \n",
      " visits               | [[[http://zebra-z... \n",
      " visited_pages        | [http://zebra-zoy... \n",
      " is_M                 | 0                    \n",
      " age_category         | 0                    \n",
      " sites_words          | [zebra, zoya, ru,... \n",
      " sites_words_filtered | [zebra, zoya, ru,... \n",
      " tf                   | (15000,[205,1297,... \n",
      " tf_idf               | (15000,[205,1297,... \n",
      " tf_idf_norm          | (15000,[205,1297,... \n",
      " rawPrediction_gender | [4.37902049421712... \n",
      " prediction_gender    | 0.0                  \n",
      " rawPrediction_age    | [0.02435235399395... \n",
      " probability          | [0.16114914011656... \n",
      " prediction_age       | 1.0                  \n",
      "-RECORD 1------------------------------------\n",
      " gender               | M                    \n",
      " age                  | 25-34                \n",
      " uid                  | d502331d-621e-472... \n",
      " user_json            | {\"visits\": [{\"url... \n",
      " visits               | [[[http://sweetra... \n",
      " visited_pages        | [http://sweetradi... \n",
      " is_M                 | 1                    \n",
      " age_category         | 1                    \n",
      " sites_words          | [sweetrading, ru,... \n",
      " sites_words_filtered | [sweetrading, ru,... \n",
      " tf                   | (15000,[181,383,4... \n",
      " tf_idf               | (15000,[181,383,4... \n",
      " tf_idf_norm          | (15000,[181,383,4... \n",
      " rawPrediction_gender | [-30.435556905625... \n",
      " prediction_gender    | 1.0                  \n",
      " rawPrediction_age    | [-0.5202230643009... \n",
      " probability          | [0.04401568472077... \n",
      " prediction_age       | 1.0                  \n",
      "-RECORD 2------------------------------------\n",
      " gender               | F                    \n",
      " age                  | 25-34                \n",
      " uid                  | d50237ea-747e-48a... \n",
      " user_json            | {\"visits\": [{\"url... \n",
      " visits               | [[[http://ru.orif... \n",
      " visited_pages        | [http://ru.orifla... \n",
      " is_M                 | 0                    \n",
      " age_category         | 1                    \n",
      " sites_words          | [ru, oriflame, co... \n",
      " sites_words_filtered | [ru, oriflame, co... \n",
      " tf                   | (15000,[748,1359,... \n",
      " tf_idf               | (15000,[748,1359,... \n",
      " tf_idf_norm          | (15000,[748,1359,... \n",
      " rawPrediction_gender | [5.21381196102329... \n",
      " prediction_gender    | 0.0                  \n",
      " rawPrediction_age    | [-0.3537760662895... \n",
      " probability          | [0.09046667962444... \n",
      " prediction_age       | 1.0                  \n",
      "-RECORD 3------------------------------------\n",
      " gender               | F                    \n",
      " age                  | 25-34                \n",
      " uid                  | d502f29f-d57a-46b... \n",
      " user_json            | {\"visits\": [{\"url... \n",
      " visits               | [[[http://transla... \n",
      " visited_pages        | [http://translate... \n",
      " is_M                 | 0                    \n",
      " age_category         | 1                    \n",
      " sites_words          | [translate, tatto... \n",
      " sites_words_filtered | [translate, tatto... \n",
      " tf                   | (15000,[246,456,5... \n",
      " tf_idf               | (15000,[246,456,5... \n",
      " tf_idf_norm          | (15000,[246,456,5... \n",
      " rawPrediction_gender | [1.36960596290691... \n",
      " prediction_gender    | 0.0                  \n",
      " rawPrediction_age    | [-0.1945252622244... \n",
      " probability          | [0.11552431953180... \n",
      " prediction_age       | 1.0                  \n",
      "-RECORD 4------------------------------------\n",
      " gender               | M                    \n",
      " age                  | >=55                 \n",
      " uid                  | d503c3b2-a0c2-4f4... \n",
      " user_json            | {\"visits\": [{\"url... \n",
      " visits               | [[[https://mail.r... \n",
      " visited_pages        | [https://mail.ram... \n",
      " is_M                 | 1                    \n",
      " age_category         | 4                    \n",
      " sites_words          | [mail, rambler, r... \n",
      " sites_words_filtered | [mail, rambler, r... \n",
      " tf                   | (15000,[280,439,4... \n",
      " tf_idf               | (15000,[280,439,4... \n",
      " tf_idf_norm          | (15000,[280,439,4... \n",
      " rawPrediction_gender | [-17.259139841091... \n",
      " prediction_gender    | 1.0                  \n",
      " rawPrediction_age    | [-0.3964525742699... \n",
      " probability          | [0.12454377300331... \n",
      " prediction_age       | 1.0                  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scored_train_data.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "    \"subscribe\": \"input_nikita.rysin\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "   \"topic\": \"nikita.rysin\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = spark.read.format(\"kafka\").options(**read_kafka_params).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "| key|               value|             topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60000|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60001|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60002|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60003|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60004|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60005|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60006|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60007|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60008|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60009|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60010|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60011|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60012|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60013|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60014|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60015|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60016|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60017|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60018|2021-03-24 17:41:...|            0|\n",
      "|null|[7B 22 75 69 64 2...|input_nikita.rysin|        0| 60019|2021-03-24 17:41:...|            0|\n",
      "+----+--------------------+------------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data_schema = StructType([\n",
    "    StructField('uid', StringType()),\n",
    "    StructField('visits', StringType())\n",
    "]\n",
    ")\n",
    "\n",
    "scoring_data_visits_schema = ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"url\", StringType()),\n",
    "          StructField(\"timestamp\", LongType())\n",
    "      ])\n",
    "   )\n",
    "\n",
    "def scoring_batch(batch_df):\n",
    "    scoring_data = batch_df.withColumn('weblog', F.from_json(F.col('value').cast(StringType()), \n",
    "                                                          scoring_data_schema)\n",
    "                                   ) \\\n",
    "                        .select('weblog.*') \\\n",
    "                        .select('uid', F.from_json(F.col('visits'), \n",
    "                                                   scoring_data_visits_schema).alias('visits')\n",
    "                               )\n",
    "    \n",
    "    scoring_data = scoring_data.withColumn('visited_pages', F.col('visits.url'))\n",
    "    \n",
    "    scoring_data = pipeline1.transform(scoring_data)\n",
    "    \n",
    "    scoring_data = scoring_data.select([col for col in scoring_data.columns if col != \"probability\"])\n",
    "    \n",
    "    scoring_data = pipeline2.transform(scoring_data)\n",
    "    \n",
    "    scoring_data = scoring_data.withColumn('gender', F.when(F.col(\"prediction_gender\") == 1, 'M').otherwise('F'))\n",
    "\n",
    "    scoring_data = scoring_data.withColumn(\"age\", F.when(F.col(\"prediction_age\") == 0, '18-24')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 1, '25-34')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 2, '35-44')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 3, '45-54')\\\n",
    "                                                   .otherwise('>=55')\n",
    "                                  )\n",
    "    \n",
    "    scoring_data = scoring_data.select('uid', 'gender', 'age')\n",
    "    \n",
    "    scoring_data = scoring_data.select(F.to_json(F.struct([scoring_data[x] for x in scoring_data.columns])) \\\n",
    "                               .cast(StringType()).alias(\"value\")\n",
    "                                      )\n",
    "    return scoring_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scoring_batch(kafka_sdf).show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_batch(kafka_sdf).write.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"kafka\").options(**{\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "    \"subscribe\": \"nikita.rysin\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}).load().select(F.col('value').cast(StringType()).alias('row')).show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streaming scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "    \"subscribe\": \"input_nikita.rysin\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "   \"topic\": \"nikita.rysin\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data_schema = StructType([\n",
    "    StructField('uid', StringType()),\n",
    "    StructField('visits', StringType())\n",
    "]\n",
    ")\n",
    "\n",
    "scoring_data_visits_schema = ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"url\", StringType()),\n",
    "          StructField(\"timestamp\", LongType())\n",
    "      ])\n",
    "   )\n",
    "\n",
    "def scoring_batch(batch_df, epoch_id):\n",
    "    scoring_data = batch_df.withColumn('weblog', F.from_json(F.col('value').cast(StringType()), \n",
    "                                                          scoring_data_schema)\n",
    "                                   ) \\\n",
    "                        .select('weblog.*') \\\n",
    "                        .select('uid', F.from_json(F.col('visits'), \n",
    "                                                   scoring_data_visits_schema).alias('visits')\n",
    "                               )\n",
    "    \n",
    "    scoring_data = scoring_data.withColumn('visited_pages', F.col('visits.url'))\n",
    "    \n",
    "    scoring_data = pipeline1.transform(scoring_data)\n",
    "    \n",
    "    scoring_data = scoring_data.select([col for col in scoring_data.columns if col != \"probability\"])\n",
    "    \n",
    "    scoring_data = pipeline2.transform(scoring_data)\n",
    "    \n",
    "    scoring_data = scoring_data.withColumn('gender', F.when(F.col(\"prediction_gender\") == 1, 'M').otherwise('F'))\n",
    "\n",
    "    scoring_data = scoring_data.withColumn(\"age\", F.when(F.col(\"prediction_age\") == 0, '18-24')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 1, '25-34')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 2, '35-44')\\\n",
    "                                                   .when(F.col(\"prediction_age\") == 3, '45-54')\\\n",
    "                                                   .otherwise('>=55')\n",
    "                                  )\n",
    "    \n",
    "    scoring_data = scoring_data.select('uid', 'gender', 'age')\n",
    "    \n",
    "    scoring_data = scoring_data.select(\n",
    "        F.to_json(\n",
    "            F.struct([scoring_data[x] for x in scoring_data.columns])\n",
    "        ).alias(\"value\") #.cast(StringType())\n",
    "                                      )\n",
    "    scoring_data.write\\\n",
    "     .format('kafka')\\\n",
    "     .options(**write_kafka_params)\\\n",
    "     .mode('append')\\\n",
    "     .save()\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    return df.writeStream\\\n",
    "            .foreachBatch(scoring_batch)\\\n",
    "            .option('checkpointLocation', 'streaming/chk/chk_kafka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream.format(\"kafka\").options(**read_kafka_params).option(\"failOnDataLoss\", \"False\").load()\n",
    "#kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = create_console_sink(kafka_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sq = sink.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(sq.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------\n",
      " row       | {\"uid\":\"bd7a30e1-a25d-4cbf-a03f-61748cbe540e\",\"gender\":\"M\",\"age\":\"35-44\"} \n",
      " timestamp | 2021-03-24 18:23:37.502                                                   \n",
      "-RECORD 1------------------------------------------------------------------------------\n",
      " row       | {\"uid\":\"bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd\",\"gender\":\"F\",\"age\":\"25-34\"} \n",
      " timestamp | 2021-03-24 18:23:37.691                                                   \n",
      "-RECORD 2------------------------------------------------------------------------------\n",
      " row       | {\"uid\":\"bd7a7fd9-ab06-42f5-bf0f-1cbb0463004c\",\"gender\":\"M\",\"age\":\"25-34\"} \n",
      " timestamp | 2021-03-24 18:23:37.708                                                   \n",
      "-RECORD 3------------------------------------------------------------------------------\n",
      " row       | {\"uid\":\"bd7c5d7a-0def-41d1-895f-fdb96c56c2d4\",\"gender\":\"M\",\"age\":\"35-44\"} \n",
      " timestamp | 2021-03-24 18:23:37.727                                                   \n",
      "-RECORD 4------------------------------------------------------------------------------\n",
      " row       | {\"uid\":\"bd7e54a2-0215-45cb-a869-9efebf250e38\",\"gender\":\"F\",\"age\":\"25-34\"} \n",
      " timestamp | 2021-03-24 18:23:37.738                                                   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .select(F.col('value').cast(StringType()).alias('row'))\n",
    "spark.read.format(\"kafka\").options(**{\n",
    "    \"kafka.bootstrap.servers\": \"spark-de-node-1.newprolab.com:6667\",\n",
    "    \"subscribe\": \"nikita.rysin\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}).load().select(F.col('value').cast(StringType()).alias('row'), 'timestamp').show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
