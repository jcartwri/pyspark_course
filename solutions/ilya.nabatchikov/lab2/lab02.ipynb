{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import HashingTF, RegexTokenizer, IDF, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Nabat_lab2\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Nabat_lab2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"lang\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"cat\", StringType()),\n",
    "    StructField(\"provider\", StringType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"desc\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/labs/slaba02/DO_record_per_line.json\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"desc\", outputCol=\"words\", pattern=\"[^a-z^а-я]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reg_t = regexTokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000000)\n",
    "featurizedData  = hashingTF.transform(df_reg_t)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "l1NormData = normalizer.transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data = l1NormData.select('lang', 'name','id','normFeatures').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dot(v1, v2):\n",
    "    return float(v1.dot(v2))\n",
    "dotUdf = f.udf(my_dot, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_cources1 = collect_data.filter(f.col('id') == \"23126\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()\n",
    "\n",
    "df_my_cources2 = collect_data.filter(f.col('id') == \"21617\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()\n",
    "\n",
    "df_my_cources3 = collect_data.filter(f.col('id') == \"16627\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()\n",
    "\n",
    "df_my_cources4 = collect_data.filter(f.col('id') == \"11556\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()\n",
    "df_my_cources5 = collect_data.filter(f.col('id') == \"16704\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()\n",
    "df_my_cources6 = collect_data.filter(f.col('id') == \"13702\")\\\n",
    "    .select('lang','id','normFeatures')\\\n",
    "    .withColumnRenamed('id', 'id_join')\\\n",
    "    .withColumnRenamed('normFeatures', 'features_point')\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data1 = collect_data.join(df_my_cources1, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()\n",
    "\n",
    "collect_data2 = collect_data.join(df_my_cources2, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()\n",
    "\n",
    "collect_data3 = collect_data.join(df_my_cources3, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()\n",
    "\n",
    "collect_data4 = collect_data.join(df_my_cources4, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()\n",
    "\n",
    "collect_data5 = collect_data.join(df_my_cources5, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()\n",
    "\n",
    "collect_data6 = collect_data.join(df_my_cources6, collect_data.lang == df_my_cources1.lang)\\\n",
    "    .select(\"name\", \"id\", \"normFeatures\", \"id_join\", \"features_point\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itog = collect_data1.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\\\n",
    "    .union(\n",
    "        collect_data2.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\n",
    "    )\\\n",
    "    .union(\n",
    "        collect_data3.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\n",
    "    )\\\n",
    "    .union(\n",
    "        collect_data4.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\n",
    "    )\\\n",
    "    .union(\n",
    "        collect_data5.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\n",
    "    )\\\n",
    "    .union(\n",
    "        collect_data6.withColumn('cosine_sim', dotUdf('normFeatures','features_point'))\\\n",
    "    .orderBy(f.desc(\"cosine_sim\"), \"name\", \"id\").limit(11).select(\"id\", \"id_join\")\n",
    "    )\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finally = df_itog.orderBy(\"id_join\").where(df_itog.id != df_itog.id_join).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_finally.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "sub_list = list()\n",
    "for id_ , id_join in res:\n",
    "    sub_list.append(int(id_))\n",
    "    if i%10 == 0:\n",
    "        d[id_join] = sub_list\n",
    "        sub_list = list()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/home/ilya.nabatchikov/lab02.json', 'w') as fp:\n",
    "    json.dump(d, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
