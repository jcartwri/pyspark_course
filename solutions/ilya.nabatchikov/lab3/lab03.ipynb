{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, Model\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, CountVectorizer,  StringIndexer, StandardScaler, VectorIndexer\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, LongType, FloatType, ArrayType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.param import Param, Params, TypeConverters\n",
    "from pyspark.ml.param.shared import HasFeaturesCol, HasLabelCol, HasPredictionCol\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = SparkConf()\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=spark_config)\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/labs/slaba03/laba03_train.csv'\n",
    "test_path = '/labs/slaba03/laba03_test.csv'\n",
    "items_path = '/labs/slaba03/laba03_items.csv'\n",
    "users_path = '/labs/slaba03/laba03_views_programmes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                              StructField('item_id', IntegerType()),\n",
    "                                              StructField('purchase', IntegerType(), nullable=True), \n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType(fields=[StructField('item_id', IntegerType()), \n",
    "                                       StructField('channel_id', IntegerType()),\n",
    "                                       StructField('datetime_availability_start', StringType()),\n",
    "                                       StructField('datetime_availability_stop', StringType()),\n",
    "                                       StructField('datetime_show_start', StringType()),\n",
    "                                       StructField('datetime_show_stop', StringType()),\n",
    "                                       StructField('content_type', IntegerType()),\n",
    "                                       StructField('title', StringType(), nullable=True),\n",
    "                                       StructField('year', FloatType(), nullable=True),\n",
    "                                       StructField('genres', StringType()),\n",
    "                                       StructField('region_id', IntegerType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = StructType(fields=[StructField('user_id', IntegerType()), \n",
    "                                       StructField('item_id', IntegerType()),\n",
    "                                       StructField('ts_start', IntegerType()),\n",
    "                                       StructField('ts_end', IntegerType()),\n",
    "                                       StructField('item_type', StringType()),\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(main_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(train_path)\\\n",
    "                .cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(main_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(test_path)\\\n",
    "                 .cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(items_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"delimiter\", \"\\\\t\")\\\n",
    "                .load(items_path)\\\n",
    "                 .select(['item_id', 'content_type', 'year', 'genres'])\\\n",
    "                 .na.fill({'year': -999, 'genres': 'unknown'}).cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users =  spark.read\\\n",
    "                .format('csv')\\\n",
    "                .schema(users_schema)\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(users_path)\\\n",
    "                .cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.join(df_items[[\"item_id\",\"content_type\", \"year\", \"genres\"]], [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_user_purchase=df_train.groupBy(\"user_id\").mean(\"purchase\")\n",
    "df_mean_item_purchase=df_train.groupBy(\"item_id\").mean(\"purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_user_purchase=df_mean_user_purchase.withColumnRenamed(\"avg(purchase)\",\"mean_user_purchase\")\n",
    "df_mean_item_purchase=df_mean_item_purchase.withColumnRenamed(\"avg(purchase)\",\"mean_item_purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.join(df_mean_user_purchase, [\"user_id\"])\n",
    "df_train=df_train.join(df_mean_item_purchase, [\"item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.join(df_mean_user_purchase, [\"user_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_user_purchase\")\n",
    "df_test=df_test.join(df_mean_item_purchase, [\"item_id\"])\n",
    "df_test=df_test.withColumnRenamed(\"avg(purchase)\",\"mean_item_purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"purchase\", featuresCol=\"features\", maxIter=100, regParam=0.01, elasticNetParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = [\"mean_user_purchase\", \"mean_item_purchase\", \"content_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=features_col, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualification_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genres_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df_1 = predictions.select(['user_id','item_id','probability'])\\\n",
    "                    .withColumnRenamed(\"probability\",\"purchase\")\\\n",
    "                        .orderBy(['user_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "total_df = predictions.select(['user_id','item_id','probability'])\\\n",
    "                        .orderBy(['user_id','item_id']).toPandas()\n",
    "total_df['purchase'] = total_df['probability'].apply(lambda x: x[1])\n",
    "total_df = total_df.drop('probability', axis=1)\n",
    "total_df.to_csv('/data/home/ilya.nabatchikov/lab03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
