{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# conf = SparkConf()\n",
    "# conf.set(\"spark.app.name\", \"natasha pritykovskaya Spark Dataframe app\") \n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SQLContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"natasha pritykovskaya Spark Dataframe app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"natasha pritykovskaya Spark Dataframe app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-3.newprolab.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>natasha pritykovskaya Spark Dataframe app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f32ec815e48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Session:\n",
    "\n",
    "    - Spark Context\n",
    "    - SQL Context\n",
    "    - Hive Context\n",
    "    - Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-de-master-3.newprolab.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>natasha pritykovskaya Spark Dataframe app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=natasha pritykovskaya Spark Dataframe app>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как создать DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прочитать из внешнего источника"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7f32e867dac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline загрузки\n",
    "```python\n",
    "spark.read\\\n",
    "     .format(...)\\\n",
    "     .option(key, value)\\\n",
    "     .option(key, value)\\\n",
    "     .load(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .load(\"/lectures/lecture02/data/ml-100k/u.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+-----+\n",
      "|_c0|_c1|_c2|       _c3|  _c4|\n",
      "+---+---+---+----------+-----+\n",
      "|  1| 24|  M|technician|85711|\n",
      "+---+---+---+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='1', _c1='24', _c2='M', _c3='technician', _c4='85711'),\n",
       " Row(_c0='2', _c1='53', _c2='F', _c3='other', _c4='94043'),\n",
       " Row(_c0='3', _c1='23', _c2='M', _c3='writer', _c4='32067'),\n",
       " Row(_c0='4', _c1='24', _c2='M', _c3='technician', _c4='43537'),\n",
       " Row(_c0='5', _c1='33', _c2='F', _c3='other', _c4='15213')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"occupation\", StringType()),\n",
    "    StructField(\"zip\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "          .schema(schema)\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .load(\"/lectures/lecture02/data/ml-100k/u.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, gender: string, occupation: string, zip: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-----+\n",
      "|user_id|age|gender|occupation|  zip|\n",
      "+-------+---+------+----------+-----+\n",
      "|      1| 24|     M|technician|85711|\n",
      "|      2| 53|     F|     other|94043|\n",
      "|      3| 23|     M|    writer|32067|\n",
      "|      4| 24|     M|technician|43537|\n",
      "|      5| 33|     F|     other|15213|\n",
      "+-------+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "|summary|          user_id|              age|gender|   occupation|               zip|\n",
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "|  count|              925|              925|   925|          925|               925|\n",
      "|   mean|470.2908108108108|34.06054054054054|  null|         null| 50868.78810810811|\n",
      "| stddev|272.1030147185632|12.25807489536592|  null|         null|30891.373254138176|\n",
      "|    min|                1|                7|     F|administrator|                 0|\n",
      "|    25%|              236|               25|  null|         null|             21227|\n",
      "|    50%|              469|               31|  null|         null|             53711|\n",
      "|    75%|              705|               43|  null|         null|             78741|\n",
      "|    max|              943|               73|     M|       writer|             99835|\n",
      "+-------+-----------------+-----------------+------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Таким образом более приближенный к реальности pipeline выглядит как\n",
    "```python\n",
    "spark.read\\\n",
    "     .schema(schema)\\\n",
    "     .format(...)\\\n",
    "     .option(key, value)\\\n",
    "     .option(key, value)\\\n",
    "     .load(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Есть так же удобные wrapper'ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/lectures/lecture02/data/ml-100k/u.user\", schema=schema, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, gender: string, occupation: string, zip: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-----+\n",
      "|user_id|age|gender|occupation|  zip|\n",
      "+-------+---+------+----------+-----+\n",
      "|      1| 24|     M|technician|85711|\n",
      "|      2| 53|     F|     other|94043|\n",
      "|      3| 23|     M|    writer|32067|\n",
      "|      4| 24|     M|technician|43537|\n",
      "|      5| 33|     F|     other|15213|\n",
      "+-------+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Много источников данных с унифицированным API!\n",
    "+ CSV\n",
    "+ JSON\n",
    "+ Hive\n",
    "+ HBase\n",
    "+ Cassandra\n",
    "+ MySQL\n",
    "+ PostgreSQL\n",
    "+ Parquet\n",
    "+ ORC\n",
    "+ Kafka\n",
    "+ ElasticSearch\n",
    "+ Amazon S3\n",
    "+ ...и еще больше через custom connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим DataFrame из RDD, pandas.DataFrame или из list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/lectures/lecture02/data/ml-100k/u.user\").map(lambda x: x.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'],\n",
       " ['2', '53', 'F', 'other', '94043'],\n",
       " ['3', '23', 'M', 'writer', '32067'],\n",
       " ['4', '24', 'M', 'technician', '43537'],\n",
       " ['5', '33', 'F', 'other', '15213']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, age: int, gender: string, occupation: string, zip: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o193.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 13, spark-de-master-3.newprolab.com, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-eb589bae8d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o193.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 13, spark-de-master-3.newprolab.com, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/hadoop/yarn/local/usercache/natalya.pritykovskaya/appcache/application_1613394706435_0160/container_e04_1613394706435_0160_01_000003/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field user_id: IntegerType can not accept object '1' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Может быть с неправильными типами можно справится путем игнорирования верификации schema???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema=schema, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+----------+----+\n",
      "|user_id| age|gender|occupation| zip|\n",
      "+-------+----+------+----------+----+\n",
      "|   null|null|     M|technician|null|\n",
      "|   null|null|     F|     other|null|\n",
      "|   null|null|     M|    writer|null|\n",
      "|   null|null|     M|technician|null|\n",
      "|   null|null|     F|     other|null|\n",
      "+-------+----+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не выходит требуется конвертировать в правильные типы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: (int(x[0]), int(x[1]), x[2], x[3], int(x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+-----+\n",
      "|user_id|age|gender|occupation|  zip|\n",
      "+-------+---+------+----------+-----+\n",
      "|      1| 24|     M|technician|85711|\n",
      "|      2| 53|     F|     other|94043|\n",
      "|      3| 23|     M|    writer|32067|\n",
      "|      4| 24|     M|technician|43537|\n",
      "|      5| 33|     F|     other|15213|\n",
      "+-------+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поработаем с искусственным access логом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x64; Trident/5.0; .NET CLR 3.5.30729;)\r",
      "\r\n",
      "247.182.249.253\t20140426165946\thttp://news.yandex.ru/7686791\t1560\t202\tOpera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)\r",
      "\r\n",
      "197.72.248.141\t20140426170846\thttp://news.yandex.ru/1949655\t1175\t404\tOpera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)\r",
      "\r\n",
      "168.146.187.80\t20140426171807\thttp://news.mail.ru/7107147\t1020\t434\tOpera/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;)\r",
      "\r\n",
      "75.208.40.166\t20140426180003\thttp://news.yandex.ru/4696319\t526\t449\tSafari/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;)\r",
      "\r\n",
      "33.49.147.163\t20140426182902\thttp://news.mail.ru/2829289\t82\t510\tOpera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729)\r",
      "\r\n",
      "33.49.147.163\t20140426191049\thttp://news.rambler.ru/4707594\t1043\t206\tOpera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /lectures/lecture02/data/logsM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "log_schema = StructType(fields=[\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"size\", IntegerType()),\n",
    "    StructField(\"code\", IntegerType()),\n",
    "    StructField(\"ua\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = spark.read.csv(\"/lectures/lecture02/data/logsM.txt\", sep=\"\\t\", schema=log_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string, size: int, code: int, ua: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                                                   \n",
      " timestamp | 20140127041332                                                                                                 \n",
      " url       | http://lenta.ru/5567208                                                                                        \n",
      " size      | 184                                                                                                            \n",
      " code      | 509                                                                                                            \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;)                             \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 222.131.187.37                                                                                                 \n",
      " timestamp | 20140112193801                                                                                                 \n",
      " url       | http://news.mail.ru/7703130                                                                                    \n",
      " size      | 903                                                                                                            \n",
      " code      | 504                                                                                                            \n",
      " ua        | Chrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)                \n",
      "-RECORD 2-------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 222.131.187.37                                                                                                 \n",
      " timestamp | 20140122151429                                                                                                 \n",
      " url       | http://lenta.ru/1035910                                                                                        \n",
      " size      | 89                                                                                                             \n",
      " code      | 502                                                                                                            \n",
      " ua        | Safari/5.0 (Windows; U; MSIE 9.0; Windows NT 8.0; Win64; x64; Trident/5.0; .NET4.0E; en)                       \n",
      "-RECORD 3-------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                  \n",
      " timestamp | 20140416123526                                                                                                 \n",
      " url       | http://news.yandex.ru/3336146                                                                                  \n",
      " size      | 29                                                                                                             \n",
      " code      | 409                                                                                                            \n",
      " ua        | Firefox/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; \n",
      "-RECORD 4-------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                  \n",
      " timestamp | 20140410014351                                                                                                 \n",
      " url       | http://newsru.com/9238321                                                                                      \n",
      " size      | 1088                                                                                                           \n",
      " code      | 409                                                                                                            \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;)                             \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections и filters\n",
    "Projection - это подмножемтво колокнок\n",
    "\n",
    "Filter - это подмножество строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip', 'timestamp', 'url', 'size', 'code', 'ua']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.select([\"ip\", \"timestamp\", \"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|             ip|     timestamp|                 url|\n",
      "+---------------+--------------+--------------------+\n",
      "| 197.72.248.141|20140313153714|http://lenta.ru/9...|\n",
      "|  75.208.40.166|20140210083843|http://news.rambl...|\n",
      "| 222.131.187.37|20140425170839|http://lenta.ru/2...|\n",
      "|   49.203.96.67|20140225042119|http://news.yande...|\n",
      "|135.124.143.193|20140313110843|http://news.mail....|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(*log.schema.fieldNames()[:3]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "|   25.62.10.220| 306|\n",
      "|   49.105.15.79| 101|\n",
      "|   49.105.15.79| 101|\n",
      "|135.124.143.193| 303|\n",
      "|135.124.143.193| 303|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ip\", \"code\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "|   25.62.10.220| 306|\n",
      "|   49.105.15.79| 101|\n",
      "|   49.105.15.79| 101|\n",
      "|135.124.143.193| 303|\n",
      "|135.124.143.193| 303|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(log.ip, log.code).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'ip'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             ip|response|\n",
      "+---------------+--------+\n",
      "|   25.62.10.220|     306|\n",
      "|   49.105.15.79|     101|\n",
      "|   49.105.15.79|     101|\n",
      "|135.124.143.193|     303|\n",
      "|135.124.143.193|     303|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(log.ip,\n",
    "           log.code.alias(\"response\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             ip|response|\n",
      "+---------------+--------+\n",
      "|   25.62.10.220|     306|\n",
      "|   49.105.15.79|     101|\n",
      "|   49.105.15.79|     101|\n",
      "|135.124.143.193|     303|\n",
      "|135.124.143.193|     303|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ip\", \n",
    "           f.col(\"code\").alias(\"response\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good ol' Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "|   25.62.10.220| 306|\n",
      "|   49.105.15.79| 101|\n",
      "|   49.105.15.79| 101|\n",
      "|135.124.143.193| 303|\n",
      "|135.124.143.193| 303|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log[[\"ip\", \"code\"]].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             ip|response|\n",
      "+---------------+--------+\n",
      "|   25.62.10.220|     306|\n",
      "|   49.105.15.79|     101|\n",
      "|   49.105.15.79|     101|\n",
      "|135.124.143.193|     303|\n",
      "|135.124.143.193|     303|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log[[log.ip, log.code.alias(\"response\")]].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "| 75.208.40.166|20140326084231|http://news.yande...| 955| 200|Opera/5.0 (Window...|\n",
      "|197.72.248.141|20140404122749|http://newsru.com...| 884| 200|Opera/5.0 (compat...|\n",
      "| 75.208.40.166|20140216075757|http://news.rambl...|1337| 200|Firefox/5.0 (comp...|\n",
      "|168.255.93.197|20140213033139|http://news.yande...|1004| 200|Chrome/5.0 (Windo...|\n",
      "|197.72.248.141|20140403130026|http://news.yande...| 856| 200|Opera/5.0 (compat...|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.where(\"code = 200\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------------------------+----+----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|ip            |timestamp     |url                           |size|code|ua                                                                                                                           |\n",
      "+--------------+--------------+------------------------------+----+----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|75.208.40.166 |20140326084231|http://news.yandex.ru/2002966 |955 |200 |Opera/5.0 (Windows; U; MSIE 9.0; Windows NT 8.1; Trident/5.0; .NET4.0E; en-AU)                                               |\n",
      "|197.72.248.141|20140404122749|http://newsru.com/7833710     |884 |200 |Opera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)                               |\n",
      "|75.208.40.166 |20140216075757|http://news.rambler.ru/8637466|1337|200 |Firefox/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)                             |\n",
      "|168.255.93.197|20140213033139|http://news.yandex.ru/9936226 |1004|200 |Chrome/5.0 (Windows; U; MSIE 9.0; Windows NT 6.0; Win64; x64; Trident/5.0; .NET CLR 3.8.50799; Media Center PC 6.0; .NET4.0E)|\n",
      "|197.72.248.141|20140403130026|http://news.yandex.ru/2010781 |856 |200 |Opera/5.0 (compatible; MSIE 9.0; Windows NT 8.0; WOW64; Trident/5.0; .NET CLR 2.7.40781; .NET4.0E; en-SG)                    |\n",
      "+--------------+--------------+------------------------------+----+----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.filter(log.code == 200).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                                 \n",
      " timestamp | 20140317090245                                                                                                                \n",
      " url       | http://news.rambler.ru/1667841                                                                                                \n",
      " size      | 166                                                                                                                           \n",
      " code      | 200                                                                                                                           \n",
      " ua        | Safari/5.0 (Windows; U; MSIE 9.0; Windows NT 6.0; Win64; x64; Trident/5.0; .NET CLR 3.8.50799; Media Center PC 6.0; .NET4.0E) \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 75.208.40.166                                                                                                                 \n",
      " timestamp | 20140310132240                                                                                                                \n",
      " url       | http://news.rambler.ru/9308492                                                                                                \n",
      " size      | 1107                                                                                                                          \n",
      " code      | 200                                                                                                                           \n",
      " ua        | Opera/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)                                \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                                 \n",
      " timestamp | 20140330043133                                                                                                                \n",
      " url       | http://news.rambler.ru/9974669                                                                                                \n",
      " size      | 945                                                                                                                           \n",
      " code      | 200                                                                                                                           \n",
      " ua        | Chrome/5.0 (compatible; MSIE 9.0; Windows NT 8.0; WOW64; Trident/5.0; .NET CLR 2.7.40781; .NET4.0E; en-SG)                    \n",
      "-RECORD 3----------------------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                                 \n",
      " timestamp | 20140329134621                                                                                                                \n",
      " url       | http://news.rambler.ru/4851890                                                                                                \n",
      " size      | 1593                                                                                                                          \n",
      " code      | 200                                                                                                                           \n",
      " ua        | Firefox/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729)    \n",
      "-RECORD 4----------------------------------------------------------------------------------------------------------------------------------\n",
      " ip        | 33.49.147.163                                                                                                                 \n",
      " timestamp | 20140106045941                                                                                                                \n",
      " url       | http://news.rambler.ru/1817681                                                                                                \n",
      " size      | 671                                                                                                                           \n",
      " code      | 200                                                                                                                           \n",
      " ua        | Firefox/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729;)                              \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.filter(\"code == 200 AND url LIKE '%rambler%'\").show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|222.131.187.37|20140219162554|http://news.rambl...| 119| 404|Chrome/5.0 (compa...|\n",
      "|197.72.248.141|20140222074905|http://news.rambl...|1264| 200|Chrome/5.0 compat...|\n",
      "| 33.49.147.163|20140109011807|http://news.rambl...|   4| 200|Safari/5.0 (compa...|\n",
      "|197.72.248.141|20140421071734|http://news.rambl...|2032| 404|Safari/5.0 (compa...|\n",
      "|197.72.248.141|20140210221913|http://news.rambl...|1654| 404|Safari/5.0 (Windo...|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.filter((log.code.isin([200, 404])) & (log.url.like(\"%rambler%\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good ol' Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|           ip|     timestamp|                 url|size|code|                  ua|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "|33.49.147.163|20140317090245|http://news.rambl...| 166| 200|Safari/5.0 (Windo...|\n",
      "|75.208.40.166|20140310132240|http://news.rambl...|1107| 200|Opera/5.0 (compat...|\n",
      "|33.49.147.163|20140330043133|http://news.rambl...| 945| 200|Chrome/5.0 (compa...|\n",
      "|33.49.147.163|20140329134621|http://news.rambl...|1593| 200|Firefox/5.0 (comp...|\n",
      "|33.49.147.163|20140106045941|http://news.rambl...| 671| 200|Firefox/5.0 (comp...|\n",
      "+-------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log[(log.code == 200) & (log.url.like(\"%rambler%\"))].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Все вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "| 197.72.248.141| 200|\n",
      "|247.182.249.253| 200|\n",
      "|  75.208.40.166| 200|\n",
      "|  75.208.40.166| 200|\n",
      "|   25.62.10.220| 200|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log[(log.code == 200) & (log.url.like(\"%rambler%\"))][[\"ip\", \"code\"]].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что же насчет SQL?! Он тут как тут)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ip, code FROM log_table \n",
    "WHERE code == 200 AND url LIKE '%rambler%'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: log_table; line 2 pos 21'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: log_table; line 2 pos 21\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'log_table' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 63 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-685af25b5f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: log_table; line 2 pos 21'"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Надо зарегистрировать свой DataFrame как table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.registerTempTable(\"log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             ip|code|\n",
      "+---------------+----+\n",
      "| 197.72.248.141| 200|\n",
      "|247.182.249.253| 200|\n",
      "|  75.208.40.166| 200|\n",
      "|  75.208.40.166| 200|\n",
      "|   25.62.10.220| 200|\n",
      "+---------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как Spark будет выполнять запрос?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['ip, 'code]\n",
      "+- 'Filter (('code = 200) && 'url LIKE %rambler%)\n",
      "   +- 'UnresolvedRelation `log_table`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ip: string, code: int\n",
      "Project [ip#572, code#576]\n",
      "+- Filter ((code#576 = 200) && url#574 LIKE %rambler%)\n",
      "   +- SubqueryAlias `log_table`\n",
      "      +- Repartition 4, true\n",
      "         +- Relation[ip#572,timestamp#573L,url#574,size#575,code#576,ua#577] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 4, true\n",
      "+- Project [ip#572, code#576]\n",
      "   +- Filter (((isnotnull(code#576) && isnotnull(url#574)) && (code#576 = 200)) && Contains(url#574, rambler))\n",
      "      +- InMemoryRelation [ip#572, timestamp#573L, url#574, size#575, code#576, ua#577], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) FileScan csv [ip#572,timestamp#573L,url#574,size#575,code#576,ua#577] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://bd-master.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange RoundRobinPartitioning(4)\n",
      "+- *(1) Project [ip#572, code#576]\n",
      "   +- *(1) Filter (((isnotnull(code#576) && isnotnull(url#574)) && (code#576 = 200)) && Contains(url#574, rambler))\n",
      "      +- InMemoryTableScan [code#576, ip#572, url#574], [isnotnull(code#576), isnotnull(url#574), (code#576 = 200), Contains(url#574, rambler)]\n",
      "            +- InMemoryRelation [ip#572, timestamp#573L, url#574, size#575, code#576, ua#577], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- *(1) FileScan csv [ip#572,timestamp#573L,url#574,size#575,code#576,ua#577] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://bd-master.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Функции есть 3 типа\n",
    "\n",
    "+ mapping (one to one)\n",
    "+ generating (one to many)\n",
    "+ aggregating (many to one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                  ua|length(ua)|\n",
      "+--------------------+----------+\n",
      "|Safari/5.0 (compa...|        95|\n",
      "|Safari/5.0 (compa...|        95|\n",
      "|Safari/5.0 (compa...|        95|\n",
      "|Safari/5.0 (compa...|        95|\n",
      "|Safari/5.0 (compa...|        95|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ua\", f.length(\"ua\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                  ua|length|\n",
      "+--------------------+------+\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "|Safari/5.0 (compa...|    95|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ua\", f.length(\"ua\").alias(\"length\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`?utm_medium=email`' given input columns: [code, url, timestamp, ua, ip, size];;\\n'Project [concat(url#604, '?utm_medium=email) AS concat(url, ?utm_medium=email)#1552]\\n+- Repartition 4, true\\n   +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o283.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`?utm_medium=email`' given input columns: [code, url, timestamp, ua, ip, size];;\n'Project [concat(url#604, '?utm_medium=email) AS concat(url, ?utm_medium=email)#1552]\n+- Repartition 4, true\n   +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-1ddb07dc3729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"?utm_medium=email\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \"\"\"\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`?utm_medium=email`' given input columns: [code, url, timestamp, ua, ip, size];;\\n'Project [concat(url#604, '?utm_medium=email) AS concat(url, ?utm_medium=email)#1552]\\n+- Repartition 4, true\\n   +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\\n\""
     ]
    }
   ],
   "source": [
    "log.select(f.concat(\"url\", \"?utm_medium=email\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`concat` необходим `Column` как аргумент. `lit` создает новую `Column` из literal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|newurl                                         |\n",
      "+-----------------------------------------------+\n",
      "|http://newsru.com/4608402?utm_medium=email     |\n",
      "|http://news.yandex.ru/4761980?utm_medium=email |\n",
      "|http://lenta.ru/2662415?utm_medium=email       |\n",
      "|http://news.mail.ru/5550951?utm_medium=email   |\n",
      "|http://news.rambler.ru/2396720?utm_medium=email|\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(f.concat(\"url\", f.lit(\"?utm_medium=email\")).alias(\"newurl\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explosions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------\n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " word_list | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------\n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " word_list | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------\n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " word_list | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------\n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " word_list | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------\n",
      " ua        | Safari/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; chromeframe/12.0.742.112)            \n",
      " word_list | [Safari/5.0, (compatible;, MSIE, 9.0;, Windows, NT, 6.1;, WOW64;, Trident/5.0;, chromeframe/12.0.742.112)] \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ua\", f.split(\"ua\", \" \").alias(\"word_list\")).show(5, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно выбирать отдельные элементы из list!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|word_list[0]|word_list[1]|\n",
      "+------------+------------+\n",
      "|  Safari/5.0|(compatible;|\n",
      "|  Safari/5.0|(compatible;|\n",
      "|  Safari/5.0|(compatible;|\n",
      "|  Safari/5.0|(compatible;|\n",
      "|  Safari/5.0|(compatible;|\n",
      "+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ua\", f.split(\"ua\", \" \").alias(\"word_list\"))\\\n",
    "   .select(f.col(\"word_list\")[0], f.col(\"word_list\")[1])\\\n",
    "   .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|        MSIE|10092|\n",
      "|        9.0;|10092|\n",
      "|          NT|10092|\n",
      "|     Windows|10092|\n",
      "|Trident/5.0;| 9121|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.select(\"ua\", f.split(\"ua\", \" \").alias(\"word_list\"))\\\n",
    "   .select(f.explode(\"word_list\").alias(\"word\"))\\\n",
    "   .groupby(\"word\").count()\\\n",
    "   .orderBy(\"count\", ascending=False)\\\n",
    "   .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.67\tKhanty–Mansi\r\n",
      "197.72.248.141\tChechnya\r\n",
      "33.49.147.163\tNizhny Novgorod Oblast\r\n",
      "56.167.169.126\tVoronezh Oblast\r\n",
      "3.183.113.77\tAstrakhan Oblast\r\n",
      "56.167.169.126\tTver Oblast\r\n",
      "56.167.169.126\tKabardino-Balkaria\r\n",
      "56.167.169.126\tNenets Autonomous Okrug\r\n",
      "33.49.147.163\tOmsk Oblast\r\n",
      "14.8.59.211\tKhabarovsk Krai\r\n",
      "75.208.40.166\tSakha\r\n",
      "135.124.143.193\tSamara Oblast\r\n",
      "75.208.40.166\tNovosibirsk Oblast\r\n",
      "75.208.40.166\tAmur Oblast\r\n",
      "75.208.40.166\tKarelia\r\n",
      "75.208.40.166\tSaint Petersburg\r\n",
      "181.217.177.35\tSamara Oblast\r\n",
      "33.49.147.163\tIrkutsk Oblast\r\n",
      "56.167.169.126\tLipetsk Oblast\r\n",
      "181.217.177.35\tKalmykia\r\n",
      "168.255.93.197\tVolgograd Oblast\r\n",
      "168.255.93.197\tOryol Oblast\r\n",
      "168.255.93.197\tKurgan Oblast\r\n",
      "168.146.187.80\tPrimorsky Krai\r\n",
      "49.105.15.79\tNorth Ossetia–Alania\r\n",
      "197.72.248.141\tStavropol Krai\r\n",
      "14.8.59.211\tKemerovo Oblast\r\n",
      "49.203.96.67\tUlyanovsk Oblast\r\n",
      "222.131.187.37\tYamalo-Nenets\r\n",
      "197.72.248.141\tZabaykalsky Krai\r\n",
      "222.131.187.37\tKaluga Oblast\r\n",
      "3.183.113.77\tSaratov Oblast\r\n",
      "168.255.93.197\tAstrakhan Oblast\r\n",
      "75.208.40.166\tNovgorod Oblast\r\n",
      "135.124.143.193\tTambov Oblast\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /lectures/lecture02/data/ipDataM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_schema = StructType(fields=[\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"region\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = spark.read.csv(\"/lectures/lecture02/data/ipDataM.txt\", schema=ip_schema, sep=\"\\t\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|            ip|            region|\n",
      "+--------------+------------------+\n",
      "|  49.105.15.79|              Komi|\n",
      "|110.91.102.196|Chelyabinsk Oblast|\n",
      "|56.167.169.126|  Saint Petersburg|\n",
      "| 75.208.40.166|  Ulyanovsk Oblast|\n",
      "|168.255.93.197|    Irkutsk Oblast|\n",
      "+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Есть трюк, который отключает автоматические broadcast'ы для joins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET spark.sql.autoBroadcastJoinThreshold = 100500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log.join(ips, on=\"ip\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ip: string, timestamp: bigint, url: string, size: int, code: int, ua: string, region: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_with_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "|          ip|     timestamp|                 url|size|code|                  ua|             region|\n",
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "|3.183.113.77|20140416052008|http://news.rambl...| 363| 404|Safari/5.0 (Windo...|           Chukotka|\n",
      "|3.183.113.77|20140416052008|http://news.rambl...| 363| 404|Safari/5.0 (Windo...|     Ivanovo Oblast|\n",
      "|3.183.113.77|20140416052008|http://news.rambl...| 363| 404|Safari/5.0 (Windo...|          Tatarstan|\n",
      "|3.183.113.77|20140416052008|http://news.rambl...| 363| 404|Safari/5.0 (Windo...|Karachay–Cherkessia|\n",
      "|3.183.113.77|20140416052008|http://news.rambl...| 363| 404|Safari/5.0 (Windo...|   Yaroslavl Oblast|\n",
      "+------------+--------------+--------------------+----+----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_with_regions.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log_with_regions.coalesce(4).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query planner использует SortMergeJoin по дефолту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 4, false\n",
      "+- Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "   +- Join Inner, (ip#602 = ip#1723)\n",
      "      :- Repartition 4, true\n",
      "      :  +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\n",
      "      +- Relation[ip#1723,region#1724] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ip: string, timestamp: bigint, url: string, size: int, code: int, ua: string, region: string\n",
      "Repartition 4, false\n",
      "+- Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "   +- Join Inner, (ip#602 = ip#1723)\n",
      "      :- Repartition 4, true\n",
      "      :  +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\n",
      "      +- Relation[ip#1723,region#1724] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   +- Coalesce 4\n",
      "      +- *(5) Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "         +- *(5) SortMergeJoin [ip#602], [ip#1723], Inner\n",
      "            :- *(2) Sort [ip#602 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(ip#602, 200)\n",
      "            :     +- Exchange RoundRobinPartitioning(4)\n",
      "            :        +- *(1) Filter isnotnull(ip#602)\n",
      "            :           +- InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], [isnotnull(ip#602)]\n",
      "            :                 +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            :                       +- *(1) FileScan csv [ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "            +- *(4) Sort [ip#1723 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(ip#1723, 200)\n",
      "                  +- *(3) Filter isnotnull(ip#1723)\n",
      "                     +- InMemoryTableScan [ip#1723, region#1724], [isnotnull(ip#1723)]\n",
      "                           +- InMemoryRelation [ip#1723, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) FileScan csv [ip#1723,region#1724] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/ipDataM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,region:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "   +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Coalesce 4\n",
      "            +- *(5) Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "               +- *(5) SortMergeJoin [ip#602], [ip#1723], Inner\n",
      "                  :- *(2) Sort [ip#602 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(ip#602, 200)\n",
      "                  :     +- Exchange RoundRobinPartitioning(4)\n",
      "                  :        +- *(1) Filter isnotnull(ip#602)\n",
      "                  :           +- InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], [isnotnull(ip#602)]\n",
      "                  :                 +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  :                       +- *(1) FileScan csv [ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "                  +- *(4) Sort [ip#1723 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(ip#1723, 200)\n",
      "                        +- *(3) Filter isnotnull(ip#1723)\n",
      "                           +- InMemoryTableScan [ip#1723, region#1724], [isnotnull(ip#1723)]\n",
      "                                 +- InMemoryRelation [ip#1723, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                       +- *(1) FileScan csv [ip#1723,region#1724] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/ipDataM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,region:string>\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log.join(f.broadcast(ips), on=\"ip\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "+- *(3) BroadcastHashJoin [ip#602], [ip#1723], Inner, BuildRight\n",
      "   :- Exchange RoundRobinPartitioning(4)\n",
      "   :  +- *(1) Filter isnotnull(ip#602)\n",
      "   :     +- InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], [isnotnull(ip#602)]\n",
      "   :           +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) FileScan csv [ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))\n",
      "      +- *(2) Filter isnotnull(ip#1723)\n",
      "         +- InMemoryTableScan [ip#1723, region#1724], [isnotnull(ip#1723)]\n",
      "               +- InMemoryRelation [ip#1723, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) FileScan csv [ip#1723,region#1724] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/ipDataM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,region:string>\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Или можно использовать метод `hint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_regions = log.join(ips.hint(\"broadcast\"), on=\"ip\", how=\"inner\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner,Buffer(ip))\n",
      ":- Repartition 4, true\n",
      ":  +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\n",
      "+- ResolvedHint (broadcast)\n",
      "   +- Relation[ip#1723,region#1724] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ip: string, timestamp: bigint, url: string, size: int, code: int, ua: string, region: string\n",
      "Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "+- Join Inner, (ip#602 = ip#1723)\n",
      "   :- Repartition 4, true\n",
      "   :  +- Relation[ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] csv\n",
      "   +- ResolvedHint (broadcast)\n",
      "      +- Relation[ip#1723,region#1724] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   +- *(3) Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "      +- *(3) BroadcastHashJoin [ip#602], [ip#1723], Inner, BuildRight\n",
      "         :- Exchange RoundRobinPartitioning(4)\n",
      "         :  +- *(1) Filter isnotnull(ip#602)\n",
      "         :     +- InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], [isnotnull(ip#602)]\n",
      "         :           +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :                 +- *(1) FileScan csv [ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))\n",
      "            +- *(2) Filter isnotnull(ip#1723)\n",
      "               +- InMemoryTableScan [ip#1723, region#1724], [isnotnull(ip#1723)]\n",
      "                     +- InMemoryRelation [ip#1723, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) FileScan csv [ip#1723,region#1724] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/ipDataM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,region:string>\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "   +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(3) Project [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607, region#1724]\n",
      "            +- *(3) BroadcastHashJoin [ip#602], [ip#1723], Inner, BuildRight\n",
      "               :- Exchange RoundRobinPartitioning(4)\n",
      "               :  +- *(1) Filter isnotnull(ip#602)\n",
      "               :     +- InMemoryTableScan [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], [isnotnull(ip#602)]\n",
      "               :           +- InMemoryRelation [ip#602, timestamp#603L, url#604, size#605, code#606, ua#607], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :                 +- *(1) FileScan csv [ip#602,timestamp#603L,url#604,size#605,code#606,ua#607] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/logsM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,timestamp:bigint,url:string,size:int,code:int,ua:string>\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))\n",
      "                  +- *(2) Filter isnotnull(ip#1723)\n",
      "                     +- InMemoryTableScan [ip#1723, region#1724], [isnotnull(ip#1723)]\n",
      "                           +- InMemoryRelation [ip#1723, region#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) FileScan csv [ip#1723,region#1724] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-de-master-1.newprolab.com:8020/lectures/lecture02/data/ipDataM.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ip:string,region:string>\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation pipeline выглядит следующим образом:\n",
    "```python\n",
    "df.groupBy(*cols)\\\n",
    "  .agg(*expressions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|            region| count|\n",
      "+------------------+------+\n",
      "|  Ulyanovsk Oblast|204275|\n",
      "|            Jewish|134523|\n",
      "|  Saint Petersburg|129362|\n",
      "|Arkhangelsk Oblast|124937|\n",
      "|    Vologda Oblast|122363|\n",
      "|   Novgorod Oblast|122306|\n",
      "|     Moscow Oblast|120336|\n",
      "|  Krasnoyarsk Krai|119285|\n",
      "|              Komi|117659|\n",
      "|          Kalmykia|117172|\n",
      "+------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.groupBy(\"region\")\\\n",
    "                .agg(f.count(\"ip\").alias(\"count\"))\\\n",
    "                .orderBy(\"count\", ascending=False)\\\n",
    "                .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|            region|row_count|\n",
      "+------------------+---------+\n",
      "|  Ulyanovsk Oblast|   204275|\n",
      "|            Jewish|   134523|\n",
      "|  Saint Petersburg|   129362|\n",
      "|Arkhangelsk Oblast|   124937|\n",
      "|    Vologda Oblast|   122363|\n",
      "|   Novgorod Oblast|   122306|\n",
      "|     Moscow Oblast|   120336|\n",
      "|  Krasnoyarsk Krai|   119285|\n",
      "|              Komi|   117659|\n",
      "|          Kalmykia|   117172|\n",
      "+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_regions.groupBy(\"region\")\\\n",
    "                .count()\\\n",
    "                .withColumnRenamed(\"count\", \"row_count\")\\\n",
    "                .orderBy(\"row_count\", ascending=False)\\\n",
    "                .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stat = log_with_regions.groupBy(f.length(\"url\").alias(\"url_length\"))\\\n",
    "                              .agg(f.count(\"*\").alias(\"row_count\"))\\\n",
    "                              .orderBy(\"row_count\", ascending=False)\\\n",
    "                              .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_length</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>1676363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>1644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>1639043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1638174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1617023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_length  row_count\n",
       "0          23    1676363\n",
       "1          27    1644000\n",
       "2          25    1639043\n",
       "3          29    1638174\n",
       "4          30    1617023"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f32d860d780>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_stat.plot(kind=\"bar\", x=\"url_length\", y=\"row_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_domains = log_with_regions.withColumn(\"domain\", f.regexp_extract(\"url\", \"http:\\/\\/(.*)\\/\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                       \n",
      " timestamp | 20140127041332                                                                     \n",
      " url       | http://lenta.ru/5567208                                                            \n",
      " size      | 184                                                                                \n",
      " code      | 509                                                                                \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region    | North Ossetia–Alania                                                               \n",
      " domain    | lenta.ru                                                                           \n",
      "-RECORD 1---------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                       \n",
      " timestamp | 20140127041332                                                                     \n",
      " url       | http://lenta.ru/5567208                                                            \n",
      " size      | 184                                                                                \n",
      " code      | 509                                                                                \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region    | Kalmykia                                                                           \n",
      " domain    | lenta.ru                                                                           \n",
      "-RECORD 2---------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                       \n",
      " timestamp | 20140127041332                                                                     \n",
      " url       | http://lenta.ru/5567208                                                            \n",
      " size      | 184                                                                                \n",
      " code      | 509                                                                                \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region    | Bryansk Oblast                                                                     \n",
      " domain    | lenta.ru                                                                           \n",
      "-RECORD 3---------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                       \n",
      " timestamp | 20140127041332                                                                     \n",
      " url       | http://lenta.ru/5567208                                                            \n",
      " size      | 184                                                                                \n",
      " code      | 509                                                                                \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region    | Kursk Oblast                                                                       \n",
      " domain    | lenta.ru                                                                           \n",
      "-RECORD 4---------------------------------------------------------------------------------------\n",
      " ip        | 49.105.15.79                                                                       \n",
      " timestamp | 20140127041332                                                                     \n",
      " url       | http://lenta.ru/5567208                                                            \n",
      " size      | 184                                                                                \n",
      " code      | 509                                                                                \n",
      " ua        | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region    | Udmurt                                                                             \n",
      " domain    | lenta.ru                                                                           \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains.show(5, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Есть ли корреляция между `url_length` и `domain`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stat = log_with_domains.groupBy(f.length(\"url\").alias(\"url_length\"), \"domain\")\\\n",
    "                              .agg(f.count(\"*\").alias(\"row_count\"))\\\n",
    "                              .orderBy(\"row_count\", ascending=False)\\\n",
    "                              .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_length</th>\n",
       "      <th>domain</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>lenta.ru</td>\n",
       "      <td>1676363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>news.mail.ru</td>\n",
       "      <td>1644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>newsru.com</td>\n",
       "      <td>1639043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>news.yandex.ru</td>\n",
       "      <td>1638174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>news.rambler.ru</td>\n",
       "      <td>1617023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_length           domain  row_count\n",
       "0          23         lenta.ru    1676363\n",
       "1          27     news.mail.ru    1644000\n",
       "2          25       newsru.com    1639043\n",
       "3          29   news.yandex.ru    1638174\n",
       "4          30  news.rambler.ru    1617023"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|url                    |\n",
      "+-----------------------+\n",
      "|http://lenta.ru/5567208|\n",
      "|http://lenta.ru/5567208|\n",
      "|http://lenta.ru/5567208|\n",
      "|http://lenta.ru/5567208|\n",
      "|http://lenta.ru/5567208|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains[log_with_domains.domain == \"lenta.ru\"][[\"url\"]].show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Currently correlation calculation for columns with dataType string not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o744.corr.\n: java.lang.IllegalArgumentException: requirement failed: Currently correlation calculation for columns with dataType string not supported.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:159)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3.apply(StatFunctions.scala:157)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.collectStatisticalData(StatFunctions.scala:157)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.pearsonCorrelation(StatFunctions.scala:109)\n\tat org.apache.spark.sql.DataFrameStatFunctions.corr(DataFrameStatFunctions.scala:159)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-66dba9a4c46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_with_domains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"domain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"url_length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(self, col1, col2, method)\u001b[0m\n\u001b[1;32m   1914\u001b[0m             raise ValueError(\"Currently only the calculation of the Pearson Correlation \" +\n\u001b[1;32m   1915\u001b[0m                              \"coefficient is supported.\")\n\u001b[0;32m-> 1916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Currently correlation calculation for columns with dataType string not supported.'"
     ]
    }
   ],
   "source": [
    "log_with_domains.withColumn(\"url_length\", f.length(\"url\")).corr(\"domain\", \"url_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "UDF может иметь один из следующих типов:\n",
    "+ **SCALAR**. scalar UDF задает transformation: Одну или несколько `pandas.Series` -> `pandas.Series`. Scalar UDFs используются с `pyspark.sql.DataFrame.withColumn()` и `pyspark.sql.DataFrame.select()`\n",
    "+ **GROUPED_MAP**.grouped map UDF задает transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`. Grouped map UDFs используются с `pyspark.sql.GroupedData.apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(domain='news.rambler.ru'),\n",
       " Row(domain='news.yandex.ru'),\n",
       " Row(domain='newsru.com'),\n",
       " Row(domain='news.mail.ru'),\n",
       " Row(domain='lenta.ru')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_with_domains[[\"domain\"]].distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(IntegerType())\n",
    "def encode_domain(domains):\n",
    "    mapping = {\n",
    "        'lenta.ru': 0,\n",
    "        'newsru.com': 1,\n",
    "        'news.mail.ru': 2,\n",
    "        'news.yandex.ru': 3,\n",
    "        'news.rambler.ru': 4\n",
    "    }\n",
    "    return domains.apply(lambda x: mapping.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------------------------\n",
      " ip             | 49.105.15.79                                                                       \n",
      " timestamp      | 20140127041332                                                                     \n",
      " url            | http://lenta.ru/5567208                                                            \n",
      " size           | 184                                                                                \n",
      " code           | 509                                                                                \n",
      " ua             | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region         | North Ossetia–Alania                                                               \n",
      " domain         | lenta.ru                                                                           \n",
      " domain_encoded | 0                                                                                  \n",
      "-RECORD 1--------------------------------------------------------------------------------------------\n",
      " ip             | 49.105.15.79                                                                       \n",
      " timestamp      | 20140127041332                                                                     \n",
      " url            | http://lenta.ru/5567208                                                            \n",
      " size           | 184                                                                                \n",
      " code           | 509                                                                                \n",
      " ua             | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region         | Kalmykia                                                                           \n",
      " domain         | lenta.ru                                                                           \n",
      " domain_encoded | 0                                                                                  \n",
      "-RECORD 2--------------------------------------------------------------------------------------------\n",
      " ip             | 49.105.15.79                                                                       \n",
      " timestamp      | 20140127041332                                                                     \n",
      " url            | http://lenta.ru/5567208                                                            \n",
      " size           | 184                                                                                \n",
      " code           | 509                                                                                \n",
      " ua             | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region         | Bryansk Oblast                                                                     \n",
      " domain         | lenta.ru                                                                           \n",
      " domain_encoded | 0                                                                                  \n",
      "-RECORD 3--------------------------------------------------------------------------------------------\n",
      " ip             | 49.105.15.79                                                                       \n",
      " timestamp      | 20140127041332                                                                     \n",
      " url            | http://lenta.ru/5567208                                                            \n",
      " size           | 184                                                                                \n",
      " code           | 509                                                                                \n",
      " ua             | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region         | Kursk Oblast                                                                       \n",
      " domain         | lenta.ru                                                                           \n",
      " domain_encoded | 0                                                                                  \n",
      "-RECORD 4--------------------------------------------------------------------------------------------\n",
      " ip             | 49.105.15.79                                                                       \n",
      " timestamp      | 20140127041332                                                                     \n",
      " url            | http://lenta.ru/5567208                                                            \n",
      " size           | 184                                                                                \n",
      " code           | 509                                                                                \n",
      " ua             | Chrome/5.0 compatible; MSIE 9.0; Windows NT 7.0; Trident/5.0; .NET CLR 2.2.50767;) \n",
      " region         | Udmurt                                                                             \n",
      " domain         | lenta.ru                                                                           \n",
      " domain_encoded | 0                                                                                  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains.withColumn(\"domain_encoded\", encode_domain(\"domain\")).show(5, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939371499379468"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_with_domains.withColumn(\"domain_encoded\", encode_domain(\"domain\"))\\\n",
    "                .withColumn(\"url_length\", f.length(\"url\"))\\\n",
    "                .corr(\"domain_encoded\", \"url_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_with_domains.filter(f.isnull(\"url\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------+\n",
      "|         domain|min(length(url))|max(length(url))|\n",
      "+---------------+----------------+----------------+\n",
      "|news.rambler.ru|              30|              30|\n",
      "| news.yandex.ru|              29|              29|\n",
      "|     newsru.com|              25|              25|\n",
      "|   news.mail.ru|              27|              27|\n",
      "|       lenta.ru|              23|              23|\n",
      "+---------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_with_domains.groupby(\"domain\").agg(f.min(f.length(\"url\")), f.max(f.length(\"url\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
