{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подсчет числа пользовательских сессий\n",
    "Вам необходимо подсчитать число пользовательских сессий в разбивке по доменам на данных из лог-файла.\n",
    "\n",
    "**Пользовательская сессия** - это пребывание пользователя на сайте такое, что между двумя последовательными кликами проходит не более 30 минут.\n",
    "Лог-файл такой же, как и на лекции. Находится в HDFS по пути `/lectures/lecture02/data/logsM.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark SQL seminar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bd-master.newprolab.com:4053\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark SQL seminar</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f65d9379160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "Создайте `DataFrame` из лог-файла. Схему можно скопировать из лекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "log_schema = StructType(fields=[\n",
    "    StructField(\"ip\", StringType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"size\", IntegerType()),\n",
    "    StructField(\"code\", IntegerType()),\n",
    "    StructField(\"ua\", StringType())\n",
    "])\n",
    "\n",
    "log = spark.read.csv(\"/lectures/lecture02/data/logsM.txt\", sep=\"\\t\", schema=log_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "| 33.49.147.163|20140101014611|http://news.rambl...| 378| 431|Safari/5.0 (compa...|\n",
      "|197.72.248.141|20140101020306|http://news.mail....|1412| 203|Safari/5.0 (compa...|\n",
      "| 33.49.147.163|20140101023103|http://lenta.ru/4...|1189| 451|Chrome/5.0 (compa...|\n",
      "| 75.208.40.166|20140101032909|http://newsru.com...|  60| 306|Safari/5.0 (Windo...|\n",
      "|197.72.248.141|20140101033626|http://newsru.com...| 736| 307|Chrome/5.0 (compa...|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "Лог не содержит столбца с доменом. Конечно можно извлечь домен с помощью функции [regexp_extract](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract), но мы так делать не будем. Напишите `pandas_udf`, которая будет извлекать домены из столбца `url`. Результаты применения функции поместите в столбец `domain`.\n",
    "\n",
    "Для извлечения домена можно воспользоваться функцией [urlparse](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_domain(url):\n",
    "    return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news.rambler.ru'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_domain(log.take(1)[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(StringType())\n",
    "def extract_domain(urls):\n",
    "    return urls.apply(_extract_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.withColumn(\"domain\", extract_domain(\"url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "|            ip|     timestamp|                 url|size|code|                  ua|         domain|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "| 33.49.147.163|20140101014611|http://news.rambl...| 378| 431|Safari/5.0 (compa...|news.rambler.ru|\n",
      "|197.72.248.141|20140101020306|http://news.mail....|1412| 203|Safari/5.0 (compa...|   news.mail.ru|\n",
      "| 33.49.147.163|20140101023103|http://lenta.ru/4...|1189| 451|Chrome/5.0 (compa...|       lenta.ru|\n",
      "| 75.208.40.166|20140101032909|http://newsru.com...|  60| 306|Safari/5.0 (Windo...|     newsru.com|\n",
      "|197.72.248.141|20140101033626|http://newsru.com...| 736| 307|Chrome/5.0 (compa...|     newsru.com|\n",
      "+--------------+--------------+--------------------+----+----+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "Для разминки давайте подсчитаем сколько дней прошло между первым и последним посещением пользователем нашего домена. Будем считать, что интересующий нас домен `news.mail.ru`. В качестве \"уникального\" идентификатора пользователя договоримся использовать ip-адрес. Использовать оконные функции в данном задании не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3.1\n",
    "Для выполнения задания №3 понадобится делать операции с датами. Заметьте, что в столбце `timestamp` хранится не настоящий timestamp, а число с датой в формате \"yyyyMMddHHmmss\". Используя функции из `pyspark.sql.functions`, создайте новый столбец `timestamp`, содержащий в себе UNIX timestamp.\n",
    "\n",
    "При возникновении ошибок, обратите внимание на типы данных. Возможно их нужно привести."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.filter(log.domain == \"news.mail.ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = log.withColumn(\"timestamp\", f.unix_timestamp(f.col(\"timestamp\").cast(\"string\"), \"yyyyMMddHHmmss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "|            ip| timestamp|                 url|size|code|                  ua|      domain|\n",
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "|197.72.248.141|1388527386|http://news.mail....|1412| 203|Safari/5.0 (compa...|news.mail.ru|\n",
      "|222.131.187.37|1388533117|http://news.mail....|1017| 416|Opera/5.0 (compat...|news.mail.ru|\n",
      "| 75.208.40.166|1388548882|http://news.mail....| 877| 301|Chrome/5.0 (compa...|news.mail.ru|\n",
      "| 33.49.147.163|1388553876|http://news.mail....| 732| 409|Opera/5.0 (compat...|news.mail.ru|\n",
      "|110.91.102.196|1388554915|http://news.mail....| 448| 201|Chrome/5.0 (compa...|news.mail.ru|\n",
      "+--------------+----------+--------------------+----+----+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- code: integer (nullable = true)\n",
      " |-- ua: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №3.2\n",
    "Приведя timestamp к правильному формату, решите исходную задачу. В результате должен получится `DataFrame` с двумя столбцами `ip` и `days`. Отсортируйте результат по столбцу `days` в порядке убывания и выведите первые 20 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.groupBy(\"ip\")\\\n",
    "   .agg(f.min(\"timestamp\").alias(\"start\"), f.max(\"timestamp\").alias(\"end\"))\\\n",
    "   .select(\"ip\", (f.col(\"end\") - f.col(\"start\")).alias(\"diff\"))\\\n",
    "   .select(\"ip\", (f.col(\"diff\") / 60 / 60 / 24).cast(\"int\").alias(\"days\"))\\\n",
    "   .orderBy(\"days\", ascending=False)\\\n",
    "   .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №4\n",
    "Подсчитайте число сессий, которое каждый пользователь (уникальный ip) сделал на домене `news.mail.ru`. Для решения этой задачи потребуется использование оконных функций (что это такое чуть ниже). Для работы с окнами в Spark SQL используется метод [over()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.over). Само окно определяется с помощью класса [Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window). Резудьтатом будет `DataFrame` со столбцами `ip` и `sessions`, отсортированный в порядке убывания числа сессий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Оконная функция_ выполняет вычисления для набора строк, некоторым образом связанных с текущей строкой. Можно сравнить её с агрегатной функцией, но, в отличие от обычной агрегатной функции, при использовании оконной функции несколько строк не группируются в одну, а продолжают существовать отдельно. Внутри же, оконная функция, как и агрегатная, может обращаться не только к текущей строке результата запроса.\n",
    "\n",
    "![](https://www.sqlitetutorial.net/wp-content/uploads/2018/11/SQLite-window-function-vs-aggregate-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот пример, показывающий, как сравнить зарплату каждого сотрудника со средней зарплатой его отдела:\n",
    "\n",
    "```sql\n",
    "SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname)\n",
    "  FROM empsalary;\n",
    "```\n",
    "\n",
    "```\n",
    "  depname  | empno | salary |          avg          \n",
    "-----------+-------+--------+-----------------------\n",
    " develop   |    11 |   5200 | 5020.0000000000000000\n",
    " develop   |     7 |   4200 | 5020.0000000000000000\n",
    " develop   |     9 |   4500 | 5020.0000000000000000\n",
    " develop   |     8 |   6000 | 5020.0000000000000000\n",
    " develop   |    10 |   5200 | 5020.0000000000000000\n",
    " personnel |     5 |   3500 | 3700.0000000000000000\n",
    " personnel |     2 |   3900 | 3700.0000000000000000\n",
    " sales     |     3 |   4800 | 4866.6666666666666667\n",
    " sales     |     1 |   5000 | 4866.6666666666666667\n",
    " sales     |     4 |   4800 | 4866.6666666666666667\n",
    "(10 rows)\n",
    "```\n",
    "\n",
    "[Документация PostgreSQL](https://postgrespro.ru/docs/postgrespro/12/tutorial-window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_window = Window.orderBy(\"timestamp\").partitionBy(\"ip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.select(\"ip\", \"timestamp\", f.lead(\"timestamp\").over(user_window).alias(\"lead\"))\\\n",
    "   .select(\"ip\", \"timestamp\", (f.col(\"lead\") - f.col(\"timestamp\")).alias(\"diff\"))\\\n",
    "   .where(\"diff >= 1800 or diff is NULL\")\\\n",
    "   .groupBy(\"ip\").agg(f.count(\"*\").alias(\"sessions\"))\\\n",
    "   .orderBy(f.col(\"sessions\").desc())\\\n",
    "   .show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание №5\n",
    "Нарисуйте гистограмму распределения числа сессий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_pd = log.select(\"ip\", \"timestamp\", f.lead(\"timestamp\").over(user_window).alias(\"lead\"))\\\n",
    "                 .select(\"ip\", \"timestamp\", (f.col(\"lead\") - f.col(\"timestamp\")).alias(\"diff\"))\\\n",
    "                 .where(\"diff >= 1800 or diff is NULL\")\\\n",
    "                 .groupBy(\"ip\").agg(f.count(\"*\").alias(\"sessions\"))\\\n",
    "                 .orderBy(f.col(\"sessions\").desc())\\\n",
    "                 .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_pd.sessions.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### БОНУС. Немножечно про Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(int(1e7), numPartitions=3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(returnType=IntegerType())\n",
    "def add_one(series):\n",
    "    return series + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.maxRecordsPerBatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.select(add_one(\"id\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.select(add_one(\"id\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
