{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes I\n",
    "**Андрей Титов**  \n",
    "tenke.iu8@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Сравнение RDD API и DataFrame API \n",
    "+ Базовые функции\n",
    "+ Очистка данных\n",
    "+ Агрегаты\n",
    "+ Кеширование \n",
    "+ Репартиционирование\n",
    "+ Встроенные функции\n",
    "+ Пользовательские функции\n",
    "+ Соединения\n",
    "+ Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение RDD API и DataFrame API \n",
    "\n",
    "### Типы данных\n",
    "**RDD**: низкоуревная распределенная коллекция данных любого типа  \n",
    "**DF**: таблица со схемой, состоящей из колонок разных типов, описанных в `org.apache.spark.sql.types`  \n",
    "\n",
    "### Обработка данных\n",
    "**RDD**: сериализуемые функции  \n",
    "**DF**: кодогенерация SQL > Java код  \n",
    "\n",
    "### Функции и алгоритмы\n",
    "**RDD**: нет ограничений  \n",
    "**DF**: ограничен SQL операторами, функциями `org.apache.spark.sql.functions` и пользовательскими функциями  \n",
    "\n",
    "### Источники данных\n",
    "**RDD**: каждый источник имеет свое API  \n",
    "**DF**: единое API для всех источников \n",
    "\n",
    "### Производительность\n",
    "**RDD**: напрямую зависит от качества кода\n",
    "**DF**: встроенные механизмы оптимизации SQL запроса\n",
    "\n",
    "\n",
    "### Потоковая обработка данных\n",
    "**RDD**: устаревший DStreams  \n",
    "**DF**: активно развивающийся Structured Streaming\n",
    "\n",
    "\n",
    "### Выводы:\n",
    "+ На текущий момент RDD является низкоуровневым API, которое постепенно уходит \"под капот\" Apache Spark\n",
    "+ DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов\n",
    "\n",
    "## Базовые функции\n",
    "\n",
    "Создать dataframe можно на основе:\n",
    "+ локальной коллекции\n",
    "+ файлов\n",
    "+ базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val cityList: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "\n",
    "// метод toDF изначально отсутствует у Vector[T], он добавляется через import spark.implicits._\n",
    "val df: DataFrame = cityList.toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У любого DF есть схема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть содержимое DF можно с помощью метода `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вывести содержимое в вертикальной ориентации - это удобно при большое количестве столбцов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(numRows = 20, truncate = 100, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в DF с помощью `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтровать данные можно с помощью метода `filter`. В отличие от RDD, он принимает SQL выражение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter('value === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter($\"value\" === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sugar free & type safe\n",
    "// Три знака равно здесь используются, тк на самом деле это метод,\n",
    "// применяемый к колонке org.apache.spark.sql.Column\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.filter(col(\"value\") === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// легко ошибиться и получить ошибку в рантайме\n",
    "\n",
    "df.filter(\"value = 'Moscow'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// промежуточный вариант между col и обычной строкой\n",
    "// expr также может использоваться для вызова SQL builtin функций, \n",
    "// отсутствующих в org.apache.spark.sql.functions\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "df.filter(expr(\"value = 'Moscow'\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить новую колонку можно с помощью метода `withColumn`. Необходимо помнить, что данный метод, как и другие, является трансформацией и не изменяет оригинальный DF, а создает новый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "df.withColumn(\"upperCity\", upper('value)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный результат получить, используя метод `select`. Данный метод может быть использован не только для выбора определенных колонок, но и для создания новых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val withUpper = df.select('value, upper('value).alias(\"upperCity\"))\n",
    "withUpper.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если передать `col(\"*\")` в `select`, то вы получите DF со всеми колонками. Это полезно, когда вы не знаете список всех колонок (например вы получили его через API), но вам нужно их все выбрать и добавить новую колонку. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// методы name, as и alias часто являются взаимозаменяемыми\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "withUpper.select(\n",
    "    col(\"*\"), \n",
    "    lower($\"value\").name(\"lowerCity\"), \n",
    "    (length('value) + 1).as(\"length\"),\n",
    "    lit(\"foo\").alias(\"bar\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости в `select` можно передать список колонок, используя обычные строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withUpper.select(\"value\", \"upperCity\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить колонку из DF можно с помощью метода `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// drop не будет выдавать ошибку, если будет указана несуществующая колонка\n",
    "\n",
    "withUpper.drop(\"upperCity\", \"abraKadabra\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ методы `filter` и `select` принимают в качестве аргументов колонки [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). Это может быть либо ссылка на существующую колонку, либо функцию из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ любые трансформации возвращают новый DF, не меняя существующий\n",
    "+ тип [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) и возвращающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). По этой причине обычное сравнение `==` не будет работать в DF API, тк `filter` принимает [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column), а не `Boolean`\n",
    "+ Класс DataFrame в последних версиях Spark представляет собой `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`, поэтому его описание следует искать в [org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "\n",
    "## Очистка данных\n",
    "\n",
    "Одной из задач обработки данных является их очистка. DF API содержит класс функций \"not available\", описанный в пакете [org.apache.spark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions). В данном пакете есть три функции:\n",
    "+ `na.drop`\n",
    "+ `na.fill`\n",
    "+ `na.replace`\n",
    "\n",
    "Для демонстрации работы данных функций создадим новый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val testData =\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "// Создаем DF из одной строки и добавляем данные в виде новой колонки\n",
    "val raw = spark.range(0,1).select(lit(testData).alias(\"value\"))\n",
    "\n",
    "// Создаем новую колонку, разибая наши данные по \\n\n",
    "val jsonStrings: Column = split(col(\"value\"), \"\\n\").alias(\"value\")\n",
    "\n",
    "// Используем функцию explode для того, чтобы развернуть массив мехом наружу и используем темную магию \n",
    "// для превращения DataFrame в Dataset[String]\n",
    "val splited: Dataset[String] = raw.select(explode(jsonStrings)).as[String]\n",
    "\n",
    "splited.show(numRows = 10, truncate = false)\n",
    "\n",
    "\n",
    "// Создаем новый датафре... датасет, в котором наши JSON строки будут распарсены\n",
    "val df: Dataset[Row] = spark.read.json(splited)\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для очистки датасета:\n",
    "+ удалим строку с навалидным JSON, сохраним ее в отдельное место\n",
    "+ удалим дубликаты\n",
    "+ заполним `null`ы в колонках\n",
    "+ исправим `Rossiya` на `Russia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val corruptData = df.select(col(\"_corrupt_record\")).na.drop(\"all\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fillData: Map[String, Any] = Map(\"continent\" -> \"Undefined\", \"population\" -> 0)\n",
    "val replaceData: Map[Any, Any] = Map(\"Rossiya\" -> \"Russia\")\n",
    "\n",
    "val cleanData = \n",
    "    df\n",
    "    .drop(col(\"_corrupt_record\"))\n",
    "    .na.drop(\"all\")\n",
    "    .na.fill(fillData)\n",
    "    .na.replace(\"country\", replaceData)\n",
    "    .dropDuplicates\n",
    "\n",
    "\n",
    "cleanData.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API обладает удобным API для очистки данных, позволяющим разработчику сконцентрироваться разработчику на бизнес логике, а не на написании функций для обработки всех возможных исключительных ситуаций\n",
    "+ метод `spark.read.json` позволяет читать не только файлы, но и `Dataset[String]`, содержащие JSON строки.\n",
    "\n",
    "## Агрегаты\n",
    "Посчитаем суммарное население и количество городов с разбивкой по континентам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggCount = cleanData.groupBy('continent).count\n",
    "aggCount.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggSum = cleanData.groupBy('continent).sum(\"population\")\n",
    "aggSum.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы совместить несколько агрегатов в одном DF, мы можем использовать метод `agg()`. Данный метод позволяет использовать любые `Aggregate functions` из пакета [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val agg = cleanData.groupBy('continent).agg(count(\"*\").alias(\"count\"), sum(\"population\").alias(\"sumPop\"))\n",
    "agg.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью агрегатов мы можем выполнять такие действия, как, например, `collect_list` и `collect_set`. Стоит отметить, что колонки в Spark могут иметь не только скалярные типы, но и структуры, словари и массивы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggList = cleanData.groupBy('continent).agg(collect_list(\"country\").alias(\"countries\"))\n",
    "aggList.printSchema\n",
    "aggList.show(numRows = 10, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы `struct` и `to_json`, мы можем превратить произвольный набор колонок в JSON строку. Этот методы часто используется перед отправкой данных в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val withStruct = aggList.select(struct('continent, 'countries).alias(\"s\"))\n",
    "withStruct.printSchema\n",
    "\n",
    "withStruct.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withStruct.withColumn(\"s\", to_json('s)).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо превратить все колонки DF в JSON String, можно воспользоваться функций `toJSON`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jString: Dataset[String] = aggList.toJSON\n",
    "jString.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам необходимо создать колонки из значений текущих колонок, мы можем воспользоваться функцией `pivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\").agg(sum(\"population\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/datasets/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `onlyRu` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val onlyRuAndHigh = airports.filter('iso_country === \"RU\" and 'elevation_ft > 1000)\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "\n",
    "onlyRuAndHigh.count\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyRuAndHigh.cache\n",
    "onlyRuAndHigh.count\n",
    "// при вычислении count данные будут помещены в cache\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show\n",
    "\n",
    "onlyRuAndHigh.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Репартиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val skewColumn = when(col(\"id\") < 900, lit(0)).otherwise(lit(1))\n",
    "\n",
    "val skewDf = spark.range(0,1000).repartition(10, skewColumn)\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(50, false)\n",
    "}\n",
    "\n",
    "printItemPerPartition[java.lang.Long](skewDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20)\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "// поэтому Spark выполнит HashPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20, col(\"id\"))\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://pngimage.net/wp-content/uploads/2018/06/соленья-png-4.png\">\n",
    "\n",
    "### Соленья\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.printSchema\n",
    "\n",
    "airports.groupBy('type).count.orderBy('count.desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val saltModTen = pmod(round((rand() * 100), 0), lit(10)).cast(\"int\")\n",
    "\n",
    "val salted = airports.withColumn(\"salt\", saltModTen)\n",
    "salted.show(numRows = 1, truncate = 200, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val firstStep = salted.groupBy('type, 'salt).count()\n",
    "\n",
    "firstStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым шагом мы делаем еще один агрегат, суммируя предыдущие значения `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val secondStep = firstStep.groupBy('type).agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "secondStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы сделали две группировки вместо одной, распределение данных по воркерам было более равномерным, что позволило избежать OOM на воркерах.\n",
    "\n",
    "### Выводы:\n",
    "+ Партиционирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений\n",
    "+ В Spark всегда работает правило 1 TASK = 1 THREAD = 1 PARTITION\n",
    "+ Репартиционирование и соление данных позволяет решить проблему перекоса данных и вычислений\n",
    "+ Важно помнить, что репартиционирование использует дисковую и сетевую подсистемы - обмен данными происходит **по сети**, а результат записывается **на диск**, что может стать узким местом при выполнении репартиционирования\n",
    "\n",
    "## Встроенные функции\n",
    "Помимо базовых SQL операторов, в Spark существует большой набор встроенных функций:\n",
    "+ API методы из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ [SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(0,10)\n",
    "\n",
    "// используем org.apache.spark.sql.functions\n",
    "val newCol: Column = pmod(col(\"id\"), lit(2))\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "// используем SQL built-in functions\n",
    "val newCol: Column = expr(\"\"\"pmod(id, 2)\"\"\")\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Spark обладает широким набором функций для работы с колонками разных типов, включая простые типы - строки, числа, и т. д., а также словари, массивы и структуры\n",
    "+ Встроенные функции принимают колонки `org.apache.spark.sql.Column` и возвращают `org.apache.spark.sql.Column` в большинстве случаев\n",
    "+ Встроенные функции доступны в двух местах - org.apache.spark.sql.functions и SQL built-in functions\n",
    "+ Встроенные функции можно (и нужно) использовать вместе - на вход во встроенные функции могут подаваться результаты встроенной функции, тк все они возвращают `sql.Column` \n",
    "\n",
    "### Пользовательские функции\n",
    "\n",
    "В том случае, если функционала встроенных функций не хватает, можно написать пользовательскую функцию - UDF. Пользовательская функция может принимать до 16 аргументов. Соответствие Spark и Scala типов описано [здесь](https://spark.apache.org/docs/latest/sql-reference.html#data-types)\n",
    "\n",
    "Необходимо помнить, что `null` в Spark превращается в `null` внутри UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val plusOne = udf { (value: Long) => value + 1 }\n",
    "\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"))).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовательская функция может возвращать:\n",
    "+ простой тип - `String`, `Long`, `Float`, `Boolean` и т.д.\n",
    "+ массив - любые коллекции, наследующие `Seq[T]` - `List[T]`, `Vector[T]` и т. д.\n",
    "+ словарь - `Map[A,B]`\n",
    "+ инстанс `case class`'а\n",
    "+ Option[T]\n",
    "\n",
    "Реализуем функцию, которая возвращает имя хоста, на котором работает воркер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.net.InetAddress\n",
    "\n",
    "val hostname = udf { () => InetAddress.getLocalHost().getHostName() }\n",
    "\n",
    "df.withColumn(\"hostname\", hostname()).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем использовать монады `Try[T]` и `Option[T]` и для написания пользовательской функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val divideTwoBy = udf { (inputValue: Long) => Try(2L / inputValue).toOption }\n",
    "\n",
    "val result = df.withColumn(\"divideTwoBy\", divideTwoBy(col(\"id\")))\n",
    "result.printSchema\n",
    "result.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API\n",
    "+ Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU\n",
    "\n",
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "По методу соединения join'ы бывают:\n",
    "![Joins](http://kirillpavlov.com/images/join-types.png)\n",
    "[Источник](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/)\n",
    "\n",
    "Добавим новую колонку к датасету `airports`, в которой будет процент заданного типа аэропорта ко всем типам аэропорта по каждой стране. Первым шагом посчитаем число аэропортов каждого типа по стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{count, round, lit}\n",
    "\n",
    "val aggTypeCountry = airports.groupBy('type, 'iso_country).agg(count(\"*\").alias(\"cnt_country_type\"))\n",
    "\n",
    "aggTypeCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем количество аэропортов по каждой стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggCountry = airports.groupBy('iso_country).agg(count(\"*\").alias(\"cnt_country\"))\n",
    "aggCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим получившиеся датасеты и получим процентное распределение типов аэропорта по стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val percent = \n",
    "    aggTypeCountry\n",
    "        .join(aggCountry, Seq(\"iso_country\"), \"inner\")\n",
    "        .select('iso_country, 'type, (round(lit(100) * 'cnt_country_type / 'cnt_country, 2).alias(\"percent\")))\n",
    "percent.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим полученный датасет с изначальным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = airports.join(percent, Seq(\"iso_country\", \"type\"), \"left\")\n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех наших джойнах присутствует массив `Seq[String]`. Это синтаксических сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет делаться по колонкам с именами, входящим в массив.\n",
    "\n",
    "В общем случае условие джойна должно быть выражено в виде колонки `sql.Column`, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "val joinCondition: Column = col(\"left_a\") === col(\"right_a\") and col(\"left_b\") === col(\"right_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в данном выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джойн двух распределенных датасетов и если условие соединения будет плохо составлено, то Spark выполнит `cross join`, производительность которого будет \"крайне мала\" &copy;\n",
    "\n",
    "### Выводы:\n",
    "+ Spark поддерживает большое число типов соединений\n",
    "+ Условием соединения может быть `Seq[String]`, либо `sql.Column`\n",
    "+ При использовании сложных условий соединения следует избегать тех, которые приведут к `cross join`\n",
    "\n",
    "## Оконные функции\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [org.apache.spark.sql.expressions.Window](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```val window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series данными.\n",
    "\n",
    "Выполним задачу с вычисление процента отношения типов аэропортов, используя оконные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val windowCountry = Window.partitionBy(\"iso_country\")\n",
    "val windowTypeCountry = Window.partitionBy(\"type\", \"iso_country\")\n",
    "\n",
    "val result = airports\n",
    "                .withColumn(\"cnt_country\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_country_type\", count(\"*\").over(windowTypeCountry))\n",
    "                .withColumn(\"percent\", round(lit(100) * 'cnt_country_type / 'cnt_country, 2))\n",
    "                            \n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Оконные функции позволяют применять функции, применительно к окнам данных\n",
    "+ Окно определяется списком колонок и сортировкой\n",
    "+ Применение оконных функций приводит к `shuffle`\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
